 3/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
 4/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
 4/2:
test = [[[1,2,3],[1,2,3],[1,2,3]],[[1,2,3],[1,2,3],[1,2,3]],[[1,2,3],[1,2,3],[1,2,3]]]
test = torch.tensor(test)
print(type(test[0][0]))
 4/3:
test = [[[1,2,3],[1,2,3],[1,2,3]],[[1,2,3],[1,2,3],[1,2,3]],[[1,2,3],[1,2,3],[1,2,3]]]
test = torch.tensor(test)
print(type(test[0][0]))
print(test[0][0])
 2/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
 2/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
 2/3:
instance_toimage = []

for im_name in os.listdir("./dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
 2/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
 2/5:
class jsrcnn(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, 3),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        for img in images:
            img = np.transpose(1,2,0)
            for i in range(img.shape[0]):
                for j in range(img.shape[1]):
                    img[i][j] = self.channel_trans(img[i][j])
            img = img.transpose(2,0,1)
            img = torch.Tensor(img)
            trniml.append(img)

        ret = self.maskrcnn(trniml,targets)
        return ret
 2/6:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn()

optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)
for i in range(2000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())
            if i%200==0:
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                scheduler.step()
                torch.save(model.state_dict(), "cif"+str(i)+".torch")
 2/7:
class jsrcnn(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, 3),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        for img in images:
            img = img.transpose(1,2,0)
            for i in range(img.shape[0]):
                for j in range(img.shape[1]):
                    img[i][j] = self.channel_trans(img[i][j])
            img = img.transpose(2,0,1)
            img = torch.Tensor(img)
            trniml.append(img)

        ret = self.maskrcnn(trniml,targets)
        return ret
 2/8:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn()

optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)
for i in range(2000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())
            if i%200==0:
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                scheduler.step()
                torch.save(model.state_dict(), "cif"+str(i)+".torch")
 2/9:
class jsrcnn(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, 3),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        for img in images:
            img = img.transpose(1,2,0)
            for i in range(img.shape[0]):
                for j in range(img.shape[1]):
                    img[i][j] = self.channel_trans(img[i][j])
            img = img.transpose(2,0,1)
            img = torch.Tensor(img)
            trniml.append(img)

        ret = self.maskrcnn(trniml,targets)
        return ret
2/10:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn()

optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)
for i in range(2000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())
            if i%200==0:
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                scheduler.step()
                torch.save(model.state_dict(), "cif"+str(i)+".torch")
2/11:
class jsrcnn(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, 3),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        for img in images:
            print(img.shape)
            img = img.transpose(1,2,0)
            for i in range(img.shape[0]):
                for j in range(img.shape[1]):
                    img[i][j] = self.channel_trans(img[i][j])
            img = img.transpose(2,0,1)
            img = torch.Tensor(img)
            trniml.append(img)

        ret = self.maskrcnn(trniml,targets)
        return ret
2/12:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn()

optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)
for i in range(2000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())
            if i%200==0:
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                scheduler.step()
                torch.save(model.state_dict(), "cif"+str(i)+".torch")
2/13:
class jsrcnn(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, 3),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        for img in images:
            print(img.shape)
            img = np.array(img)
            img = img.transpose(1,2,0)
            for i in range(img.shape[0]):
                for j in range(img.shape[1]):
                    img[i][j] = self.channel_trans(img[i][j])
            img = img.transpose(2,0,1)
            img = torch.Tensor(img)
            trniml.append(img)

        ret = self.maskrcnn(trniml,targets)
        return ret
2/14:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn()

optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)
for i in range(2000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())
            if i%200==0:
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                scheduler.step()
                torch.save(model.state_dict(), "cif"+str(i)+".torch")
2/15:
class jsrcnn(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, 3),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        for img in images:
            print(img.shape)
            img = np.array(img.cpu())
            img = img.transpose(1,2,0)
            for i in range(img.shape[0]):
                for j in range(img.shape[1]):
                    img[i][j] = self.channel_trans(img[i][j])
            img = img.transpose(2,0,1)
            img = torch.Tensor(img).to(device)
            trniml.append(img)

        ret = self.maskrcnn(trniml,targets)
        return ret
2/16:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn()

optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)
for i in range(2000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())
            if i%200==0:
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                scheduler.step()
                torch.save(model.state_dict(), "cif"+str(i)+".torch")
2/17:
class jsrcnn(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, 3),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        for img in images:
            print(img.shape)
            #img = np.array(img.cpu())
            img = img.permute(1,2,0)
            for i in range(img.shape[0]):
                for j in range(img.shape[1]):
                    img[i][j] = self.channel_trans(img[i][j])
            img = img.permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            trniml.append(img)

        ret = self.maskrcnn(trniml,targets)
        return ret
2/18:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn()

optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)
for i in range(2000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())
            if i%200==0:
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                scheduler.step()
                torch.save(model.state_dict(), "cif"+str(i)+".torch")
2/19:
class jsrcnn(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, 3),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        for img in images:
            print(img.shape)
            #img = np.array(img.cpu())
            img = img.permute(1,2,0)
            for i in range(img.shape[0]):
                for j in range(img.shape[1]):
                    img[i][j] = self.channel_trans(img[i][j])
            img = img.permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            trniml.append(img)
        trniml.to(device)
        ret = self.maskrcnn(trniml,targets)
        return ret
2/20:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn()

optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)
for i in range(2000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())
            if i%200==0:
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                scheduler.step()
                torch.save(model.state_dict(), "cif"+str(i)+".torch")
2/21:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn()
model.to(device)
optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)
for i in range(2000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())
            if i%200==0:
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                scheduler.step()
                torch.save(model.state_dict(), "cif"+str(i)+".torch")
 4/4:
l = [1,2,3,4]

for a in l:
    a = a + 1

print(l)
 4/5:
l = [1,2,3,4]

for a in range(len(l)):
    l[a] = l[a] + 1

print(l)
2/22:
class jsrcnn(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, 3),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        for ind in len(images):
            print(img)
            #img = np.array(img.cpu())
            images[ind] = images[ind].permute(1,2,0)
            for i in range(images[ind].shape[0]):
                for j in range(images[ind].shape[1]):
                    img[i][j] = self.channel_trans(img[i][j])
            img = img.permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        return ret
2/23:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn()
model.to(device)
optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)
for i in range(2000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())
            if i%200==0:
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                scheduler.step()
                torch.save(model.state_dict(), "cif"+str(i)+".torch")
2/24:
class jsrcnn(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, 3),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        for ind in range(len(images)):
            print(img)
            #img = np.array(img.cpu())
            images[ind] = images[ind].permute(1,2,0)
            for i in range(images[ind].shape[0]):
                for j in range(images[ind].shape[1]):
                    img[i][j] = self.channel_trans(img[i][j])
            img = img.permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        return ret
2/25:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn()
model.to(device)
optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)
for i in range(2000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())
            if i%200==0:
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                scheduler.step()
                torch.save(model.state_dict(), "cif"+str(i)+".torch")
2/26:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn()
model.to(device)
optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)
for i in range(2000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())
            if i%200==0:
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                scheduler.step()
                torch.save(model.state_dict(), "cif"+str(i)+".torch")
2/27:
class jsrcnn(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, 3),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        for ind in range(len(images)):
            print(ind)
            #img = np.array(img.cpu())
            images[ind] = images[ind].permute(1,2,0)
            for i in range(images[ind].shape[0]):
                for j in range(images[ind].shape[1]):
                    img[i][j] = self.channel_trans(img[i][j])
            img = img.permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        return ret
2/28:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn()
model.to(device)
optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)
for i in range(2000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())
            if i%200==0:
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                scheduler.step()
                torch.save(model.state_dict(), "cif"+str(i)+".torch")
2/29:
class jsrcnn(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, 3),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        for ind in range(len(images)):
            print(ind)
            #img = np.array(img.cpu())
            images[ind] = images[ind].permute(1,2,0)
            for i in range(images[ind].shape[0]):
                for j in range(images[ind].shape[1]):
                    images[ind][i][j] = self.channel_trans(images[ind][i][j])
            images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        return ret
2/30:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn()
model.to(device)
optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)
for i in range(2000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())
            if i%200==0:
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                scheduler.step()
                torch.save(model.state_dict(), "cif"+str(i)+".torch")
 6/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
 6/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
 6/3:
instance_toimage = []

# for im_name in os.listdir("./dataset/images"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("./dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
 6/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
 6/5:
class jsrcnn(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, 3),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        for ind in range(len(images)):
            print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            images[ind] = images[ind].reshape([H*W,3])
            print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        return ret
 6/6:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn()
model.to(device)
optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)
for i in range(2000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())
            if i%200==0:
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                scheduler.step()
                torch.save(model.state_dict(), "cif"+str(i)+".torch")
 8/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
 8/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
 8/3:
instance_toimage = []

# for im_name in os.listdir("./dataset/images"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("./dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
 8/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
 8/5:
class jsrcnn(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, 3),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        for ind in range(len(images)):
            print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            images[ind] = images[ind].reshape([H*W,3])
            print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        return ret
10/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
10/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
10/3:
instance_toimage = []

for im_name in os.listdir("./dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
10/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
10/5:
class jsrcnn(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, 3),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        for ind in range(len(images)):
            print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            images[ind] = images[ind].reshape([H*W,3])
            print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        return ret
10/6:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn()
model.to(device)
optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)
for i in range(5000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())
            if i%200==0:
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                scheduler.step()
                torch.save(model.state_dict(), "jsr"+str(i)+".torch")
12/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
12/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
12/3:
instance_toimage = []

for im_name in os.listdir("./dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
12/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
12/5:
class jsrcnn(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, 3),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        return ret
12/6:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn()
model.to(device)
optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)
for i in range(5000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())
            if i%200==0:
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                scheduler.step()
                torch.save(model.state_dict(), "jsr"+str(i)+".torch")
14/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
14/2: torch.cuda.empty_cache()
15/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
15/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
15/3:
instance_toimage = []

for im_name in os.listdir("./dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
15/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
15/5:
class jsrcnn(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128, 3),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        return ret
15/6:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn()
model.to(device)
model.load_state_dict("jsr4200")
optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)
for i in range(5000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())
            if i%200==0:
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                scheduler.step()
                torch.save(model.state_dict(), "jsrv1"+str(i)+".torch")
15/7:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn()
model.to(device)
model.load_state_dict(torch.load("jsr4200"))
optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)
for i in range(5000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())
            if i%200==0:
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                scheduler.step()
                torch.save(model.state_dict(), "jsrv1"+str(i)+".torch")
15/8:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn()
model.to(device)
model.load_state_dict(torch.load("jsr4200.torch"))
optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)
for i in range(5000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())
            if i%200==0:
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                scheduler.step()
                torch.save(model.state_dict(), "jsrv1"+str(i)+".torch")
15/9:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn()
model.to(device)
model.load_state_dict(torch.load("jsr4200.torch"))
optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)
for i in range(5000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())
            if i%200==0:
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                scheduler.step()
                torch.save(model.state_dict(), "jsrv1"+str(i)+".torch")
15/10: torch.save(model.state_dict(), "jsrv1"+"5000"+".torch")
15/11:
for i in range(5000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())
            if i%500==0:
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                scheduler.step()
                torch.save(model.state_dict(), "jsrv2"+str(i)+".torch")
15/12:
for i in range(5000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())
            if i%200==0:
                scheduler.step()
            if i%500==0:
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                scheduler.step()
                torch.save(model.state_dict(), "jsrv2"+str(i)+".torch")
15/13:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn()
model.to(device)
model.load_state_dict(torch.load("jsrv21000.torch"))
optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-5)
model.train()
lmbda = lambda epoch: 0.8
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)
15/14:
for i in range(1001,5000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())
            if i%200==0:
                scheduler.step()
            if i%500==0:
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                scheduler.step()
                torch.save(model.state_dict(), "jsrv2"+str(i)+".torch")
15/15:
for i in range(1001,5000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())
            if i%200==0:
                scheduler.step()
            if i%500==0:
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                scheduler.step()
                torch.save(model.state_dict(), "jsrv2"+str(i)+".torch")
15/16: torch.save(model.state_dict(), "jsrv25000"+".torch")
16/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
16/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
16/3:
instance_toimage = []

for im_name in os.listdir("./dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
16/4:
batchSize = 4
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
16/5:
class jsrcnn_v1(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v1, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 64),
            nn.ReLU(),
            nn.Linear(64, 3),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        return ret
16/6:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v1()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
16/7:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())
            if(losses.item()<minloss):
                torch.save(model.state_dict(), "jsr_v1_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
            if i%500==0:
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v1_"+str(i)+"_"+str(losses.item())+".torch")
18/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
18/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
18/3:
instance_toimage = []

for im_name in os.listdir("./dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
18/4:
batchSize = 3
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
18/5:
class jsrcnn_v1(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v1, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 64),
            nn.ReLU(),
            nn.Linear(64, 3),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        return ret
18/6:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v1()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
18/7:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())
            if(losses.item()<minloss):
                torch.save(model.state_dict(), "jsr_v1_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
            if i%500==0:
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v1_"+str(i)+"_"+str(losses.item())+".torch")
19/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
19/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
19/3:
instance_toimage = []

for im_name in os.listdir("./dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
19/4:
batchSize = 3
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
19/5:
class jsrcnn_v1(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v1, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 64),
            nn.ReLU(),
            nn.Linear(64, 3),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        return ret
19/6:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v1()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
19/7:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())
            if(losses.item()<minloss):
                torch.save(model.state_dict(), "jsr_v1_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
            if i%500==0:
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v1_"+str(i)+"_"+str(losses.item())+".torch")
20/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
20/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
20/3:
instance_toimage = []

for im_name in os.listdir("./dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
20/4:
batchSize = 3
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
20/5:
class jsrcnn_v1(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v1, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 64),
            nn.ReLU(),
            nn.Linear(64, 3),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        return ret
20/6:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v1()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
20/7:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())
            if(losses.item()<minloss):
                torch.save(model.state_dict(), "jsr_v1_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
            if i%500==0:
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v1_"+str(i)+"_"+str(losses.item())+".torch")
20/8:
class jsrcnn_v1(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v1, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 64),
            nn.Linear(64, 3),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        return ret
21/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
21/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
21/3:
instance_toimage = []

for im_name in os.listdir("./dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
21/4:
batchSize = 3
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
21/5:
class jsrcnn_v1(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v1, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 64),
            nn.Linear(64, 3),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        return ret
21/6:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v1()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
21/7:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())
            if(losses.item()<minloss):
                torch.save(model.state_dict(), "jsr_v1_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
            if i%500==0:
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v1_"+str(i)+"_"+str(losses.item())+".torch")
22/1:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 10),
            nn.Hardtanh(),
            nn.Linear(10, 40),
            nn.Hardtanh(),
            nn.Linear(40,3),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            if self.train:
               sem_val = torch.mul(images[ind],targets["sem_mask"]).sum((1,2))/targets["sem_mask_c"]
               neg_val = torch.mul(images[ind],targets["neg_mask"]).sum((1,2))/targets["neg_mask_c"]
               color_loss = torch.mul(sem_val,neg_val)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            ret.update(color_loss)
        return ret
22/2:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
22/3:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
22/4:
instance_toimage = []

for im_name in os.listdir("./dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
23/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
23/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
23/3:
instance_toimage = []

# for im_name in os.listdir("./dataset/images"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("./dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
23/4:
batchSize = 3
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = sem_mask.sum()
        neg_mask = (sem_mask == 0)
        neg_mask_c = neg_mask.sum()
        sem_mask = np.array(sem_mask)
        sem_mask = torch.Tensor([sem_mask,sem_mask,sem_mask],dtype=torch.uint8)
        neg_mask = np.array(neg_mask)
        neg_mask = torch.Tensor([neg_mask,neg_mask,neg_mask],dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
23/5:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 10),
            nn.Hardtanh(),
            nn.Linear(10, 40),
            nn.Hardtanh(),
            nn.Linear(40,3),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            if self.train:
               sem_val = torch.mul(images[ind],targets["sem_mask"]).sum((1,2))/targets["sem_mask_c"]
               neg_val = torch.mul(images[ind],targets["neg_mask"]).sum((1,2))/targets["neg_mask_c"]
               color_loss = torch.mul(sem_val,neg_val)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            ret.update(color_loss)
        return ret
23/6:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v1()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
23/7:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
23/8:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())
            if(losses.item()<minloss):
                torch.save(model.state_dict(), "jsr_v1_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
            if i%500==0:
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v1_"+str(i)+"_"+str(losses.item())+".torch")
23/9:
batchSize = 3
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = sem_mask.sum()
        neg_mask = (sem_mask == 0)
        neg_mask_c = neg_mask.sum()
        sem_mask = np.array(sem_mask)
        sem_mask = torch.as_tensor([sem_mask,sem_mask,sem_mask],dtype=torch.uint8)
        neg_mask = np.array(neg_mask)
        neg_mask = torch.as_tensor([neg_mask,neg_mask,neg_mask],dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
23/10:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 10),
            nn.Hardtanh(),
            nn.Linear(10, 40),
            nn.Hardtanh(),
            nn.Linear(40,3),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            if self.train:
               sem_val = torch.mul(images[ind],targets["sem_mask"]).sum((1,2))/targets["sem_mask_c"]
               neg_val = torch.mul(images[ind],targets["neg_mask"]).sum((1,2))/targets["neg_mask_c"]
               color_loss = torch.mul(sem_val,neg_val)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            ret.update(color_loss)
        return ret
23/11:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
23/12:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())
            if(losses.item()<minloss):
                torch.save(model.state_dict(), "jsr_v1_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
            if i%500==0:
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v1_"+str(i)+"_"+str(losses.item())+".torch")
23/13:
batchSize = 3
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
23/14:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 10),
            nn.Hardtanh(),
            nn.Linear(10, 40),
            nn.Hardtanh(),
            nn.Linear(40,3),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            if self.train:
               sem_val = torch.mul(images[ind],targets["sem_mask"]).sum((1,2))/targets["sem_mask_c"]
               neg_val = torch.mul(images[ind],targets["neg_mask"]).sum((1,2))/targets["neg_mask_c"]
               color_loss = torch.mul(sem_val,neg_val)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            ret.update(color_loss)
        return ret
23/15:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
23/16:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())
            if(losses.item()<minloss):
                torch.save(model.state_dict(), "jsr_v1_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
            if i%500==0:
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v1_"+str(i)+"_"+str(losses.item())+".torch")
23/17:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 10),
            nn.Hardtanh(),
            nn.Linear(10, 40),
            nn.Hardtanh(),
            nn.Linear(40,3),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            if self.train:
               sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))/targets[ind]["sem_mask_c"]
               neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))/targets[ind]["neg_mask_c"]
               color_loss = torch.mul(sem_val,neg_val)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            ret.update(color_loss)
        return ret
23/18:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
23/19:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())
            if(losses.item()<minloss):
                torch.save(model.state_dict(), "jsr_v1_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
            if i%500==0:
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v1_"+str(i)+"_"+str(losses.item())+".torch")
23/20:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 10),
            nn.Hardtanh(),
            nn.Linear(10, 40),
            nn.Hardtanh(),
            nn.Linear(40,3),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            if self.train:
               sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))/targets[ind]["sem_mask_c"]
               neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))/targets[ind]["neg_mask_c"]
               color_loss = torch.mul(sem_val,neg_val)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            ret["color_loss"] = color_loss
        return ret
23/21:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
23/22:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())
            if(losses.item()<minloss):
                torch.save(model.state_dict(), "jsr_v1_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
            if i%500==0:
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v1_"+str(i)+"_"+str(losses.item())+".torch")
27/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
27/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
27/3:
instance_toimage = []

# for im_name in os.listdir("./dataset/images"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("./dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
27/4:
batchSize = 1
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
27/5:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 10),
            nn.Hardtanh(),
            nn.Linear(10, 40),
            nn.Hardtanh(),
            nn.Linear(40,3),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            if self.train:
               sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))/targets[ind]["sem_mask_c"]
               neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))/targets[ind]["neg_mask_c"]
               color_loss = torch.mul(sem_val,neg_val)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            ret["color_loss"] = color_loss
        return ret
27/6:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
27/7:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())
            if(losses.item()<minloss):
                torch.save(model.state_dict(), "jsr_v1_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
            if i%500==0:
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v1_"+str(i)+"_"+str(losses.item())+".torch")
27/8:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 10),
            nn.Hardtanh(),
            nn.Linear(10, 40),
            nn.Hardtanh(),
            nn.Linear(40,3),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            if self.train:
               sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))/targets[ind]["sem_mask_c"]
               neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))/targets[ind]["neg_mask_c"]
               color_loss = torch.mul(sem_val,neg_val)
               print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            ret["color_loss"] = color_loss
        return ret
27/9:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
27/10:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())
            if(losses.item()<minloss):
                torch.save(model.state_dict(), "jsr_v1_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
            if i%500==0:
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v1_"+str(i)+"_"+str(losses.item())+".torch")
27/11:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 10),
            nn.Hardtanh(),
            nn.Linear(10, 40),
            nn.Hardtanh(),
            nn.Linear(40,3),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            if self.train:
               sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))/targets[ind]["sem_mask_c"]
               neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))/targets[ind]["neg_mask_c"]
               color_loss = torch.mul(sem_val,neg_val).sum()
               print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            ret["color_loss"] = color_loss
        return ret
27/12:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
27/13:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())
            if(losses.item()<minloss):
                torch.save(model.state_dict(), "jsr_v1_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
            if i%500==0:
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v1_"+str(i)+"_"+str(losses.item())+".torch")
29/1:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())
            if(losses.item()<minloss):
                torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2_"+str(i)+"_"+str(losses.item())+".torch")
30/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
30/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
30/3:
instance_toimage = []

# for im_name in os.listdir("./dataset/images"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("./dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
32/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
32/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
32/3:
instance_toimage = []

for im_name in os.listdir("./dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
32/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
32/5:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 10),
            nn.Hardtanh(),
            nn.Linear(10, 40),
            nn.Hardtanh(),
            nn.Linear(40,3),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            if self.train:
               sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))/targets[ind]["sem_mask_c"]
               neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))/targets[ind]["neg_mask_c"]
               color_loss = torch.mul(sem_val,neg_val).sum()
               print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            ret["color_loss"] = color_loss
        return ret
32/6:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
32/7:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())
            if(losses.item()<minloss):
                torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2_"+str(i)+"_"+str(losses.item())+".torch")
33/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
33/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
33/3:
instance_toimage = []

for im_name in os.listdir("./dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
33/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
33/5:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 10),
            nn.Hardtanh(),
            nn.Linear(10, 40),
            nn.Hardtanh(),
            nn.Linear(40,3),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            if self.train:
               sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))/targets[ind]["sem_mask_c"]
               neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))/targets[ind]["neg_mask_c"]
               color_loss = torch.mul(sem_val,neg_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            ret["color_loss"] = color_loss
        return ret
33/6:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
33/7:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())
            if(losses.item()<minloss):
                torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2_"+str(i)+"_"+str(losses.item())+".torch")
34/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
34/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
34/3:
instance_toimage = []

for im_name in os.listdir("./dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
34/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
34/5:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 10),
            nn.Hardtanh(),
            nn.Linear(10, 40),
            nn.Hardtanh(),
            nn.Linear(40,3),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            if self.train:
               sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))/targets[ind]["sem_mask_c"]
               neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))/targets[ind]["neg_mask_c"]
               color_loss = torch.mul(sem_val,neg_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            ret["color_loss"] = color_loss
        return ret
34/6:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
34/7:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())
            for loss in loss_dict.keys():
                print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2_"+str(i)+"_"+str(losses.item())+".torch")
36/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
36/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
36/3:
instance_toimage = []

for im_name in os.listdir("./dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
36/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
36/5:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 10),
            nn.Hardtanh(),
            nn.Linear(10, 40),
            nn.Hardtanh(),
            nn.Linear(40,3),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            if self.train:
               sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))/targets[ind]["sem_mask_c"]
               neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))/targets[ind]["neg_mask_c"]
               color_loss = torch.mul(sem_val,neg_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            ret["color_loss"] = color_loss
        return ret
36/6:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters()},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-2}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
36/7:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())
            for loss in loss_dict.keys():
                print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2_"+str(i)+"_"+str(losses.item())+".torch")
37/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
37/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
37/3:
instance_toimage = []

for im_name in os.listdir("./dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
37/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
37/5:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 10),
            nn.Hardtanh(),
            nn.Linear(10, 40),
            nn.Hardtanh(),
            nn.Linear(40,3),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            if self.train:
               sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))/targets[ind]["sem_mask_c"]
               neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))/targets[ind]["neg_mask_c"]
               color_loss = torch.mul(sem_val,neg_val).sum()/(sem_val.sum() + neg_val.sum())
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            ret["color_loss"] = color_loss
        return ret
37/6:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters()},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-2}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
37/7:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())
            for loss in loss_dict.keys():
                print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2_"+str(i)+"_"+str(losses.item())+".torch")
38/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
38/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
38/3:
instance_toimage = []

for im_name in os.listdir("./dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
38/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
38/5:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 10),
            nn.Hardtanh(),
            nn.Linear(10, 40),
            nn.Hardtanh(),
            nn.Linear(40,3),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            if self.train:
               sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))/targets[ind]["sem_mask_c"]
               neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))/targets[ind]["neg_mask_c"]
               color_loss = torch.mul(sem_val,neg_val).sum()/(sem_val.sum()*neg_val.sum())
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            ret["color_loss"] = color_loss
        return ret
38/6:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters()},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-2}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
38/7:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())
            for loss in loss_dict.keys():
                print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2_"+str(i)+"_"+str(losses.item())+".torch")
39/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
39/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
39/3:
instance_toimage = []

for im_name in os.listdir("./dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
39/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
39/5:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 10),
            nn.Hardtanh(),
            nn.Linear(10, 40),
            nn.Hardtanh(),
            nn.Linear(40,3),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            if self.train:
               sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))/targets[ind]["sem_mask_c"]
               neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))/targets[ind]["neg_mask_c"]
               color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5)
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            ret["color_loss"] = color_loss
        return ret
39/6:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 10),
            nn.Hardtanh(),
            nn.Linear(10, 40),
            nn.Hardtanh(),
            nn.Linear(40,3),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            if self.train:
               sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))/targets[ind]["sem_mask_c"]
               neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))/targets[ind]["neg_mask_c"]
               color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            ret["color_loss"] = color_loss
        return ret
39/7:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters()},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-2}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
39/8:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())
            for loss in loss_dict.keys():
                print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2_"+str(i)+"_"+str(losses.item())+".torch")
40/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
40/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
40/3:
instance_toimage = []

for im_name in os.listdir("./dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
41/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
41/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
41/3:
instance_toimage = []

for im_name in os.listdir("./dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
41/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
41/5:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 10),
            nn.Hardtanh(),
            nn.Linear(10, 40),
            nn.Hardtanh(),
            nn.Linear(40,3),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            if self.train:
               sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))/targets[ind]["sem_mask_c"]
               neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))/targets[ind]["neg_mask_c"]
               color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            ret["color_loss"] = color_loss
        return ret
41/6:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters()},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-1}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
41/7:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2_"+str(i)+"_"+str(losses.item())+".torch")
41/8:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters()},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e+1}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
41/9:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2_"+str(i)+"_"+str(losses.item())+".torch")
41/10:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters()},{ 'params' : model.channel_trans.parameters(), 'lr' : 1}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
41/11:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2_"+str(i)+"_"+str(losses.item())+".torch")
41/12:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters()},{ 'params' : model.channel_trans.parameters(), 'lr' : 0.5}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
41/13:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2_"+str(i)+"_"+str(losses.item())+".torch")
41/14:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters()},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-1}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
41/15:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2_"+str(i)+"_"+str(losses.item())+".torch")
43/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
43/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
43/3:
instance_toimage = []

for im_name in os.listdir("./dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
43/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
43/5:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 10),
            nn.Hardtanh(),
            nn.Linear(10, 40),
            nn.Hardtanh(),
            nn.Linear(40,3),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            if self.train:
               sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))/targets[ind]["sem_mask_c"]
               neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))/targets[ind]["neg_mask_c"]
               color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            ret["color_loss"] = color_loss
        return ret
43/6:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 0},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-1}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
43/7:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.1_"+str(i)+"_"+str(losses.item())+".torch")
43/8:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 0},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-1}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.maskrcnn.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
43/9:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.1_"+str(i)+"_"+str(losses.item())+".torch")
45/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
45/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
45/3:
instance_toimage = []

for im_name in os.listdir("./dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
45/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
45/5:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 10),
            nn.ReLU(),
            nn.Linear(10, 10),
            nn.ReLu(),
            nn.Linear(10,3),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            if self.train:
               sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))/targets[ind]["sem_mask_c"]
               neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))/targets[ind]["neg_mask_c"]
               #color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
               color_loss = neg_val/sem_val
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            ret["color_loss"] = color_loss
        return ret
45/6:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 0},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-1}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.maskrcnn.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
45/7:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 10),
            nn.ReLU(),
            nn.Linear(10, 10),
            nn.ReLU(),
            nn.Linear(10,3),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            if self.train:
               sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))/targets[ind]["sem_mask_c"]
               neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))/targets[ind]["neg_mask_c"]
               #color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
               color_loss = neg_val/sem_val
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            ret["color_loss"] = color_loss
        return ret
45/8:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 0},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-1}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.maskrcnn.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
45/9:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.1_"+str(i)+"_"+str(losses.item())+".torch")
45/10:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 10),
            nn.ReLU(),
            nn.Linear(10, 10),
            nn.ReLU(),
            nn.Linear(10,3),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            if self.train:
               sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))/targets[ind]["sem_mask_c"]
               neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))/targets[ind]["neg_mask_c"]
               #color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
               color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            ret["color_loss"] = color_loss
        return ret
45/11:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 0},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-1}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.maskrcnn.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
45/12:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.1_"+str(i)+"_"+str(losses.item())+".torch")
45/13:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 0},{ 'params' : model.channel_trans.parameters(), 'lr' : 5e-2}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.maskrcnn.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
45/14:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.1_"+str(i)+"_"+str(losses.item())+".torch")
46/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
46/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
46/3:
instance_toimage = []

for im_name in os.listdir("./dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
46/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
46/5:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 10),
            nn.Hardtanh(),
            nn.Linear(10, 128),
            nn.Hardtanh(),
            nn.Linear(128,3),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind]
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            if self.train:
               sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))/targets[ind]["sem_mask_c"]
               neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))/targets[ind]["neg_mask_c"]
               color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
               
            images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            ret["color_loss"] = color_loss
        return ret
46/6:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 5e-2}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
46/7:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.1_"+str(i)+"_"+str(losses.item())+".torch")
46/8:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
46/9:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
46/10:
instance_toimage = []

for im_name in os.listdir("./dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
46/11:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
46/12:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 10),
            nn.Hardtanh(),
            nn.Linear(10, 128),
            nn.Hardtanh(),
            nn.Linear(128,3),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind]
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            if self.train:
               sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))/targets[ind]["sem_mask_c"]
               neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))/targets[ind]["neg_mask_c"]
               color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
               
            images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            ret["color_loss"] = color_loss
        return ret
46/13:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-1}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
46/14:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.1_"+str(i)+"_"+str(losses.item())+".torch")
46/15:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
46/16:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
46/17:
instance_toimage = []

for im_name in os.listdir("./dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
47/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
47/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
47/3:
instance_toimage = []

# for im_name in os.listdir("./dataset/images"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("./dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
47/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
47/5:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 10),
            nn.Hardtanh(),
            nn.Linear(10, 128),
            nn.Hardtanh(),
            nn.Linear(128,3),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind]
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            if self.train:
               sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))/targets[ind]["sem_mask_c"]
               print(sem_val)
               neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))/targets[ind]["neg_mask_c"]
               color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
               
            images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            ret["color_loss"] = color_loss
        return ret
47/6:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-1}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
47/7:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.1_"+str(i)+"_"+str(losses.item())+".torch")
47/8:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
47/9:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
47/10:
instance_toimage = []

# for im_name in os.listdir("./dataset/images"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("./dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
47/11:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
47/12:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 10),
            nn.Hardtanh(),
            nn.Linear(10, 128),
            nn.Hardtanh(),
            nn.Linear(128,3),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind]
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            if self.train:
               sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
               print(sem_val)
               neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
               color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
               
            images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            ret["color_loss"] = color_loss
        return ret
47/13:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-1}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
47/14:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.1_"+str(i)+"_"+str(losses.item())+".torch")
48/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
48/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
48/3:
instance_toimage = []

# for im_name in os.listdir("./dataset/images"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("./dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
48/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
48/5:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 10),
            nn.Hardtanh(),
            nn.Linear(10, 128),
            nn.Hardtanh(),
            nn.Linear(128,3),
            nn.Sigmoid(),
        )
        self.mask_maker = nn.Sequential(nn.Linear(3,1),nn.Sigmoid())
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor(0)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind]
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            masker = masker.reshape([3,H,W])
            
            if self.train:
                losser = torch.nn.CrossEntropyLoss()
                color_loss += losser(masker,targets[ind]["sem_mask"])
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
               
            images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            ret["color_loss"] = color_loss
        return ret
48/6:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-1}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
48/7:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.1_"+str(i)+"_"+str(losses.item())+".torch")
48/8:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 10),
            nn.Hardtanh(),
            nn.Linear(10, 128),
            nn.Hardtanh(),
            nn.Linear(128,3),
            nn.Sigmoid(),
        )
        self.mask_maker = nn.Sequential(nn.Linear(3,1),nn.Sigmoid())
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor(0)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind]
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            masker = masker.reshape([1,H,W])
            if self.train:
                losser = torch.nn.CrossEntropyLoss()
                color_loss += losser(masker,targets[ind]["sem_mask"])
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
               
            images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            ret["color_loss"] = color_loss
        return ret
48/9:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-1}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
48/10:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.1_"+str(i)+"_"+str(losses.item())+".torch")
48/11:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 10),
            nn.Hardtanh(),
            nn.Linear(10, 128),
            nn.Hardtanh(),
            nn.Linear(128,3),
            nn.Sigmoid(),
        )
        self.mask_maker = nn.Sequential(nn.Linear(3,1),nn.Sigmoid())
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor(0)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind]
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            masker = masker.reshape([1,H,W])
            if self.train:
                losser = torch.nn.CrossEntropyLoss()
                color_loss += losser(masker,targets[ind]["sem_mask"][0])
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
               
            images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            ret["color_loss"] = color_loss
        return ret
48/12:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-1}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
48/13:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.1_"+str(i)+"_"+str(losses.item())+".torch")
48/14:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 10),
            nn.Hardtanh(),
            nn.Linear(10, 128),
            nn.Hardtanh(),
            nn.Linear(128,3),
            nn.Sigmoid(),
        )
        self.mask_maker = nn.Sequential(nn.Linear(3,1),nn.Sigmoid())
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor(0)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind]
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            masker = masker.reshape([1,H,W])
            if self.train:
                losser = torch.nn.CrossEntropyLoss()
                print(targets[ind]["sem_mask"].shape)
                color_loss += losser(masker,targets[ind]["sem_mask"][0])
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
               
            images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            ret["color_loss"] = color_loss
        return ret
48/15:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-1}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
48/16:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.1_"+str(i)+"_"+str(losses.item())+".torch")
48/17:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 10),
            nn.Hardtanh(),
            nn.Linear(10, 128),
            nn.Hardtanh(),
            nn.Linear(128,3),
            nn.Sigmoid(),
        )
        self.mask_maker = nn.Sequential(nn.Linear(3,1),nn.Sigmoid())
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor(0)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind]
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            masker = masker.reshape([1,H,W])
            if self.train:
                losser = torch.nn.CrossEntropyLoss()
                print(targets[ind]["sem_mask"][0].shape)
                color_loss += losser(masker,targets[ind]["sem_mask"][0])
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
               
            images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            ret["color_loss"] = color_loss
        return ret
48/18:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 10),
            nn.Hardtanh(),
            nn.Linear(10, 128),
            nn.Hardtanh(),
            nn.Linear(128,3),
            nn.Sigmoid(),
        )
        self.mask_maker = nn.Sequential(nn.Linear(3,1),nn.Sigmoid())
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor(0)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind]
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            masker = masker.reshape([1,H,W])
            if self.train:
                losser = torch.nn.CrossEntropyLoss()
                print("jsr ",targets[ind]["sem_mask"][1].shape)
                color_loss += losser(masker,targets[ind]["sem_mask"][0])
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
               
            images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            ret["color_loss"] = color_loss
        return ret
48/19:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-1}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
48/20:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.1_"+str(i)+"_"+str(losses.item())+".torch")
48/21:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 10),
            nn.Hardtanh(),
            nn.Linear(10, 128),
            nn.Hardtanh(),
            nn.Linear(128,3),
            nn.Sigmoid(),
        )
        self.mask_maker = nn.Sequential(nn.Linear(3,1),nn.Sigmoid())
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor(0)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind]
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            masker = masker.reshape([1,H,W])
            if self.train:
                losser = torch.nn.CrossEntropyLoss()
                print("jsr ",targets[ind]["sem_mask"][1].shape)
                color_loss += losser(masker,[targets[ind]["sem_mask"][0]])
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
               
            images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            ret["color_loss"] = color_loss
        return ret
48/22:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-1}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
48/23:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.1_"+str(i)+"_"+str(losses.item())+".torch")
48/24:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 10),
            nn.Hardtanh(),
            nn.Linear(10, 128),
            nn.Hardtanh(),
            nn.Linear(128,3),
            nn.Sigmoid(),
        )
        self.mask_maker = nn.Sequential(nn.Linear(3,1),nn.Sigmoid())
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor(0)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind]
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            masker = masker.reshape([1,H,W])
            if self.train:
                losser = torch.nn.CrossEntropyLoss()
                print("jsr ",targets[ind]["sem_mask"][0:1].shape)
                color_loss += losser(masker,targets[ind]["sem_mask"][0])
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
               
            images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            ret["color_loss"] = color_loss
        return ret
48/25:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-1}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
48/26:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.1_"+str(i)+"_"+str(losses.item())+".torch")
48/27:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 10),
            nn.Hardtanh(),
            nn.Linear(10, 128),
            nn.Hardtanh(),
            nn.Linear(128,3),
            nn.Sigmoid(),
        )
        self.mask_maker = nn.Sequential(nn.Linear(3,1),nn.Sigmoid())
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor(0)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind]
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            masker = masker.reshape([1,H,W])
            if self.train:
                losser = torch.nn.CrossEntropyLoss()
                print("jsr ",targets[ind]["sem_mask"][0:1].shape)
                color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
               
            images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            ret["color_loss"] = color_loss
        return ret
48/28:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-1}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
48/29:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.1_"+str(i)+"_"+str(losses.item())+".torch")
48/30:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 10),
            nn.Hardtanh(),
            nn.Linear(10, 128),
            nn.Hardtanh(),
            nn.Linear(128,3),
            nn.Sigmoid(),
        )
        self.mask_maker = nn.Sequential(nn.Linear(3,1),nn.Sigmoid())
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor(0)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind]
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            masker = masker.reshape([1,H,W])
            if self.train:
                losser = torch.nn.MSELoss()
                print("jsr ",targets[ind]["sem_mask"][0:1].shape)
                color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
               
            images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            ret["color_loss"] = color_loss
        return ret
48/31:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-1}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
48/32:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.1_"+str(i)+"_"+str(losses.item())+".torch")
48/33:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 10),
            nn.Hardtanh(),
            nn.Linear(10, 128),
            nn.Hardtanh(),
            nn.Linear(128,3),
            nn.Sigmoid(),
        )
        self.mask_maker = nn.Sequential(nn.Linear(3,1),nn.Sigmoid())
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor(0)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind]
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            masker = self.mask_maker(images[ind]).to('device')
            images[ind] = images[ind].reshape([3,H,W])
            masker = masker.reshape([1,H,W])
            if self.train:
                losser = torch.nn.MSELoss()
                print("jsr ",targets[ind]["sem_mask"][0:1].shape)
                color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
               
            images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            ret["color_loss"] = color_loss
        return ret
48/34:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-1}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
48/35:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.1_"+str(i)+"_"+str(losses.item())+".torch")
48/36:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 10),
            nn.Hardtanh(),
            nn.Linear(10, 128),
            nn.Hardtanh(),
            nn.Linear(128,3),
            nn.Sigmoid(),
        )
        self.mask_maker = nn.Sequential(nn.Linear(3,1),nn.Sigmoid())
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor(0)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind]
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            masker = self.mask_maker(images[ind]).to(device)
            images[ind] = images[ind].reshape([3,H,W])
            masker = masker.reshape([1,H,W])
            if self.train:
                losser = torch.nn.MSELoss()
                print("jsr ",targets[ind]["sem_mask"][0:1].shape)
                color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
               
            images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            ret["color_loss"] = color_loss
        return ret
48/37:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-1}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
48/38:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.1_"+str(i)+"_"+str(losses.item())+".torch")
48/39:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 10),
            nn.Hardtanh(),
            nn.Linear(10, 128),
            nn.Hardtanh(),
            nn.Linear(128,3),
            nn.Sigmoid(),
        )
        self.mask_maker = nn.Sequential(nn.Linear(3,1),nn.Sigmoid())
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor(0)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            masker = masker.reshape([1,H,W])
            if self.train:
                losser = torch.nn.MSELoss()
                print("jsr ",targets[ind]["sem_mask"][0:1].shape)
                color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
               
            images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            ret["color_loss"] = color_loss
        return ret
48/40:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-1}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
48/41:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.1_"+str(i)+"_"+str(losses.item())+".torch")
48/42:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 10),
            nn.Hardtanh(),
            nn.Linear(10, 128),
            nn.Hardtanh(),
            nn.Linear(128,3),
            nn.Sigmoid(),
        )
        self.mask_maker = nn.Sequential(nn.Linear(3,1),nn.Sigmoid())
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor(0).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            masker = masker.reshape([1,H,W])
            if self.train:
                losser = torch.nn.MSELoss()
                print("jsr ",targets[ind]["sem_mask"][0:1].shape)
                color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
               
            images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            ret["color_loss"] = color_loss
        return ret
48/43:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-1}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
48/44:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.1_"+str(i)+"_"+str(losses.item())+".torch")
48/45:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 10),
            nn.Hardtanh(),
            nn.Linear(10, 128),
            nn.Hardtanh(),
            nn.Linear(128,3),
            nn.Sigmoid(),
        )
        self.mask_maker = nn.Sequential(nn.Linear(3,1),nn.Sigmoid())
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor(0).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            masker = masker.reshape([1,H,W])
            if self.train:
                losser = torch.nn.MSELoss()
                print("jsr ",targets[ind]["sem_mask"][0:1].shape)
                print(losser(masker,targets[ind]["sem_mask"][0:1]).shape)
                color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
               
            images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            ret["color_loss"] = color_loss
        return ret
48/46:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-1}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
48/47:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.1_"+str(i)+"_"+str(losses.item())+".torch")
48/48:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 10),
            nn.Hardtanh(),
            nn.Linear(10, 128),
            nn.Hardtanh(),
            nn.Linear(128,3),
            nn.Sigmoid(),
        )
        self.mask_maker = nn.Sequential(nn.Linear(3,1),nn.Sigmoid())
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor(0).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            masker = masker.reshape([1,H,W])
            if self.train:
                losser = torch.nn.MSELoss()
                print("jsr ",type(targets[ind]["sem_mask"][0:1]))
                color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
               
            images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            ret["color_loss"] = color_loss
        return ret
48/49:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-1}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
48/50:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.1_"+str(i)+"_"+str(losses.item())+".torch")
48/51:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 10),
            nn.Hardtanh(),
            nn.Linear(10, 128),
            nn.Hardtanh(),
            nn.Linear(128,3),
            nn.Sigmoid(),
        )
        self.mask_maker = nn.Sequential(nn.Linear(3,1),nn.Sigmoid())
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor(0).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            masker = masker.reshape([1,H,W])
            if self.train:
                losser = torch.nn.MSELoss()
                #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
                color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
               
            images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            print("color_loss",color_loss.shape)
            ret["color_loss"] = color_loss
        return ret
48/52:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-1}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
48/53:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.1_"+str(i)+"_"+str(losses.item())+".torch")
49/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
49/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
49/3:
instance_toimage = []

# for im_name in os.listdir("./dataset/images"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("./dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
49/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
49/5:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 10),
            nn.Hardtanh(),
            nn.Linear(10, 128),
            nn.Hardtanh(),
            nn.Linear(128,3),
            nn.Sigmoid(),
        )
        self.mask_maker = nn.Sequential(nn.Linear(3,1),nn.Sigmoid())
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor(0).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            masker = masker.reshape([1,H,W])
            if self.train:
                losser = torch.nn.MSELoss()
                #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
                color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
               
            images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            print("color_loss",color_loss.shape)
            ret["color_loss"] = color_loss
        return ret
49/6:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-1}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
49/7:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.1_"+str(i)+"_"+str(losses.item())+".torch")
49/8:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 10),
            nn.Hardtanh(),
            nn.Linear(10, 128),
            nn.Hardtanh(),
            nn.Linear(128,3),
            nn.Sigmoid(),
        )
        self.mask_maker = nn.Sequential(nn.Linear(3,1),nn.Sigmoid())
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor(0).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            masker = masker.reshape([1,H,W])
            if self.train:
                losser = torch.nn.MSELoss()
                #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
                color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
               
            images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            print("color_loss",color_loss.shape)
            color_loss = color_loss.sum()
            ret["color_loss"] = color_loss
        return ret
49/9:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-1}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
49/10:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.1_"+str(i)+"_"+str(losses.item())+".torch")
49/11:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 10),
            nn.Hardtanh(),
            nn.Linear(10, 128),
            nn.Hardtanh(),
            nn.Linear(128,3),
            nn.Sigmoid(),
        )
        self.mask_maker = nn.Sequential(nn.Linear(3,1),nn.Sigmoid())
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor(0).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            masker = masker.reshape([1,H,W])
            if self.train:
                losser = torch.nn.MSELoss()
                #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
                color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
               
            images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            print("color_loss",color_loss.shape)
            color_loss = np.float32(color_loss[0])
            ret["color_loss"] = color_loss
        return ret
49/12:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-1}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
49/13:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.1_"+str(i)+"_"+str(losses.item())+".torch")
49/14:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 10),
            nn.Hardtanh(),
            nn.Linear(10, 128),
            nn.Hardtanh(),
            nn.Linear(128,3),
            nn.Sigmoid(),
        )
        self.mask_maker = nn.Sequential(nn.Linear(3,1),nn.Sigmoid())
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor(0).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            masker = masker.reshape([1,H,W])
            if self.train:
                losser = torch.nn.MSELoss()
                #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
                color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
               
            images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            print("color_loss",color_loss.shape)
            color_loss = np.float32(color_loss.sum())
            ret["color_loss"] = color_loss
        return ret
49/15:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-1}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
49/16:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.1_"+str(i)+"_"+str(losses.item())+".torch")
50/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
50/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
50/3:
instance_toimage = []

# for im_name in os.listdir("./dataset/images"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("./dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
50/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
50/5:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 10),
            nn.Hardtanh(),
            nn.Linear(10, 128),
            nn.Hardtanh(),
            nn.Linear(128,3),
            nn.Sigmoid(),
        )
        self.mask_maker = nn.Sequential(nn.Linear(3,1),nn.Sigmoid())
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor(0).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            masker = masker.reshape([1,H,W])
            if self.train:
                losser = torch.nn.MSELoss()
                #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
                color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
               
            images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            print("color_loss",color_loss.shape)
            color_loss = np.float32(color_loss.sum())
            ret["color_loss"] = color_loss
        return ret
50/6:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-1}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
50/7:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.1_"+str(i)+"_"+str(losses.item())+".torch")
50/8:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 10),
            nn.Hardtanh(),
            nn.Linear(10, 128),
            nn.Hardtanh(),
            nn.Linear(128,3),
            nn.Sigmoid(),
        )
        self.mask_maker = nn.Sequential(nn.Linear(3,1),nn.Sigmoid())
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor(0).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            masker = masker.reshape([1,H,W])
            if self.train:
                losser = torch.nn.MSELoss()
                #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
                color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
               
            images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            print("color_loss",color_loss.shape)
            color_loss = torch.float(color_loss.sum())
            ret["color_loss"] = color_loss
        return ret
50/9:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-1}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
50/10:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.1_"+str(i)+"_"+str(losses.item())+".torch")
50/11:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 10),
            nn.Hardtanh(),
            nn.Linear(10, 128),
            nn.Hardtanh(),
            nn.Linear(128,3),
            nn.Sigmoid(),
        )
        self.mask_maker = nn.Sequential(nn.Linear(3,1),nn.Sigmoid())
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor(0).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            masker = masker.reshape([1,H,W])
            if self.train:
                losser = torch.nn.MSELoss()
                #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
                color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
               
            images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            print("color_loss",color_loss.shape)
            color_loss = torch.float16(color_loss.sum())
            ret["color_loss"] = color_loss
        return ret
50/12:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-1}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
50/13:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.1_"+str(i)+"_"+str(losses.item())+".torch")
51/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
51/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
51/3:
instance_toimage = []

# for im_name in os.listdir("./dataset/images"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("./dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
51/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
51/5:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 10),
            nn.Hardtanh(),
            nn.Linear(10, 128),
            nn.Hardtanh(),
            nn.Linear(128,3),
            nn.Sigmoid(),
        )
        self.mask_maker = nn.Sequential(nn.Linear(3,1),nn.Sigmoid())
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor(0).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            masker = masker.reshape([1,H,W])
            if self.train:
                losser = torch.nn.MSELoss()
                #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
                color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
               
            images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            print("color_loss",color_loss.shape)
            color_loss = torch.float16(color_loss.sum())
            ret["color_loss"] = color_loss
        return ret
51/6:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-1}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
51/7:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.1_"+str(i)+"_"+str(losses.item())+".torch")
51/8:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 10),
            nn.Hardtanh(),
            nn.Linear(10, 128),
            nn.Hardtanh(),
            nn.Linear(128,3),
            nn.Sigmoid(),
        )
        self.mask_maker = nn.Sequential(nn.Linear(3,1),nn.Sigmoid())
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor(0).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            masker = masker.reshape([1,H,W])
            if self.train:
                losser = torch.nn.MSELoss()
                #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
                color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
               
            images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            print("color_loss",color_loss.shape)
            color_loss = torch.as_tensor(color_loss,torch.float16)
            color_loss = color_loss.sum()
            ret["color_loss"] = color_loss
        return ret
51/9:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-1}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
51/10:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.1_"+str(i)+"_"+str(losses.item())+".torch")
52/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
52/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
52/3:
instance_toimage = []

# for im_name in os.listdir("./dataset/images"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("./dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
52/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
52/5:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 10),
            nn.Hardtanh(),
            nn.Linear(10, 128),
            nn.Hardtanh(),
            nn.Linear(128,3),
            nn.Sigmoid(),
        )
        self.mask_maker = nn.Sequential(nn.Linear(3,1),nn.Sigmoid())
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor(0).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            masker = masker.reshape([1,H,W])
            if self.train:
                losser = torch.nn.MSELoss()
                #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
                color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
               
            images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            print("color_loss",color_loss.shape)
            color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
            color_loss = color_loss.sum()
            ret["color_loss"] = color_loss
        return ret
52/6:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-1}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
52/7:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.1_"+str(i)+"_"+str(losses.item())+".torch")
52/8:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 10),
            nn.Hardtanh(),
            nn.Linear(10, 128),
            nn.Hardtanh(),
            nn.Linear(128,3),
            nn.Sigmoid(),
        )
        self.mask_maker = nn.Sequential(nn.Linear(3,1),nn.Sigmoid())
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor(0).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            masker = masker.reshape([1,H,W])
            if self.train:
                losser = torch.nn.MSELoss()
                #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
                color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
               
            images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            print("color_loss",color_loss.shape)
            #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
            color_loss = color_loss.sum().float()
            ret["color_loss"] = color_loss
        return ret
52/9:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-1}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
52/10:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.1_"+str(i)+"_"+str(losses.item())+".torch")
52/11:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 10),
            nn.Hardtanh(),
            nn.Linear(10, 128),
            nn.Hardtanh(),
            nn.Linear(128,3),
            nn.Sigmoid(),
        )
        self.mask_maker = nn.Sequential(nn.Linear(3,1),nn.Sigmoid())
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            masker = masker.reshape([1,H,W])
            if self.train:
                losser = torch.nn.MSELoss()
                #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
                color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
               
            images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            print("color_loss",color_loss.shape)
            #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
            color_loss = color_loss.sum().float()
            ret["color_loss"] = color_loss
        return ret
52/12:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-1}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
52/13:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.1_"+str(i)+"_"+str(losses.item())+".torch")
52/14:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 10),
            nn.Hardtanh(),
            nn.Linear(10, 128),
            nn.Hardtanh(),
            nn.Linear(128,3),
            nn.Sigmoid(),
        )
        self.mask_maker = nn.Sequential(nn.Linear(3,1),nn.Sigmoid())
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            masker = masker.reshape([1,H,W])
            if self.train:
                losser = torch.nn.MSELoss()
                #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
                color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
                print(color_loss)
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
               
            images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            print("color_loss",color_loss.shape)
            #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
            color_loss = color_loss.sum().float()
            ret["color_loss"] = color_loss
        return ret
52/15:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-1}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
52/16:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.1_"+str(i)+"_"+str(losses.item())+".torch")
52/17:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 10),
            nn.Hardtanh(),
            nn.Linear(10, 128),
            nn.Hardtanh(),
            nn.Linear(128,3),
            nn.Sigmoid(),
        )
        self.mask_maker = nn.Sequential(nn.Linear(3,1),nn.Sigmoid())
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            masker = masker.reshape([1,H,W])
            if self.train:
                losser = torch.nn.MSELoss()
                #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
                color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
                print(color_loss)
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
               
            images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            print("color_loss",color_loss.shape)
            #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        return ret
52/18:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-1}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
52/19:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.1_"+str(i)+"_"+str(losses.item())+".torch")
54/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
54/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
54/3:
instance_toimage = []

# for im_name in os.listdir("./dataset/images"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("./dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
54/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
54/5:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 10),
            nn.Hardtanh(),
            nn.Linear(10, 128),
            nn.Hardtanh(),
            nn.Linear(128,3),
            nn.Sigmoid(),
        )
        self.mask_maker = nn.Sequential(nn.Linear(3,1),nn.Sigmoid())
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            masker = masker.reshape([1,H,W])
            if self.train:
                losser = torch.nn.MSELoss()
                #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
                color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
                print(color_loss)
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
               
            images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            print("color_loss",color_loss.shape)
            #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        return ret
54/6:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-1}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
54/7:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.1_"+str(i)+"_"+str(losses.item())+".torch")
55/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
55/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
55/3:
instance_toimage = []

# for im_name in os.listdir("./dataset/images"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("./dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
55/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
55/5:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 10),
            nn.Hardtanh(),
            nn.Linear(10, 20),
            nn.Hardtanh(),
            nn.Linear(20,3),
            nn.Sigmoid(),
        )
        self.mask_maker = nn.Sequential(nn.Linear(3,1),nn.Sigmoid())
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            #masker = masker.reshape([1,H,W])
            # if self.train:
            #     losser = torch.nn.MSELoss()
            #     #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
            #     color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #     print(color_loss)
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
               
            images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        # if self.train:
        #     print("color_loss",color_loss.shape)
        #     #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        return ret
55/6:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-1}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
55/7:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.1_"+str(i)+"_"+str(losses.item())+".torch")
55/8:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.1_"+str(i)+"_"+str(losses.item())+".torch")
56/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
56/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
56/3:
instance_toimage = []

# for im_name in os.listdir("./dataset/images"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("./dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
56/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
56/5:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 48),
            nn.Hardtanh(),
            nn.Linear(48, 3),
            nn.Sigmoid(),
        )
        self.mask_maker = nn.Sequential(nn.Linear(3,1),nn.Sigmoid())
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            #masker = masker.reshape([1,H,W])
            # if self.train:
            #     losser = torch.nn.MSELoss()
            #     #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
            #     color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #     print(color_loss)
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
               
            images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        # if self.train:
        #     print("color_loss",color_loss.shape)
        #     #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        return ret
56/6:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-2}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
56/7:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.1_"+str(i)+"_"+str(losses.item())+".torch")
58/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
58/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
58/3:
instance_toimage = []

# for im_name in os.listdir("./dataset/images"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("./dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
58/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
58/5:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 128,bias=False),
            nn.ReLU(),
            nn.Linear(128, 3,bias=False),
            nn.Sigmoid(),
        )
        self.mask_maker = nn.Sequential(nn.Linear(3,1),nn.Sigmoid())
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            #masker = masker.reshape([1,H,W])
            # if self.train:
            #     losser = torch.nn.MSELoss()
            #     #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
            #     color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #     print(color_loss)
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
               
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        # if self.train:
        #     print("color_loss",color_loss.shape)
        #     #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        return ret
58/6:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-2}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
58/7:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.1_"+str(i)+"_"+str(losses.item())+".torch")
61/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
61/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
61/3:
instance_toimage = []

# for im_name in os.listdir("./dataset/images"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("./dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
61/4:
batchSize = 2
img_siz = [700,700]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    # iaa.Sometimes(
    #     0.5,
    #     #iaa.GaussianBlur(sigma=(0, 0.5))
    # ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(700,patch_stack.shape[2])
        k_h = random.randint(700,patch_stack.shape[1])
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
61/5:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 128,bias=False),
            nn.ReLU(),
            nn.Linear(128, 256),
            nn.ReLU(),
            nn.Linear(256,3,bias=False),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].reshape([H*W,3])
            images[ind] = self.channel_trans(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            
            if self.train:
               sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
               #print(sem_val)
               neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
               color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)+((torch.square(neg_val).sum())**0.5))
               
            images[ind] = (images[ind] + kek)/2
            #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            print("color_loss",color_loss.shape)
            #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
            color_loss = color_loss[0]*0.1
            ret["color_loss"] = color_loss
        return ret
61/6:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-2}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
61/7:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v3_"+str(i)+"_"+str(losses.item())+".torch")
62/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
62/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
62/3:
instance_toimage = []

for im_name in os.listdir("./dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
62/4:
batchSize = 2
img_siz = [700,700]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    # iaa.Sometimes(
    #     0.5,
    #     #iaa.GaussianBlur(sigma=(0, 0.5))
    # ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(700,patch_stack.shape[2])
        k_h = random.randint(700,patch_stack.shape[1])
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
62/5:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 128,bias=False),
            nn.ReLU(),
            nn.Linear(128, 256),
            nn.ReLU(),
            nn.Linear(256,3,bias=False),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].reshape([H*W,3])
            images[ind] = self.channel_trans(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            
            if self.train:
               sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
               #print(sem_val)
               neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
               color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)+((torch.square(neg_val).sum())**0.5))
               
            images[ind] = (images[ind] + kek)/2
            #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            #print("color_loss",color_loss.shape)
            #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
            color_loss = color_loss.sum()
            ret["color_loss"] = color_loss
        return ret
62/6:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-2}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
62/7:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v3_"+str(i)+"_"+str(losses.item())+".torch")
65/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
65/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
65/3:
instance_toimage = []

for im_name in os.listdir("./dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
65/4:
batchSize = 2
img_siz = [700,700]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    # iaa.Sometimes(
    #     0.5,
    #     #iaa.GaussianBlur(sigma=(0, 0.5))
    # ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(700,patch_stack.shape[2])
        k_h = random.randint(700,patch_stack.shape[1])
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
65/5:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 128,bias=False),
            nn.ReLU(),
            nn.Linear(128, 256),
            nn.ReLU(),
            nn.Linear(256,3,bias=False),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].reshape([H*W,3])
            images[ind] = self.channel_trans(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            
            if self.train:
               sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
               #print(sem_val)
               neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
               color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)+((torch.square(neg_val).sum())**0.5))
               
            images[ind] = (images[ind] + kek)/2
            #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            #print("color_loss",color_loss.shape)
            #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
            color_loss = color_loss.sum()
            ret["color_loss"] = color_loss
        return ret
65/6:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-2}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
65/7:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v3_"+str(i)+"_"+str(losses.item())+".torch")
67/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
67/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
67/3:
instance_toimage = []

for im_name in os.listdir("./dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
67/4:
batchSize = 2
img_siz = [700,700]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    # iaa.Sometimes(
    #     0.5,
    #     #iaa.GaussianBlur(sigma=(0, 0.5))
    # ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(700,patch_stack.shape[2])
        k_h = random.randint(700,patch_stack.shape[1])
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
67/5:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 9,bias=False),
            nn.ReLU(),
            nn.Linear(9,128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128,3,bias=False),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].reshape([H*W,3])
            images[ind] = self.channel_trans(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            
            if self.train:
               sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
               #print(sem_val)
               neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
               color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5)+5)
               #color_loss = neg_val - sem_val
            images[ind] = (images[ind] + kek)/2
            #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            #print("color_loss",color_loss.shape)
            #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
            color_loss = color_loss.sum()
            ret["color_loss"] = color_loss
        return ret
67/6:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-2}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
67/7:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v3_"+str(i)+"_"+str(losses.item())+".torch")
68/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
68/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
68/3:
instance_toimage = []

for im_name in os.listdir("./dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
68/4:
batchSize = 2
img_siz = [700,700]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    # iaa.Sometimes(
    #     0.5,
    #     #iaa.GaussianBlur(sigma=(0, 0.5))
    # ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(700,patch_stack.shape[2])
        k_h = random.randint(700,patch_stack.shape[1])
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
68/5:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 9,bias=False),
            nn.ReLU(),
            nn.Linear(9,128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128,3,bias=False),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].reshape([H*W,3])
            images[ind] = self.channel_trans(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            
            if self.train:
               sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
               #print(sem_val)
               neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
               color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5)+5)
               #color_loss = neg_val - sem_val
            images[ind] = (images[ind] + kek)/2
            #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            #print("color_loss",color_loss.shape)
            #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
            color_loss = color_loss.sum()
            ret["color_loss"] = color_loss
        return ret
68/6:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-2}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
68/7:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v3_"+str(i)+"_"+str(losses.item())+".torch")
68/8:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 9,bias=False),
            nn.ReLU(),
            nn.Linear(9,128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128,3,bias=False),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].reshape([H*W,3])
            images[ind] = self.channel_trans(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            
            if self.train:
               sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
               print(sem_val)
               neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
               print("semval",sem_val)
               print("neg_val",neg_val)
               color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5)+5)
               print("top",torch.mul(sem_val,neg_val).sum())
               print("bot",(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5)+5))
               #color_loss = neg_val - sem_val
            images[ind] = (images[ind] + kek)/2
            #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            #print("color_loss",color_loss.shape)
            #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
            color_loss = color_loss.sum()
            ret["color_loss"] = color_loss
        return ret
68/9:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-2}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
68/10:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v3_"+str(i)+"_"+str(losses.item())+".torch")
68/11:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 9,bias=False),
            nn.ReLU(),
            nn.Linear(9,128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128,3,bias=False),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].reshape([H*W,3])
            images[ind] = self.channel_trans(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            
            if self.train:
               sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))/targets[ind]["sem_mask_c"]
               print(sem_val)
               neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))/targets[ind]["neg_mask_c"]
               print("semval",sem_val)
               print("neg_val",neg_val)
               color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5)+5)
               print("top",torch.mul(sem_val,neg_val).sum())
               print("bot",(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5)+5))
               color_loss = sem_val
               #color_loss = neg_val - sem_val
            images[ind] = (images[ind] + kek)/2
            #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            #print("color_loss",color_loss.shape)
            #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
            color_loss = color_loss.sum()
            ret["color_loss"] = color_loss
        return ret
68/12:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-2}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
68/13:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v3_"+str(i)+"_"+str(losses.item())+".torch")
68/14:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 9,bias=False),
            nn.ReLU(),
            nn.Linear(9,128),
            nn.ReLU(),
            nn.Linear(128, 128),
            nn.ReLU(),
            nn.Linear(128,3,bias=False),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].reshape([H*W,3])
            images[ind] = self.channel_trans(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            
            if self.train:
               sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))/targets[ind]["sem_mask_c"]
               print(targets[ind]["sem_mask"])
               neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))/targets[ind]["neg_mask_c"]
               print("semval",sem_val)
               print("neg_val",neg_val)
               color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5)+5)
               print("top",torch.mul(sem_val,neg_val).sum())
               print("bot",(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5)+5))
               color_loss = sem_val
               #color_loss = neg_val - sem_val
            images[ind] = (images[ind] + kek)/2
            #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            #print("color_loss",color_loss.shape)
            #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
            color_loss = color_loss.sum()
            ret["color_loss"] = color_loss
        return ret
68/15:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-2}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
68/16:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v3_"+str(i)+"_"+str(losses.item())+".torch")
68/17:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 9,bias=False),
            nn.ReLU(),
            nn.Linear(9,128),
            nn.ReLU(),
            nn.Linear(128,3,bias=False),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].reshape([H*W,3])
            images[ind] = self.channel_trans(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            
            if self.train:
               sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))/targets[ind]["sem_mask_c"]
               print(targets[ind]["sem_mask"])
               neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))/targets[ind]["neg_mask_c"]
               print("semval",sem_val)
               print("neg_val",neg_val)
               color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5)+5)
               print("top",torch.mul(sem_val,neg_val).sum())
               print("bot",(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5)+5))
               color_loss = sem_val
               #color_loss = neg_val - sem_val
            images[ind] = (images[ind] + kek)/2
            #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            #print("color_loss",color_loss.shape)
            #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
            color_loss = color_loss.sum()
            ret["color_loss"] = color_loss
        return ret
68/18:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-2}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
68/19:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v3_"+str(i)+"_"+str(losses.item())+".torch")
68/20:
batchSize = 2
img_siz = [700,700]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    # iaa.Sometimes(
    #     0.5,
    #     #iaa.GaussianBlur(sigma=(0, 0.5))
    # ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(700,patch_stack.shape[2])
        k_h = random.randint(700,patch_stack.shape[1])
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim/255
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks,dtype=np.int16)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
68/21:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 9,bias=False),
            nn.ReLU(),
            nn.Linear(9,128),
            nn.ReLU(),
            nn.Linear(128,3,bias=False),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].reshape([H*W,3])
            images[ind] = self.channel_trans(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            
            if self.train:
               sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))/targets[ind]["sem_mask_c"]
               print(targets[ind]["sem_mask"])
               neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))/targets[ind]["neg_mask_c"]
               print("semval",sem_val)
               print("neg_val",neg_val)
               color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5)+5)
               print("top",torch.mul(sem_val,neg_val).sum())
               print("bot",(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5)+5))
               color_loss = sem_val
               #color_loss = neg_val - sem_val
            images[ind] = (images[ind] + kek)/2
            #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            #print("color_loss",color_loss.shape)
            #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
            color_loss = color_loss.sum()
            ret["color_loss"] = color_loss
        return ret
68/22:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-2}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
68/23:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v3_"+str(i)+"_"+str(losses.item())+".torch")
68/24:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 9,bias=False),
            nn.ReLU(),
            nn.Linear(9,128),
            nn.ReLU(),
            nn.Linear(128,3,bias=False),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].reshape([H*W,3])
            images[ind] = self.channel_trans(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            
            if self.train:
               sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))/targets[ind]["sem_mask_c"]
               print(targets[ind]["sem_mask"] || targets[ind]["neg_mask"])
               neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))/targets[ind]["neg_mask_c"]
               print("semval",sem_val)
               print("neg_val",neg_val)
               color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5)+5)
               print("top",torch.mul(sem_val,neg_val).sum())
               print("bot",(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5)+5))
               color_loss = sem_val
               #color_loss = neg_val - sem_val
            images[ind] = (images[ind] + kek)/2
            #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            #print("color_loss",color_loss.shape)
            #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
            color_loss = color_loss.sum()
            ret["color_loss"] = color_loss
        return ret
68/25:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 9,bias=False),
            nn.ReLU(),
            nn.Linear(9,128),
            nn.ReLU(),
            nn.Linear(128,3,bias=False),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].reshape([H*W,3])
            images[ind] = self.channel_trans(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            
            if self.train:
               sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))/targets[ind]["sem_mask_c"]
               print(targets[ind]["sem_mask"] or targets[ind]["neg_mask"])
               neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))/targets[ind]["neg_mask_c"]
               print("semval",sem_val)
               print("neg_val",neg_val)
               color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5)+5)
               print("top",torch.mul(sem_val,neg_val).sum())
               print("bot",(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5)+5))
               color_loss = sem_val
               #color_loss = neg_val - sem_val
            images[ind] = (images[ind] + kek)/2
            #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            #print("color_loss",color_loss.shape)
            #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
            color_loss = color_loss.sum()
            ret["color_loss"] = color_loss
        return ret
68/26:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-2}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
68/27:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v3_"+str(i)+"_"+str(losses.item())+".torch")
68/28:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 9,bias=False),
            nn.ReLU(),
            nn.Linear(9,128),
            nn.ReLU(),
            nn.Linear(128,3,bias=False),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].reshape([H*W,3])
            images[ind] = self.channel_trans(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            
            if self.train:
               sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))/targets[ind]["sem_mask_c"]
               print(targets[ind]["sem_mask"] + targets[ind]["neg_mask"])
               neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))/targets[ind]["neg_mask_c"]
               print("semval",sem_val)
               print("neg_val",neg_val)
               color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5)+5)
               print("top",torch.mul(sem_val,neg_val).sum())
               print("bot",(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5)+5))
               color_loss = sem_val
               #color_loss = neg_val - sem_val
            images[ind] = (images[ind] + kek)/2
            #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            #print("color_loss",color_loss.shape)
            #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
            color_loss = color_loss.sum()
            ret["color_loss"] = color_loss
        return ret
68/29:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-2}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
68/30:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v3_"+str(i)+"_"+str(losses.item())+".torch")
68/31:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 9,bias=False),
            nn.ReLU(),
            nn.Linear(9,128),
            nn.ReLU(),
            nn.Linear(128,3,bias=False),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].reshape([H*W,3])
            images[ind] = self.channel_trans(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            
            if self.train:
               sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))/targets[ind]["sem_mask_c"]
               #print(targets[ind]["sem_mask"] + targets[ind]["neg_mask"])
               neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))/targets[ind]["neg_mask_c"]
               print("semval",sem_val)
               print("neg_val",neg_val)
               color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5)+5)
               print("top",torch.mul(sem_val,neg_val).sum())
               print("bot",(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5)+5))
               color_loss = sem_val
               #color_loss = neg_val - sem_val
            images[ind] = (images[ind] + kek)/2
            #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            #print("color_loss",color_loss.shape)
            #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
            color_loss = color_loss.sum()
            ret["color_loss"] = color_loss
        return ret
68/32:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-2}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
68/33:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v3_"+str(i)+"_"+str(losses.item())+".torch")
68/34:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 9,bias=False),
            nn.ReLU(),
            nn.Linear(9,128),
            nn.ReLU(),
            nn.Linear(128,3,bias=False),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].reshape([H*W,3])
            images[ind] = self.channel_trans(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            
            if self.train:
               sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))/targets[ind]["sem_mask_c"]
               #print(targets[ind]["sem_mask"] + targets[ind]["neg_mask"])
               neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))/targets[ind]["neg_mask_c"]
               print("semval",sem_val)
               print("neg_val",neg_val)
               color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5)+5*torch.ones(3))
               print("top",torch.mul(sem_val,neg_val).sum())
               print("bot",(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5)+5))
               #color_loss = sem_val
               #color_loss = neg_val - sem_val
            images[ind] = (images[ind] + kek)/2
            #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            #print("color_loss",color_loss.shape)
            #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
            color_loss = color_loss.sum()
            ret["color_loss"] = color_loss
        return ret
68/35:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-2}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
68/36:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v3_"+str(i)+"_"+str(losses.item())+".torch")
68/37:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 9,bias=False),
            nn.ReLU(),
            nn.Linear(9,128),
            nn.ReLU(),
            nn.Linear(128,3,bias=False),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].reshape([H*W,3])
            images[ind] = self.channel_trans(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            
            if self.train:
               sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))/targets[ind]["sem_mask_c"]
               #print(targets[ind]["sem_mask"] + targets[ind]["neg_mask"])
               neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))/targets[ind]["neg_mask_c"]
               print("semval",sem_val)
               print("neg_val",neg_val)
               color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)+((torch.square(neg_val).sum())**0.5))
               print("top",torch.mul(sem_val,neg_val).sum())
               print("bot",(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5)+5))
               #color_loss = sem_val
               #color_loss = neg_val - sem_val
            images[ind] = (images[ind] + kek)/2
            #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            #print("color_loss",color_loss.shape)
            #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
            color_loss = color_loss.sum()
            ret["color_loss"] = color_loss
        return ret
68/38:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-2}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
68/39:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v3_"+str(i)+"_"+str(losses.item())+".torch")
64/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
64/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
64/3:
instance_toimage = []

# for im_name in os.listdir("./dataset/images"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("./dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
64/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
64/5:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 9,bias=False),
            nn.ReLU(),
            nn.Linear(9,128),
            nn.ReLU(),
            nn.Linear(128,3,bias=False),
            nn.Sigmoid(),
        )
        self.mask_maker = nn.Sequential(nn.Linear(3,1),nn.Sigmoid())
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            #masker = masker.reshape([1,H,W])
            # if self.train:
            #     losser = torch.nn.MSELoss()
            #     #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
            #     color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #     print(color_loss)
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            images[ind] = (images[ind] + kek)/2   
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        # if self.train:
        #     print("color_loss",color_loss.shape)
        #     #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        return ret
64/6:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-2}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
64/7:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.1_"+str(i)+".torch")
64/8:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
64/9:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
64/10:
instance_toimage = []

for im_name in os.listdir("./dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
64/11:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
64/12:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 9,bias=False),
            nn.ReLU(),
            nn.Linear(9,128),
            nn.ReLU(),
            nn.Linear(128,3,bias=False),
            nn.Sigmoid(),
        )
        self.mask_maker = nn.Sequential(nn.Linear(3,1),nn.Sigmoid())
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            #masker = masker.reshape([1,H,W])
            # if self.train:
            #     losser = torch.nn.MSELoss()
            #     #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
            #     color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #     print(color_loss)
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            images[ind] = (images[ind] + kek)/2   
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        # if self.train:
        #     print("color_loss",color_loss.shape)
        #     #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        return ret
64/13:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-2}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
64/14:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.1_"+str(i)+".torch")
73/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
73/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
73/3:
instance_toimage = []

for im_name in os.listdir("./dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
73/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
73/5:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 9,bias=False),
            nn.ReLU(),
            nn.Linear(9,128),
            nn.ReLU(),
            nn.Linear(128,3,bias=False),
            nn.Sigmoid(),
        )
        self.mask_maker = nn.Sequential(nn.Linear(3,1),nn.Sigmoid())
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            #masker = masker.reshape([1,H,W])
            # if self.train:
            #     losser = torch.nn.MSELoss()
            #     #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
            #     color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #     print(color_loss)
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            images[ind] = (images[ind] + kek)/2   
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        # if self.train:
        #     print("color_loss",color_loss.shape)
        #     #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        return ret
73/6:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
model.load_state_dict(torch.load('jsr_v2.1_3600.torch'))
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-2}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
73/7:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.2_"+str(i)+".torch")
75/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
75/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
75/3:
instance_toimage = []

for im_name in os.listdir("./dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
75/4:
batchSize = 2
img_siz = [700,700]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    # iaa.Sometimes(
    #     0.5,
    #     #iaa.GaussianBlur(sigma=(0, 0.5))
    # ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(700,patch_stack.shape[2])
        k_h = random.randint(700,patch_stack.shape[1])
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim/255
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks,dtype=np.int16)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
75/5:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 9,bias=False),
            nn.ReLU(),
            nn.Linear(9,128),
            nn.ReLU(),
            nn.Linear(128,3,bias=False),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].reshape([H*W,3])
            images[ind] = self.channel_trans(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            
            if self.train:
               sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))/targets[ind]["sem_mask_c"]
               #print(targets[ind]["sem_mask"] + targets[ind]["neg_mask"])
               neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))/targets[ind]["neg_mask_c"]
               print("semval",sem_val)
               print("neg_val",neg_val)
               color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)+((torch.square(neg_val).sum())**0.5))
               print("top",torch.mul(sem_val,neg_val).sum())
               print("bot",(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5)+5))
               #color_loss = sem_val
               #color_loss = neg_val - sem_val
            images[ind] = (images[ind] + kek)/2
            #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            #print("color_loss",color_loss.shape)
            #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
            color_loss = color_loss.sum()
            ret["color_loss"] = color_loss
        return ret
75/6:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
model.load_state_dict(torch.load('jsr_v3_1400_0.2683873176574707.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-2}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
75/7:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v3.1_"+str(i)+"_"+str(losses.item())+".torch")
76/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
76/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
76/3:
instance_toimage = []

for im_name in os.listdir("./dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
76/4:
batchSize = 2
img_siz = [700,700]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    # iaa.Sometimes(
    #     0.5,
    #     #iaa.GaussianBlur(sigma=(0, 0.5))
    # ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(700,patch_stack.shape[2])
        k_h = random.randint(700,patch_stack.shape[1])
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim/255
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks,dtype=np.int16)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
76/5:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 9,bias=False),
            nn.ReLU(),
            nn.Linear(9,128),
            nn.ReLU(),
            nn.Linear(128,3,bias=False),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].reshape([H*W,3])
            images[ind] = self.channel_trans(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            
            if self.train:
               sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))/targets[ind]["sem_mask_c"]
               #print(targets[ind]["sem_mask"] + targets[ind]["neg_mask"])
               neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))/targets[ind]["neg_mask_c"]
               #print("semval",sem_val)
               #print("neg_val",neg_val)
               color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)+((torch.square(neg_val).sum())**0.5))
               #print("top",torch.mul(sem_val,neg_val).sum())
               #print("bot",(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5)+5))
               #color_loss = sem_val
               #color_loss = neg_val - sem_val
            images[ind] = (images[ind] + kek)/2
            #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            #print("color_loss",color_loss.shape)
            #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
            color_loss = color_loss.sum()
            ret["color_loss"] = color_loss
        return ret
76/6:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
model.load_state_dict(torch.load('jsr_v3_1400_0.2683873176574707.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-2}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
76/7:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v3.1_"+str(i)+"_"+str(losses.item())+".torch")
77/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
77/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
77/3:
instance_toimage = []

for im_name in os.listdir("./dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
77/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
77/5:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 12,bias=False),
            nn.ReLU(),
            nn.Linear(12,128),
            nn.ReLU(),
            nn.Linear(128,3,bias=False),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            #masker = masker.reshape([1,H,W])
            # if self.train:
            #     losser = torch.nn.MSELoss()
            #     #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
            #     color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #     print(color_loss)
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            images[ind] = (images[ind]*0.7 + kek)/1.7  
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        # if self.train:
        #     print("color_loss",color_loss.shape)
        #     #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        return ret
77/6:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load('jsr_v2.1_3600.torch'))
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.SGD([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-2},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-1}], lr=1e-2)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
77/7:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.3_"+str(i)+".torch")
79/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
79/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
79/3:
instance_toimage = []

for im_name in os.listdir("./dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
79/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
79/5:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 12,bias=False),
            nn.ReLU(),
            nn.Linear(12,128),
            nn.ReLU(),
            nn.Linear(128,3,bias=False),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            #masker = masker.reshape([1,H,W])
            # if self.train:
            #     losser = torch.nn.MSELoss()
            #     #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
            #     color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #     print(color_loss)
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            images[ind] = (images[ind]*0.7 + kek)/1.7  
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        # if self.train:
        #     print("color_loss",color_loss.shape)
        #     #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        return ret
79/6:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load('jsr_v2.1_3600.torch'))
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.SGD([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-2},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-1}], lr=1e-2)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
79/7:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200:
                scheduler.step()
            if i%1000==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.3_"+str(i)+".torch")
81/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
from engine import train_one_epoch, evaluate
81/2:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
from engine import train_one_epoch, evaluate
83/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
from engine import train_one_epoch, evaluate
83/2:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
from engine import train_one_epoch, evaluate
83/3:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
83/4:
instance_toimage = []

for im_name in os.listdir("./dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
83/5:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
83/6:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 12,bias=False),
            nn.ReLU(),
            nn.Linear(12,128),
            nn.ReLU(),
            nn.Linear(128,3,bias=False),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            #masker = masker.reshape([1,H,W])
            # if self.train:
            #     losser = torch.nn.MSELoss()
            #     #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
            #     color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #     print(color_loss)
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            images[ind] = (images[ind]*0.7 + kek)/1.7  
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        # if self.train:
        #     print("color_loss",color_loss.shape)
        #     #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        return ret
83/7:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("./dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("./dataset/images/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
84/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
from engine import train_one_epoch, evaluate
84/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
84/3:
instance_toimage = []

for im_name in os.listdir("./dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
84/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
84/5:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 12,bias=False),
            nn.ReLU(),
            nn.Linear(12,128),
            nn.ReLU(),
            nn.Linear(128,3,bias=False),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            #masker = masker.reshape([1,H,W])
            # if self.train:
            #     losser = torch.nn.MSELoss()
            #     #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
            #     color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #     print(color_loss)
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            images[ind] = (images[ind]*0.7 + kek)/1.7  
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        # if self.train:
        #     print("color_loss",color_loss.shape)
        #     #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        return ret
84/6:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("./dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("./dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
84/7:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-2}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
84/8:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%50:
                evaluate(model, test_dataloader, device='cuda:0')
            if i%200:
                scheduler.step()
            if i%1000==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.3_"+str(i)+".torch")
85/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
from engine import train_one_epoch, evaluate
85/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
85/3:
instance_toimage = []

for im_name in os.listdir("./dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
85/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
85/5:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 12,bias=False),
            nn.ReLU(),
            nn.Linear(12,128),
            nn.ReLU(),
            nn.Linear(128,3,bias=False),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            #masker = masker.reshape([1,H,W])
            # if self.train:
            #     losser = torch.nn.MSELoss()
            #     #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
            #     color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #     print(color_loss)
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            images[ind] = (images[ind]*0.7 + kek)/1.7  
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        # if self.train:
        #     print("color_loss",color_loss.shape)
        #     #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        return ret
85/6:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("./dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("./dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
85/7:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-2}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
85/8:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%50:
                evaluate(model, test_dataloader, device='cuda:0')
            if i%200:
                scheduler.step()
            if i%1000==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.3_"+str(i)+".torch")
85/9:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%50:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
            if i%200:
                scheduler.step()
            if i%1000==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.3_"+str(i)+".torch")
86/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
from engine import train_one_epoch, evaluate
86/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
86/3:
instance_toimage = []

for im_name in os.listdir("./dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
86/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
86/5:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 12,bias=False),
            nn.ReLU(),
            nn.Linear(12,128),
            nn.ReLU(),
            nn.Linear(128,3,bias=False),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            #masker = masker.reshape([1,H,W])
            # if self.train:
            #     losser = torch.nn.MSELoss()
            #     #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
            #     color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #     print(color_loss)
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            images[ind] = (images[ind]*0.7 + kek)/1.7  
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        # if self.train:
        #     print("color_loss",color_loss.shape)
        #     #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        return ret
86/6:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("./dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("./dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
86/7:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-2}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
86/8:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%50:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
            if i%200:
                scheduler.step()
            if i%1000==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.3_"+str(i)+".torch")
87/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
from engine import train_one_epoch, evaluate
87/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
87/3:
instance_toimage = []

for im_name in os.listdir("./dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
87/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
87/5:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 12,bias=False),
            nn.ReLU(),
            nn.Linear(12,128),
            nn.ReLU(),
            nn.Linear(128,3,bias=False),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            #masker = masker.reshape([1,H,W])
            # if self.train:
            #     losser = torch.nn.MSELoss()
            #     #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
            #     color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #     print(color_loss)
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            images[ind] = (images[ind]*0.7 + kek)/1.7  
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        # if self.train:
        #     print("color_loss",color_loss.shape)
        #     #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        return ret
87/6:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("./dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("./dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
87/7:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-2}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
87/8:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%50==0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
            if i%200==0:
                scheduler.step()
            if i%1000==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.3_"+str(i)+".torch")
89/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
from engine import train_one_epoch, evaluate
89/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
89/3:
instance_toimage = []

for im_name in os.listdir("./dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
89/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
89/5:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 12,bias=False),
            nn.ReLU(),
            nn.Linear(12,128),
            nn.ReLU(),
            nn.Linear(128,3,bias=False),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            #masker = masker.reshape([1,H,W])
            # if self.train:
            #     losser = torch.nn.MSELoss()
            #     #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
            #     color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #     print(color_loss)
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            images[ind] = (images[ind]*0.7 + kek)/1.7  
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        # if self.train:
        #     print("color_loss",color_loss.shape)
        #     #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        return ret
89/6:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("./dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("./dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
89/7:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
89/8:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%50==0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
            if i%200==0:
                scheduler.step()
            if i%100==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.4_"+str(i)+".torch")
90/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
from engine import train_one_epoch, evaluate
90/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
90/3:
instance_toimage = []

for im_name in os.listdir("./dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
90/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
90/5:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 12,bias=False),
            nn.ReLU(),
            nn.Linear(12,128),
            nn.ReLU(),
            nn.Linear(128,3,bias=False),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            print(images[ind][:,0,0])
            images[ind] = images[ind].reshape([H*W,3])
            print(images[ind][0])
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            
            #masker = masker.reshape([1,H,W])
            # if self.train:
            #     losser = torch.nn.MSELoss()
            #     #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
            #     color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #     print(color_loss)
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            images[ind] = (images[ind]*0.7 + kek)/1.7  
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        # if self.train:
        #     print("color_loss",color_loss.shape)
        #     #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        return ret
90/6:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("./dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("./dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
90/7:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
90/8:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%50==0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
            if i%200==0:
                scheduler.step()
            if i%100==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.4_"+str(i)+".torch")
90/9:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 12,bias=False),
            nn.ReLU(),
            nn.Linear(12,128),
            nn.ReLU(),
            nn.Linear(128,3,bias=False),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            print(images[ind][:,0,0].shape)
            print(images[ind][:,0,0])
            images[ind] = images[ind].reshape([H*W,3])
            print(images[ind][0])
            print(images[ind][0].shape)
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            
            #masker = masker.reshape([1,H,W])
            # if self.train:
            #     losser = torch.nn.MSELoss()
            #     #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
            #     color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #     print(color_loss)
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            images[ind] = (images[ind]*0.7 + kek)/1.7  
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        # if self.train:
        #     print("color_loss",color_loss.shape)
        #     #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        return ret
90/10:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("./dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("./dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
90/11:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
90/12:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%50==0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
            if i%200==0:
                scheduler.step()
            if i%100==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.4_"+str(i)+".torch")
90/13:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 12,bias=False),
            nn.ReLU(),
            nn.Linear(12,128),
            nn.ReLU(),
            nn.Linear(128,3,bias=False),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images = images.permute((1,2,0))
            print(images[ind][:,0,0].shape)
            print(images[ind][:,0,0])
            images[ind] = images[ind].reshape([H*W,3])
            print(images[ind][0])
            print(images[ind][0].shape)
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            
            #masker = masker.reshape([1,H,W])
            # if self.train:
            #     losser = torch.nn.MSELoss()
            #     #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
            #     color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #     print(color_loss)
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            images[ind] = (images[ind]*0.7 + kek)/1.7  
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        # if self.train:
        #     print("color_loss",color_loss.shape)
        #     #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        return ret
90/14:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("./dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("./dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
90/15:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
90/16:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%50==0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
            if i%200==0:
                scheduler.step()
            if i%100==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.4_"+str(i)+".torch")
90/17:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 12,bias=False),
            nn.ReLU(),
            nn.Linear(12,128),
            nn.ReLU(),
            nn.Linear(128,3,bias=False),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images = images[ind].permute((1,2,0))
            print(images[ind][:,0,0].shape)
            print(images[ind][:,0,0])
            images[ind] = images[ind].reshape([H*W,3])
            print(images[ind][0])
            print(images[ind][0].shape)
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            
            #masker = masker.reshape([1,H,W])
            # if self.train:
            #     losser = torch.nn.MSELoss()
            #     #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
            #     color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #     print(color_loss)
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            images[ind] = (images[ind]*0.7 + kek)/1.7  
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        # if self.train:
        #     print("color_loss",color_loss.shape)
        #     #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        return ret
90/18:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("./dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("./dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
90/19:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
90/20:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%50==0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
            if i%200==0:
                scheduler.step()
            if i%100==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.4_"+str(i)+".torch")
90/21:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 12,bias=False),
            nn.ReLU(),
            nn.Linear(12,128),
            nn.ReLU(),
            nn.Linear(128,3,bias=False),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images = images[ind].permute((1,2,0))
            print(images[ind].shape)
            #print(images[ind][:,0,0])
            images[ind] = images[ind].reshape([H*W,3])
            print(images[ind][0])
            print(images[ind][0].shape)
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            
            #masker = masker.reshape([1,H,W])
            # if self.train:
            #     losser = torch.nn.MSELoss()
            #     #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
            #     color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #     print(color_loss)
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            images[ind] = (images[ind]*0.7 + kek)/1.7  
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        # if self.train:
        #     print("color_loss",color_loss.shape)
        #     #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        return ret
90/22:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("./dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("./dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
90/23:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
90/24:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%50==0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
            if i%200==0:
                scheduler.step()
            if i%100==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.4_"+str(i)+".torch")
90/25:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 12,bias=False),
            nn.ReLU(),
            nn.Linear(12,128),
            nn.ReLU(),
            nn.Linear(128,3,bias=False),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].permute((1,2,0))
            print(images[ind].shape)
            #print(images[ind][:,0,0])
            images[ind] = images[ind].reshape([H*W,3])
            print(images[ind][0])
            print(images[ind][0].shape)
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            
            #masker = masker.reshape([1,H,W])
            # if self.train:
            #     losser = torch.nn.MSELoss()
            #     #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
            #     color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #     print(color_loss)
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            images[ind] = (images[ind]*0.7 + kek)/1.7  
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        # if self.train:
        #     print("color_loss",color_loss.shape)
        #     #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        return ret
90/26:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("./dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("./dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
90/27:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
90/28:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%50==0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
            if i%200==0:
                scheduler.step()
            if i%100==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.4_"+str(i)+".torch")
90/29:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 12,bias=False),
            nn.ReLU(),
            nn.Linear(12,128),
            nn.ReLU(),
            nn.Linear(128,3,bias=False),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].permute((1,2,0))
            print(images[ind].shape)
            print(images[ind][:,0,0])
            images[ind] = images[ind].reshape([H*W,3])
            print(images[ind][0])
            print(images[ind][0].shape)
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            
            #masker = masker.reshape([1,H,W])
            # if self.train:
            #     losser = torch.nn.MSELoss()
            #     #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
            #     color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #     print(color_loss)
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            images[ind] = (images[ind]*0.7 + kek)/1.7  
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        # if self.train:
        #     print("color_loss",color_loss.shape)
        #     #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        return ret
90/30:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("./dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("./dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
90/31:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
90/32:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%50==0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
            if i%200==0:
                scheduler.step()
            if i%100==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.4_"+str(i)+".torch")
90/33:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 12,bias=False),
            nn.ReLU(),
            nn.Linear(12,128),
            nn.ReLU(),
            nn.Linear(128,3,bias=False),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].permute((1,2,0))
            print(images[ind].shape)
            print(images[ind][0,0])
            images[ind] = images[ind].reshape([H*W,3])
            print(images[ind][0])
            print(images[ind][0].shape)
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            
            #masker = masker.reshape([1,H,W])
            # if self.train:
            #     losser = torch.nn.MSELoss()
            #     #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
            #     color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #     print(color_loss)
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            images[ind] = (images[ind]*0.7 + kek)/1.7  
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        # if self.train:
        #     print("color_loss",color_loss.shape)
        #     #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        return ret
90/34:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("./dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("./dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
90/35:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
90/36:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%50==0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
            if i%200==0:
                scheduler.step()
            if i%100==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.4_"+str(i)+".torch")
90/37:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
from engine import train_one_epoch, evaluate
90/38:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
90/39:
instance_toimage = []

for im_name in os.listdir("./dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
90/40:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
90/41:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 12,bias=False),
            nn.ReLU(),
            nn.Linear(12,128),
            nn.ReLU(),
            nn.Linear(128,3,bias=False),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].permute((1,2,0))
            #print(images[ind].shape)
            #print(images[ind][0,0])
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind][0])
            #print(images[ind][0].shape)
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([3,H,W])
            
            #masker = masker.reshape([1,H,W])
            # if self.train:
            #     losser = torch.nn.MSELoss()
            #     #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
            #     color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #     print(color_loss)
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            images[ind] = (images[ind]*0.7 + kek)/1.7  
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        # if self.train:
        #     print("color_loss",color_loss.shape)
        #     #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        return ret
90/42:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("./dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("./dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
90/43:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
90/44:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%50==0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
            if i%200==0:
                scheduler.step()
            if i%100==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v2.4_"+str(i)+".torch")
91/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
from engine import train_one_epoch, evaluate
91/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
91/3:
instance_toimage = []

for im_name in os.listdir("./dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
93/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
from engine import train_one_epoch, evaluate
93/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
93/3:
instance_toimage = []

for im_name in os.listdir("./dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
93/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
93/5:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 12,bias=False),
            nn.ReLU(),
            nn.Linear(12,128),
            nn.ReLU(),
            nn.Linear(128,3,bias=False),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].permute((1,2,0))
            #print(images[ind].shape)
            #print(images[ind][0,0])
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind][0])
            #print(images[ind][0].shape)
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([H,W,3])
            images[ind] = images[ind].permute((2,0,1))
            #masker = masker.reshape([1,H,W])
            # if self.train:
            #     losser = torch.nn.MSELoss()
            #     #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
            #     color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #     print(color_loss)
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            images[ind] = (images[ind]*0.7 + kek)/1.7  
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        # if self.train:
        #     print("color_loss",color_loss.shape)
        #     #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        return ret
93/6:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("./dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("./dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("./dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
93/7:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
93/8:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%50==0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
            if i%200==0:
                scheduler.step()
            if i%100==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v4_"+str(i)+".torch")
97/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
from pycoco.engine import train_one_epoch, evaluate
97/2:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
from pycoco.engine import train_one_epoch, evaluate
97/3:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
from pycoco.engine import train_one_epoch, evaluate
97/4:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
from pycoco.engine import train_one_epoch, evaluate
97/5:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
from pycoco import all
from engine import train_one_epoch, evaluate
97/6:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
from engine import train_one_epoch, evaluate
97/7:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader

import os
path="~/Experimental"
os.chdir(path)


from engine import train_one_epoch, evaluate
97/8:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader

import os
path="/Experimental"
os.chdir(path)


from engine import train_one_epoch, evaluate
97/9:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader

import os
path="/prakharug/home/Experimental"
os.chdir(path)


from engine import train_one_epoch, evaluate
97/10:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(0, '/home/Experimental')
from engine import train_one_epoch, evaluate
97/11:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(0, '/home/Experimental/')
from engine import train_one_epoch, evaluate
97/12:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(0, '/home/Experimental/')
from engine import train_one_epoch, evaluate
97/13:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/Experimental/')
from engine import train_one_epoch, evaluate
98/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/Experimental/')
from engine import train_one_epoch, evaluate
98/2:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/Experimental/')
from engine import train_one_epoch, evaluate
98/3:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/Experimental/')
from engine import train_one_epoch, evaluate
98/4:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
from engine import train_one_epoch, evaluate
98/5:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
from pycoco.engine import train_one_epoch, evaluate
98/6:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
from pycoco.engine import train_one_epoch, evaluate
98/7:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
98/8:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("../dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
98/9:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
98/10:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 12,bias=False),
            nn.ReLU(),
            nn.Linear(12,128),
            nn.ReLU(),
            nn.Linear(128,3,bias=False),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].permute((1,2,0))
            #print(images[ind].shape)
            #print(images[ind][0,0])
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind][0])
            #print(images[ind][0].shape)
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([H,W,3])
            images[ind] = images[ind].permute((2,0,1))
            #masker = masker.reshape([1,H,W])
            # if self.train:
            #     losser = torch.nn.MSELoss()
            #     #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
            #     color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #     print(color_loss)
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            images[ind] = (images[ind]*0.7 + kek)/1.7  
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        # if self.train:
        #     print("color_loss",color_loss.shape)
        #     #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        return ret
98/11:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
98/12:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-2}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
98/13:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%50==0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
            # if i%200==0:
            #     scheduler.step()
            if i%100==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v4_"+str(i)+".torch")
98/14:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-2}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
98/15:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%50==0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
            # if i%200==0:
            #     scheduler.step()
            if i%100==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v4_"+str(i)+".torch")
101/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
from pycoco.engine import train_one_epoch, evaluate
102/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
from pycoco.engine import train_one_epoch, evaluate
102/2:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
from pycoco.engine import train_one_epoch, evaluate
103/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
from pycoco.engine import train_one_epoch, evaluate
103/2:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
104/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
104/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
104/3:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("../dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
104/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
104/5:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 12,bias=False),
            nn.ReLU(),
            nn.Linear(12,128),
            nn.ReLU(),
            nn.Linear(128,3,bias=False),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].permute((1,2,0))
            #print(images[ind].shape)
            #print(images[ind][0,0])
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind][0])
            #print(images[ind][0].shape)
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([H,W,3])
            images[ind] = images[ind].permute((2,0,1))
            #masker = masker.reshape([1,H,W])
            # if self.train:
            #     losser = torch.nn.MSELoss()
            #     #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
            #     color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #     print(color_loss)
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            images[ind] = (images[ind]*0.7 + kek)/1.7  
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        # if self.train:
        #     print("color_loss",color_loss.shape)
        #     #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        return ret
104/6:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
104/7:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-2}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
104/8:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%50==0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
            # if i%200==0:
            #     scheduler.step()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v4_"+str(i)+".torch")
105/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
105/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
105/3:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("../dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
105/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
105/5:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 12,bias=False),
            nn.ReLU(),
            nn.Linear(12,128),
            nn.ReLU(),
            nn.Linear(128,3,bias=False),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].permute((1,2,0))
            #print(images[ind].shape)
            #print(images[ind][0,0])
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind][0])
            #print(images[ind][0].shape)
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([H,W,3])
            images[ind] = images[ind].permute((2,0,1))
            #masker = masker.reshape([1,H,W])
            # if self.train:
            #     losser = torch.nn.MSELoss()
            #     #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
            #     color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #     print(color_loss)
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            images[ind] = (images[ind]*0.7 + kek)/1.7  
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        # if self.train:
        #     print("color_loss",color_loss.shape)
        #     #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        return ret
105/6:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
105/7:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-2}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
105/8:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%500==0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
            # if i%200==0:
            #     scheduler.step()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v4_"+str(i)+".torch")
108/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
108/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
108/3:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("../dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
108/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
108/5:
class jsrcnn_v2(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 12,bias=False),
            nn.Hardtanh(),
            nn.Linear(12,128),
            nn.Hardtanh(),
            nn.Linear(128,3,bias=False),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].permute((1,2,0))
            #print(images[ind].shape)
            #print(images[ind][0,0])
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind][0])
            #print(images[ind][0].shape)
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([H,W,3])
            images[ind] = images[ind].permute((2,0,1))
            #masker = masker.reshape([1,H,W])
            if self.train:
                #losser = torch.nn.MSELoss()
                #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
                #color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
                #print(color_loss)
                sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
                #print(sem_val)
                neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
                color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            images[ind] = (images[ind] + kek)/2  
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            print("color_loss",color_loss.shape)
            #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        return ret
108/6:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
108/7:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-2}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
108/8:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%50==0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
            # if i%200==0:
            #     scheduler.step()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v4.2_"+str(i)+".torch")
109/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
109/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
109/3:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("../dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
109/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
109/5:
class jsrcnn_v2(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 12,bias=False),
            nn.Hardtanh(),
            nn.Linear(12,128),
            nn.Hardtanh(),
            nn.Linear(128,3,bias=False),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].permute((1,2,0))
            #print(images[ind].shape)
            #print(images[ind][0,0])
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind][0])
            #print(images[ind][0].shape)
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([H,W,3])
            images[ind] = images[ind].permute((2,0,1))
            #masker = masker.reshape([1,H,W])
            if self.train:
                #losser = torch.nn.MSELoss()
                #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
                #color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
                #print(color_loss)
                sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
                #print(sem_val)
                neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
                color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            images[ind] = (images[ind] + kek)/2  
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            print("color_loss",color_loss.shape)
            #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
            #color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        return ret
109/6:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
109/7:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-2}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
109/8:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%50==0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
            # if i%200==0:
            #     scheduler.step()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v4.2_"+str(i)+".torch")
110/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
110/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
110/3:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("../dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
110/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
110/5:
class jsrcnn_v2(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 12,bias=False),
            nn.Hardtanh(),
            nn.Linear(12,128),
            nn.Hardtanh(),
            nn.Linear(128,3,bias=False),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].permute((1,2,0))
            #print(images[ind].shape)
            #print(images[ind][0,0])
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind][0])
            #print(images[ind][0].shape)
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([H,W,3])
            images[ind] = images[ind].permute((2,0,1))
            #masker = masker.reshape([1,H,W])
            if self.train:
                #losser = torch.nn.MSELoss()
                #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
                #color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
                #print(color_loss)
                sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
                #print(sem_val)
                neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
                color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            images[ind] = (images[ind] + kek)/2  
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            print("color_loss",color_loss.shape)
            #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
            #color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        return ret
110/6:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)

        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)


        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)

        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c

        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
110/7:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-2}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
110/8:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%50==0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
            # if i%200==0:
            #     scheduler.step()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v4.2_"+str(i)+".torch")
111/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
111/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
111/3:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("../dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
111/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
111/5:
class jsrcnn_v2(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 12,bias=False),
            nn.Hardtanh(),
            nn.Linear(12,128),
            nn.Hardtanh(),
            nn.Linear(128,3,bias=False),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].permute((1,2,0))
            #print(images[ind].shape)
            #print(images[ind][0,0])
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind][0])
            #print(images[ind][0].shape)
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([H,W,3])
            images[ind] = images[ind].permute((2,0,1))
            #masker = masker.reshape([1,H,W])
            if self.train:
                #losser = torch.nn.MSELoss()
                #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
                #color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
                #print(color_loss)
                sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
                #print(sem_val)
                neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
                color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            images[ind] = (images[ind] + kek)/2  
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            print("color_loss",color_loss.shape)
            #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
            #color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        return ret
111/6:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)


        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)

        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c

        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
111/7:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-2}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
111/8:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%50==0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
            # if i%200==0:
            #     scheduler.step()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v4.2_"+str(i)+".torch")
111/9:
class jsrcnn_v2(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 12,bias=False),
            nn.Hardtanh(),
            nn.Linear(12,128),
            nn.Hardtanh(),
            nn.Linear(128,3,bias=False),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].permute((1,2,0))
            #print(images[ind].shape)
            #print(images[ind][0,0])
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind][0])
            #print(images[ind][0].shape)
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([H,W,3])
            images[ind] = images[ind].permute((2,0,1))
            #masker = masker.reshape([1,H,W])
            if self.train:
                #losser = torch.nn.MSELoss()
                #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
                #color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
                #print(color_loss)
                sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))/targets[ind]["sem_mask_c"]
                #print(sem_val)
                neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))/targets[ind]["neg_mask_c"]
                color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            images[ind] = (images[ind] + kek)/2  
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            print("color_loss",color_loss.shape)
            #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
            #color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        return ret
111/10:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)


        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)

        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c

        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
111/11:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-2}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
111/12:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%50==0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
            # if i%200==0:
            #     scheduler.step()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v4.2_"+str(i)+".torch")
111/13:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)


        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)

        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c

        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
111/14:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-2}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
111/15:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%50==0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
            # if i%200==0:
            #     scheduler.step()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v4.2_"+str(i)+".torch")
111/16:
class jsrcnn_v2(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 12,bias=False),
            nn.Hardtanh(),
            nn.Linear(12,128),
            nn.Hardtanh(),
            nn.Linear(128,3,bias=False),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        print(images[ind])
        for ind in range(len(images)):
            print(images[ind])
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].permute((1,2,0))
            #print(images[ind].shape)
            #print(images[ind][0,0])
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind][0])
            #print(images[ind][0].shape)
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([H,W,3])
            images[ind] = images[ind].permute((2,0,1))
            #masker = masker.reshape([1,H,W])
            if self.train:
                #losser = torch.nn.MSELoss()
                #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
                #color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
                #print(color_loss)
                sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))/targets[ind]["sem_mask_c"]
                #print(sem_val)
                neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))/targets[ind]["neg_mask_c"]
                color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            images[ind] = (images[ind] + kek)/2  
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            print("color_loss",color_loss.shape)
            #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
            #color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        return ret
111/17:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)


        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)

        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c

        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
111/18:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-2}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
111/19:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%50==0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
            # if i%200==0:
            #     scheduler.step()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v4.2_"+str(i)+".torch")
112/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
112/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
112/3:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("../dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
112/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
112/5:
class jsrcnn_v2(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 12,bias=False),
            nn.Hardtanh(),
            nn.Linear(12,128),
            nn.Hardtanh(),
            nn.Linear(128,3,bias=False),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        print(images[ind])
        for ind in range(len(images)):
            print(images[ind])
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].permute((1,2,0))
            #print(images[ind].shape)
            #print(images[ind][0,0])
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind][0])
            #print(images[ind][0].shape)
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([H,W,3])
            images[ind] = images[ind].permute((2,0,1))
            #masker = masker.reshape([1,H,W])
            if self.train:
                #losser = torch.nn.MSELoss()
                #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
                #color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
                #print(color_loss)
                sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))/targets[ind]["sem_mask_c"]
                #print(sem_val)
                neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))/targets[ind]["neg_mask_c"]
                color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            images[ind] = (images[ind] + kek)/2  
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            print("color_loss",color_loss.shape)
            #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
            #color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        return ret
112/6:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)


        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)

        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c

        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
112/7:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-2}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
112/8:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%50==0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
            # if i%200==0:
            #     scheduler.step()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v4.2_"+str(i)+".torch")
112/9:
class jsrcnn_v2(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 12,bias=False),
            nn.Hardtanh(),
            nn.Linear(12,128),
            nn.Hardtanh(),
            nn.Linear(128,3,bias=False),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            print(images[ind])
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].permute((1,2,0))
            #print(images[ind].shape)
            #print(images[ind][0,0])
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind][0])
            #print(images[ind][0].shape)
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([H,W,3])
            images[ind] = images[ind].permute((2,0,1))
            #masker = masker.reshape([1,H,W])
            if self.train:
                #losser = torch.nn.MSELoss()
                #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
                #color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
                #print(color_loss)
                sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))/targets[ind]["sem_mask_c"]
                #print(sem_val)
                neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))/targets[ind]["neg_mask_c"]
                color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            images[ind] = (images[ind] + kek)/2  
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            print("color_loss",color_loss.shape)
            #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
            #color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        return ret
112/10:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)


        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)

        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c

        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
112/11:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-2}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
112/12:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%50==0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
            # if i%200==0:
            #     scheduler.step()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v4.2_"+str(i)+".torch")
113/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
113/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
113/3:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("../dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
113/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
113/5:
class jsrcnn_v2(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 12,bias=False),
            nn.Hardtanh(),
            nn.Linear(12,128),
            nn.Hardtanh(),
            nn.Linear(128,3,bias=False),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            print(images[ind])
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].permute((1,2,0))
            #print(images[ind].shape)
            #print(images[ind][0,0])
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind][0])
            #print(images[ind][0].shape)
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([H,W,3])
            images[ind] = images[ind].permute((2,0,1))
            #masker = masker.reshape([1,H,W])
            if self.train:
                #losser = torch.nn.MSELoss()
                #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
                #color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
                #print(color_loss)
                sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))/targets[ind]["sem_mask_c"]
                #print(sem_val)
                neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))/targets[ind]["neg_mask_c"]
                color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            images[ind] = (images[ind] + kek)/2  
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            print("color_loss",color_loss.shape)
            #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
            #color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        return ret
113/6:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)


        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)

        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        return img,data
        
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
113/7:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-2}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
113/8:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            # if i%50==0:
            #     evaluate(model, test_dataloader, device='cuda:0')
            #     model.train()
            # if i%200==0:
            #     scheduler.step()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v4.2_"+str(i)+".torch")
116/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
116/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
116/3:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("../dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
116/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
116/5:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 12,bias=False),
            nn.ReLU(),
            nn.Linear(12,128),
            nn.ReLU(),
            nn.Linear(128,3,bias=False),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].permute((1,2,0))
            #print(images[ind].shape)
            #print(images[ind][0,0])
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind][0])
            #print(images[ind][0].shape)
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([H,W,3])
            images[ind] = images[ind].permute((2,0,1))
            #masker = masker.reshape([1,H,W])
            # if self.train:
            #     losser = torch.nn.MSELoss()
            #     #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
            #     color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #     print(color_loss)
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            images[ind] = (images[ind]*0.7 + kek)/1.7  
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        # if self.train:
        #     print("color_loss",color_loss.shape)
        #     #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        return ret
116/6:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
116/7:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load)
model.load_state_dict(torch.load('jsr_v4_2000.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-2}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
116/8:
test = 2000
for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%500==0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
            # if i%200==0:
            #     scheduler.step()
            if i%200==0 and i!=0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v4_"+str(+test+i)+".torch")
117/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
117/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
117/3:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("../dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
117/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
117/5:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 12,bias=False),
            nn.ReLU(),
            nn.Linear(12,128),
            nn.ReLU(),
            nn.Linear(128,3,bias=False),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].permute((1,2,0))
            #print(images[ind].shape)
            #print(images[ind][0,0])
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind][0])
            #print(images[ind][0].shape)
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([H,W,3])
            images[ind] = images[ind].permute((2,0,1))
            #masker = masker.reshape([1,H,W])
            # if self.train:
            #     losser = torch.nn.MSELoss()
            #     #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
            #     color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #     print(color_loss)
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            images[ind] = (images[ind]*0.7 + kek)/1.7  
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        # if self.train:
        #     print("color_loss",color_loss.shape)
        #     #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        return ret
117/6:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
117/7:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load)
model.load_state_dict(torch.load('jsr_v4_2000.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-5},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
117/8:
test = 2000
for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%500==0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
            # if i%200==0:
            #     scheduler.step()
            if i%500==0 and i!=0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v4_"+str(+test+i)+".torch")
117/9:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load)
model.load_state_dict(torch.load('jsr_v4_2000.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-5},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
117/10:
test = 2000
for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
            # if i%200==0:
            #     scheduler.step()
            if i%500==0 and i!=0:

                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v4_"+str(+test+i)+".torch")
118/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
118/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
118/3:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("../dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
118/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
118/5:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 12,bias=False),
            nn.ReLU(),
            nn.Linear(12,128),
            nn.ReLU(),
            nn.Linear(128,3,bias=False),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].permute((1,2,0))
            #print(images[ind].shape)
            #print(images[ind][0,0])
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind][0])
            #print(images[ind][0].shape)
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([H,W,3])
            images[ind] = images[ind].permute((2,0,1))
            #masker = masker.reshape([1,H,W])
            # if self.train:
            #     losser = torch.nn.MSELoss()
            #     #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
            #     color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #     print(color_loss)
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            images[ind] = (images[ind]*0.7 + kek)/1.7  
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        # if self.train:
        #     print("color_loss",color_loss.shape)
        #     #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        return ret
118/6:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
118/7:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load)
model.load_state_dict(torch.load('jsr_v4_2000.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-5},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
118/8:
test = 2000
for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
            # if i%200==0:
            #     scheduler.step()
            if i%500==0 and i!=0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v4_"+str(+test+i)+".torch")
115/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
115/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
115/3:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("../dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
115/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
115/5:
class jsrcnn_v2(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 12,bias=False),
            nn.Hardtanh(),
            nn.Linear(12,128),
            nn.Hardtanh(),
            nn.Linear(128,3,bias=False),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind])
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].permute((1,2,0))
            #print(images[ind].shape)
            #print(images[ind][0,0])
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind][0])
            #print(images[ind][0].shape)
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([H,W,3])
            images[ind] = images[ind].permute((2,0,1))
            #masker = masker.reshape([1,H,W])
            if self.train:
                #losser = torch.nn.MSELoss()
                #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
                #color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
                #print(color_loss)
                sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))/targets[ind]["sem_mask_c"]
                #print(sem_val)
                neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))/targets[ind]["neg_mask_c"]
                color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            images[ind] = (images[ind] + kek)/2  
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        if self.train:
            #print("color_loss",color_loss.shape)
            #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
            #color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        return ret
115/6:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)


        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)

        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        return img,data
        
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
115/7:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load)
#model.load_state_dict(torch.load("jsrv15000.torch"))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-2}], lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
115/8:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            # if i%50==0:
            #     evaluate(model, test_dataloader, device='cuda:0')
            #     model.train()
            # if i%200==0:
            #     scheduler.step()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v4.2_"+str(i)+".torch")
115/9:

for i in range(20000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            # if i%50==0:
            #     evaluate(model, test_dataloader, device='cuda:0')
            #     model.train()
            # if i%200==0:
            #     scheduler.step()
            if i%200==0:
                scheduler.step()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v4.2_"+str(i)+".torch")
121/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
121/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
121/3:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

# for im_name in os.listdir("../dataset/test"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)
121/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
121/5:
class jsrcnn_v2(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v2, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(3, 12,bias=False),
            nn.ReLU(),
            nn.Linear(12,128),
            nn.ReLU(),
            nn.Linear(128,3,bias=False),
            nn.Sigmoid(),
        )
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].permute((1,2,0))
            #print(images[ind].shape)
            #print(images[ind][0,0])
            images[ind] = images[ind].reshape([H*W,3])
            #print(images[ind][0])
            #print(images[ind][0].shape)
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([H,W,3])
            images[ind] = images[ind].permute((2,0,1))
            #masker = masker.reshape([1,H,W])
            # if self.train:
            #     losser = torch.nn.MSELoss()
            #     #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
            #     color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #     print(color_loss)
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            #images[ind] = (images[ind]*0.7 + kek)/1.7  
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        # if self.train:
        #     print("color_loss",color_loss.shape)
        #     #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        return ret
121/6:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
121/7:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v2()
model.to(device)
#model.load_state_dict(torch.load)
#model.load_state_dict(torch.load('jsr_v4_2000.torch'))
#optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-5},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
optimizer = torch.optim.AdamW(lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
122/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
122/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
122/3:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

# for im_name in os.listdir("../dataset/test"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)
122/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
122/5:
class jsrcnn_v4(nn.Module):
    def __init__(self) -> None:
        super(jsrcnn_v4, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(12, 12,bias=False),
            nn.ReLU(),
            nn.Linear(12,128,Bias=False),
            nn.ReLU(),
            nn.Linear(128,12,bias=False),
            nn.ReLU(),
            nn.Linear(12,3,Bias=False),
            nn.Sigmoid(),
        )
        self.trans_layer_1 = torch.nn.Conv2d(3,3,(50,50),stride=1,padding='same',padding_mode='replicate')
        self.trans_layer_2 = torch.nn.Conv2d(3,6,(80,80),stride=1,padding='same',padding_mode='replicate')
        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        xconv1 = self.trans_layer_1(images)
        xconv2 = self.trans_layer_2(xconv1)
        torch.cat((images,xconv2,xconv1),1)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].permute((1,2,0))
            #print(images[ind].shape)
            #print(images[ind][0,0])
            images[ind] = images[ind].reshape([H*W,12])
            #print(images[ind][0])
            #print(images[ind][0].shape)
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([H,W,12])
            images[ind] = images[ind].permute((2,0,1))
            #masker = masker.reshape([1,H,W])
            # if self.train:
            #     losser = torch.nn.MSELoss()
            #     #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
            #     color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #     print(color_loss)
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            #images[ind] = (images[ind]*0.7 + kek)/1.7  
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        # if self.train:
        #     print("color_loss",color_loss.shape)
        #     #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        return ret
122/6:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
122/7:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

from symbol import parameters


model = jsrcnn_v4( box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load)
#model.load_state_dict(torch.load('jsr_v4_2000.torch'))
#optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-5},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
122/8:
class jsrcnn_v4(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v4, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(12, 12,bias=False),
            nn.ReLU(),
            nn.Linear(12,128,Bias=False),
            nn.ReLU(),
            nn.Linear(128,12,bias=False),
            nn.ReLU(),
            nn.Linear(12,3,Bias=False),
            nn.Sigmoid(),
        )
        self.trans_layer_1 = torch.nn.Conv2d(3,3,(50,50),stride=1,padding='same',padding_mode='replicate')
        self.trans_layer_2 = torch.nn.Conv2d(3,6,(80,80),stride=1,padding='same',padding_mode='replicate')
        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2,**kwargs)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        xconv1 = self.trans_layer_1(images)
        xconv2 = self.trans_layer_2(xconv1)
        torch.cat((images,xconv2,xconv1),1)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].permute((1,2,0))
            #print(images[ind].shape)
            #print(images[ind][0,0])
            images[ind] = images[ind].reshape([H*W,12])
            #print(images[ind][0])
            #print(images[ind][0].shape)
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([H,W,12])
            images[ind] = images[ind].permute((2,0,1))
            #masker = masker.reshape([1,H,W])
            # if self.train:
            #     losser = torch.nn.MSELoss()
            #     #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
            #     color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #     print(color_loss)
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            #images[ind] = (images[ind]*0.7 + kek)/1.7  
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        # if self.train:
        #     print("color_loss",color_loss.shape)
        #     #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        return ret
122/9:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
122/10:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

from symbol import parameters


model = jsrcnn_v4( box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load)
#model.load_state_dict(torch.load('jsr_v4_2000.torch'))
#optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-5},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
123/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
123/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
123/3:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

# for im_name in os.listdir("../dataset/test"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)
123/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
123/5:
class jsrcnn_v4(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v4, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(12, 12,bias=False),
            nn.ReLU(),
            nn.Linear(12,128,bias=False),
            nn.ReLU(),
            nn.Linear(128,12,bias=False),
            nn.ReLU(),
            nn.Linear(12,3,bias=False),
            nn.Sigmoid(),
        )
        self.trans_layer_1 = torch.nn.Conv2d(3,3,(50,50),stride=1,padding='same',padding_mode='replicate')
        self.trans_layer_2 = torch.nn.Conv2d(3,6,(80,80),stride=1,padding='same',padding_mode='replicate')
        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2,**kwargs)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        xconv1 = self.trans_layer_1(images)
        xconv2 = self.trans_layer_2(xconv1)
        torch.cat((images,xconv2,xconv1),1)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].permute((1,2,0))
            #print(images[ind].shape)
            #print(images[ind][0,0])
            images[ind] = images[ind].reshape([H*W,12])
            #print(images[ind][0])
            #print(images[ind][0].shape)
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([H,W,12])
            images[ind] = images[ind].permute((2,0,1))
            #masker = masker.reshape([1,H,W])
            # if self.train:
            #     losser = torch.nn.MSELoss()
            #     #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
            #     color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #     print(color_loss)
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            #images[ind] = (images[ind]*0.7 + kek)/1.7  
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        # if self.train:
        #     print("color_loss",color_loss.shape)
        #     #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        return ret
123/6:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
123/7:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

from symbol import parameters


model = jsrcnn_v4( box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load)
#model.load_state_dict(torch.load('jsr_v4_2000.torch'))
#optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-5},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
123/8:
class jsrcnn_v4(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v4, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(12, 12,bias=False),
            nn.ReLU(),
            nn.Linear(12,128,bias=False),
            nn.ReLU(),
            nn.Linear(128,12,bias=False),
            nn.ReLU(),
            nn.Linear(12,3,bias=False),
            nn.Sigmoid(),
        )
        self.trans_layer_1 = torch.nn.Conv2d(3,3,(50,50),stride=1,padding='same',padding_mode='replicate')
        self.trans_layer_2 = torch.nn.Conv2d(3,6,(80,80),stride=1,padding='same',padding_mode='replicate')
        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2,**kwargs)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        xconv1 = self.trans_layer_1(images)
        xconv2 = self.trans_layer_2(xconv1)
        torch.cat((images,xconv2,xconv1),1)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].permute((1,2,0))
            #print(images[ind].shape)
            #print(images[ind][0,0])
            images[ind] = images[ind].reshape([H*W,12])
            #print(images[ind][0])
            #print(images[ind][0].shape)
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([H,W,12])
            images[ind] = images[ind].permute((2,0,1))
            #masker = masker.reshape([1,H,W])
            # if self.train:
            #     losser = torch.nn.MSELoss()
            #     #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
            #     color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #     print(color_loss)
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            #images[ind] = (images[ind]*0.7 + kek)/1.7  
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        # if self.train:
        #     print("color_loss",color_loss.shape)
        #     #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        return ret
123/9:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
123/10:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

from symbol import parameters


model = jsrcnn_v4( box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load)
#model.load_state_dict(torch.load('jsr_v4_2000.torch'))
#optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-5},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
123/11:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v4(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load)
#model.load_state_dict(torch.load('jsr_v4_2000.torch'))
#optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-5},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
123/12:
class jsrcnn_v4(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v4, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(12, 12,bias=False),
            nn.ReLU(),
            nn.Linear(12,128,bias=False),
            nn.ReLU(),
            nn.Linear(128,12,bias=False),
            nn.ReLU(),
            nn.Linear(12,3,bias=False),
            nn.Sigmoid(),
        )
        self.trans_layer_1 = torch.nn.Conv2d(3,3,(50,50),stride=1,padding='same',padding_mode='replicate')
        self.trans_layer_2 = torch.nn.Conv2d(3,6,(80,80),stride=1,padding='same',padding_mode='replicate')
        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        xconv1 = self.trans_layer_1(images)
        xconv2 = self.trans_layer_2(xconv1)
        torch.cat((images,xconv2,xconv1),1)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].permute((1,2,0))
            #print(images[ind].shape)
            #print(images[ind][0,0])
            images[ind] = images[ind].reshape([H*W,12])
            #print(images[ind][0])
            #print(images[ind][0].shape)
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([H,W,12])
            images[ind] = images[ind].permute((2,0,1))
            #masker = masker.reshape([1,H,W])
            # if self.train:
            #     losser = torch.nn.MSELoss()
            #     #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
            #     color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #     print(color_loss)
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            #images[ind] = (images[ind]*0.7 + kek)/1.7  
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        # if self.train:
        #     print("color_loss",color_loss.shape)
        #     #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        return ret
123/13:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
123/14:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v4(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load)
#model.load_state_dict(torch.load('jsr_v4_2000.torch'))
#optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-5},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
123/15:
test = 0
for i in range(1000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
            # if i%200==0:
            #     scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v4.4_"+str(+test+i)+".torch")
123/16:
class jsrcnn_v4(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v4, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(12, 12,bias=False),
            nn.ReLU(),
            nn.Linear(12,128,bias=False),
            nn.ReLU(),
            nn.Linear(128,12,bias=False),
            nn.ReLU(),
            nn.Linear(12,3,bias=False),
            nn.Sigmoid(),
        )
        self.trans_layer_1 = torch.nn.Conv2d(3,3,(50,50),stride=1,padding='same',padding_mode='replicate')
        self.trans_layer_2 = torch.nn.Conv2d(3,6,(80,80),stride=1,padding='same',padding_mode='replicate')
        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        images = torch.Tensor(images,device)
        xconv1 = self.trans_layer_1(images)
        xconv2 = self.trans_layer_2(xconv1)
        torch.cat((images,xconv2,xconv1),1)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].permute((1,2,0))
            #print(images[ind].shape)
            #print(images[ind][0,0])
            images[ind] = images[ind].reshape([H*W,12])
            #print(images[ind][0])
            #print(images[ind][0].shape)
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([H,W,12])
            images[ind] = images[ind].permute((2,0,1))
            #masker = masker.reshape([1,H,W])
            # if self.train:
            #     losser = torch.nn.MSELoss()
            #     #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
            #     color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #     print(color_loss)
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            #images[ind] = (images[ind]*0.7 + kek)/1.7  
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        # if self.train:
        #     print("color_loss",color_loss.shape)
        #     #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        return ret
123/17:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
123/18:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v4(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load)
#model.load_state_dict(torch.load('jsr_v4_2000.torch'))
#optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-5},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
123/19:
test = 0
for i in range(1000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
            # if i%200==0:
            #     scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v4.4_"+str(+test+i)+".torch")
123/20:
class jsrcnn_v4(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v4, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(12, 12,bias=False),
            nn.ReLU(),
            nn.Linear(12,128,bias=False),
            nn.ReLU(),
            nn.Linear(128,12,bias=False),
            nn.ReLU(),
            nn.Linear(12,3,bias=False),
            nn.Sigmoid(),
        )
        self.trans_layer_1 = torch.nn.Conv2d(3,3,(50,50),stride=1,padding='same',padding_mode='replicate')
        self.trans_layer_2 = torch.nn.Conv2d(3,6,(80,80),stride=1,padding='same',padding_mode='replicate')
        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        images = torch.Tensor(images,device=device)
        xconv1 = self.trans_layer_1(images)
        xconv2 = self.trans_layer_2(xconv1)
        torch.cat((images,xconv2,xconv1),1)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].permute((1,2,0))
            #print(images[ind].shape)
            #print(images[ind][0,0])
            images[ind] = images[ind].reshape([H*W,12])
            #print(images[ind][0])
            #print(images[ind][0].shape)
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([H,W,12])
            images[ind] = images[ind].permute((2,0,1))
            #masker = masker.reshape([1,H,W])
            # if self.train:
            #     losser = torch.nn.MSELoss()
            #     #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
            #     color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #     print(color_loss)
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            #images[ind] = (images[ind]*0.7 + kek)/1.7  
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        # if self.train:
        #     print("color_loss",color_loss.shape)
        #     #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        return ret
123/21:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
123/22:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v4(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load)
#model.load_state_dict(torch.load('jsr_v4_2000.torch'))
#optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-5},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
123/23:
test = 0
for i in range(1000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
            # if i%200==0:
            #     scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v4.4_"+str(+test+i)+".torch")
123/24:
class jsrcnn_v4(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v4, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(12, 12,bias=False),
            nn.ReLU(),
            nn.Linear(12,128,bias=False),
            nn.ReLU(),
            nn.Linear(128,12,bias=False),
            nn.ReLU(),
            nn.Linear(12,3,bias=False),
            nn.Sigmoid(),
        )
        self.trans_layer_1 = torch.nn.Conv2d(3,3,(50,50),stride=1,padding='same',padding_mode='replicate')
        self.trans_layer_2 = torch.nn.Conv2d(3,6,(80,80),stride=1,padding='same',padding_mode='replicate')
        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        images = torch.as_tensor(images,device=device)
        xconv1 = self.trans_layer_1(images)
        xconv2 = self.trans_layer_2(xconv1)
        torch.cat((images,xconv2,xconv1),1)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].permute((1,2,0))
            #print(images[ind].shape)
            #print(images[ind][0,0])
            images[ind] = images[ind].reshape([H*W,12])
            #print(images[ind][0])
            #print(images[ind][0].shape)
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([H,W,12])
            images[ind] = images[ind].permute((2,0,1))
            #masker = masker.reshape([1,H,W])
            # if self.train:
            #     losser = torch.nn.MSELoss()
            #     #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
            #     color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #     print(color_loss)
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            #images[ind] = (images[ind]*0.7 + kek)/1.7  
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        # if self.train:
        #     print("color_loss",color_loss.shape)
        #     #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        return ret
123/25:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
123/26:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v4(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load)
#model.load_state_dict(torch.load('jsr_v4_2000.torch'))
#optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-5},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
123/27:
test = 0
for i in range(1000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
            # if i%200==0:
            #     scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v4.4_"+str(+test+i)+".torch")
123/28:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    batch_Imgs = torch.as_tensor(batch_Imgs,device=device)
    return batch_Imgs, batch_Data
#loadBatch()
123/29:
class jsrcnn_v4(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v4, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(12, 12,bias=False),
            nn.ReLU(),
            nn.Linear(12,128,bias=False),
            nn.ReLU(),
            nn.Linear(128,12,bias=False),
            nn.ReLU(),
            nn.Linear(12,3,bias=False),
            nn.Sigmoid(),
        )
        self.trans_layer_1 = torch.nn.Conv2d(3,3,(50,50),stride=1,padding='same',padding_mode='replicate')
        self.trans_layer_2 = torch.nn.Conv2d(3,6,(80,80),stride=1,padding='same',padding_mode='replicate')
        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        xconv1 = self.trans_layer_1(images)
        xconv2 = self.trans_layer_2(xconv1)
        torch.cat((images,xconv2,xconv1),1)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].permute((1,2,0))
            #print(images[ind].shape)
            #print(images[ind][0,0])
            images[ind] = images[ind].reshape([H*W,12])
            #print(images[ind][0])
            #print(images[ind][0].shape)
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([H,W,12])
            images[ind] = images[ind].permute((2,0,1))
            #masker = masker.reshape([1,H,W])
            # if self.train:
            #     losser = torch.nn.MSELoss()
            #     #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
            #     color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #     print(color_loss)
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            #images[ind] = (images[ind]*0.7 + kek)/1.7  
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        # if self.train:
        #     print("color_loss",color_loss.shape)
        #     #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        return ret
123/30:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
123/31:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v4(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load)
#model.load_state_dict(torch.load('jsr_v4_2000.torch'))
#optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-5},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
123/32:
test = 0
for i in range(1000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
            # if i%200==0:
            #     scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v4.4_"+str(+test+i)+".torch")
123/33:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    batch_Imgs = torch.stack(batch_Imgs)
    return batch_Imgs, batch_Data
#loadBatch()
123/34:
class jsrcnn_v4(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v4, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(12, 12,bias=False),
            nn.ReLU(),
            nn.Linear(12,128,bias=False),
            nn.ReLU(),
            nn.Linear(128,12,bias=False),
            nn.ReLU(),
            nn.Linear(12,3,bias=False),
            nn.Sigmoid(),
        )
        self.trans_layer_1 = torch.nn.Conv2d(3,3,(50,50),stride=1,padding='same',padding_mode='replicate')
        self.trans_layer_2 = torch.nn.Conv2d(3,6,(80,80),stride=1,padding='same',padding_mode='replicate')
        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        xconv1 = self.trans_layer_1(images)
        xconv2 = self.trans_layer_2(xconv1)
        torch.cat((images,xconv2,xconv1),1)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].permute((1,2,0))
            #print(images[ind].shape)
            #print(images[ind][0,0])
            images[ind] = images[ind].reshape([H*W,12])
            #print(images[ind][0])
            #print(images[ind][0].shape)
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([H,W,12])
            images[ind] = images[ind].permute((2,0,1))
            #masker = masker.reshape([1,H,W])
            # if self.train:
            #     losser = torch.nn.MSELoss()
            #     #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
            #     color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #     print(color_loss)
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            #images[ind] = (images[ind]*0.7 + kek)/1.7  
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        # if self.train:
        #     print("color_loss",color_loss.shape)
        #     #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        return ret
123/35:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
123/36:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v4(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load)
#model.load_state_dict(torch.load('jsr_v4_2000.torch'))
#optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-5},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
123/37:
test = 0
for i in range(1000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
            # if i%200==0:
            #     scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v4.4_"+str(+test+i)+".torch")
123/38:
class jsrcnn_v4(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v4, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(12, 12,bias=False),
            nn.ReLU(),
            nn.Linear(12,128,bias=False),
            nn.ReLU(),
            nn.Linear(128,12,bias=False),
            nn.ReLU(),
            nn.Linear(12,3,bias=False),
            nn.Sigmoid(),
        )
        self.trans_layer_1 = torch.nn.Conv2d(3,3,(50,50),stride=1,padding='same',padding_mode='replicate')
        self.trans_layer_2 = torch.nn.Conv2d(3,6,(80,80),stride=1,padding='same',padding_mode='replicate')
        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            xconv1 = self.trans_layer_1(images[ind])
            xconv2 = self.trans_layer_2(xconv1[ind])
            torch.cat((images[ind],xconv2,xconv1),out=images[ind])
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].permute((1,2,0))
            #print(images[ind].shape)
            #print(images[ind][0,0])
            images[ind] = images[ind].reshape([H*W,12])
            #print(images[ind][0])
            #print(images[ind][0].shape)
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([H,W,12])
            images[ind] = images[ind].permute((2,0,1))
            #masker = masker.reshape([1,H,W])
            # if self.train:
            #     losser = torch.nn.MSELoss()
            #     #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
            #     color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #     print(color_loss)
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            #images[ind] = (images[ind]*0.7 + kek)/1.7  
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        # if self.train:
        #     print("color_loss",color_loss.shape)
        #     #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        return ret
123/39:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
123/40:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v4(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load)
#model.load_state_dict(torch.load('jsr_v4_2000.torch'))
#optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-5},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
123/41:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
123/42:
class jsrcnn_v4(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v4, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(12, 12,bias=False),
            nn.ReLU(),
            nn.Linear(12,128,bias=False),
            nn.ReLU(),
            nn.Linear(128,12,bias=False),
            nn.ReLU(),
            nn.Linear(12,3,bias=False),
            nn.Sigmoid(),
        )
        self.trans_layer_1 = torch.nn.Conv2d(3,3,(50,50),stride=1,padding='same',padding_mode='replicate')
        self.trans_layer_2 = torch.nn.Conv2d(3,6,(80,80),stride=1,padding='same',padding_mode='replicate')
        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            xconv1 = self.trans_layer_1(images[ind])
            xconv2 = self.trans_layer_2(xconv1[ind])
            torch.cat((images[ind],xconv2,xconv1),out=images[ind])
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].permute((1,2,0))
            #print(images[ind].shape)
            #print(images[ind][0,0])
            images[ind] = images[ind].reshape([H*W,12])
            #print(images[ind][0])
            #print(images[ind][0].shape)
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([H,W,12])
            images[ind] = images[ind].permute((2,0,1))
            #masker = masker.reshape([1,H,W])
            # if self.train:
            #     losser = torch.nn.MSELoss()
            #     #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
            #     color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #     print(color_loss)
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            #images[ind] = (images[ind]*0.7 + kek)/1.7  
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        # if self.train:
        #     print("color_loss",color_loss.shape)
        #     #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        return ret
123/43:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
123/44:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v4(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load)
#model.load_state_dict(torch.load('jsr_v4_2000.torch'))
#optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-5},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
123/45:
test = 0
for i in range(1000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
            # if i%200==0:
            #     scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v4.4_"+str(+test+i)+".torch")
123/46:
class jsrcnn_v4(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v4, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(12, 12,bias=False),
            nn.ReLU(),
            nn.Linear(12,128,bias=False),
            nn.ReLU(),
            nn.Linear(128,12,bias=False),
            nn.ReLU(),
            nn.Linear(12,3,bias=False),
            nn.Sigmoid(),
        )
        self.trans_layer_1 = torch.nn.Conv2d(3,3,(50,50),stride=1,padding='same',padding_mode='replicate')
        self.trans_layer_2 = torch.nn.Conv2d(3,6,(80,80),stride=1,padding='same',padding_mode='replicate')
        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            xconv1 = self.trans_layer_1([torch.stack(images[ind])])
            xconv2 = self.trans_layer_2(xconv1[ind])
            torch.cat((images[ind],xconv2,xconv1),out=images[ind])
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].permute((1,2,0))
            #print(images[ind].shape)
            #print(images[ind][0,0])
            images[ind] = images[ind].reshape([H*W,12])
            #print(images[ind][0])
            #print(images[ind][0].shape)
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([H,W,12])
            images[ind] = images[ind].permute((2,0,1))
            #masker = masker.reshape([1,H,W])
            # if self.train:
            #     losser = torch.nn.MSELoss()
            #     #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
            #     color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #     print(color_loss)
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            #images[ind] = (images[ind]*0.7 + kek)/1.7  
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        # if self.train:
        #     print("color_loss",color_loss.shape)
        #     #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        return ret
123/47:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
123/48:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v4(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load)
#model.load_state_dict(torch.load('jsr_v4_2000.torch'))
#optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-5},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
123/49:
test = 0
for i in range(1000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
            # if i%200==0:
            #     scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v4.4_"+str(+test+i)+".torch")
123/50:
class jsrcnn_v4(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v4, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(12, 12,bias=False),
            nn.ReLU(),
            nn.Linear(12,128,bias=False),
            nn.ReLU(),
            nn.Linear(128,12,bias=False),
            nn.ReLU(),
            nn.Linear(12,3,bias=False),
            nn.Sigmoid(),
        )
        self.trans_layer_1 = torch.nn.Conv2d(3,3,(50,50),stride=1,padding='same',padding_mode='replicate')
        self.trans_layer_2 = torch.nn.Conv2d(3,6,(80,80),stride=1,padding='same',padding_mode='replicate')
        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            xconv1 = self.trans_layer_1([torch.stack(images[ind,])])
            xconv2 = self.trans_layer_2(xconv1[ind])
            torch.cat((images[ind],xconv2,xconv1),out=images[ind])
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].permute((1,2,0))
            #print(images[ind].shape)
            #print(images[ind][0,0])
            images[ind] = images[ind].reshape([H*W,12])
            #print(images[ind][0])
            #print(images[ind][0].shape)
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([H,W,12])
            images[ind] = images[ind].permute((2,0,1))
            #masker = masker.reshape([1,H,W])
            # if self.train:
            #     losser = torch.nn.MSELoss()
            #     #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
            #     color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #     print(color_loss)
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            #images[ind] = (images[ind]*0.7 + kek)/1.7  
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        # if self.train:
        #     print("color_loss",color_loss.shape)
        #     #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        return ret
123/51:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v4(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load)
#model.load_state_dict(torch.load('jsr_v4_2000.torch'))
#optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-5},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
123/52:
test = 0
for i in range(1000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
            # if i%200==0:
            #     scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v4.4_"+str(+test+i)+".torch")
123/53:
class jsrcnn_v4(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v4, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(12, 12,bias=False),
            nn.ReLU(),
            nn.Linear(12,128,bias=False),
            nn.ReLU(),
            nn.Linear(128,12,bias=False),
            nn.ReLU(),
            nn.Linear(12,3,bias=False),
            nn.Sigmoid(),
        )
        self.trans_layer_1 = torch.nn.Conv2d(3,3,(50,50),stride=1,padding='same',padding_mode='replicate')
        self.trans_layer_2 = torch.nn.Conv2d(3,6,(80,80),stride=1,padding='same',padding_mode='replicate')
        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            xconv1 = self.trans_layer_1(torch.stack([images[ind]]))
            xconv2 = self.trans_layer_2(xconv1[ind])
            torch.cat((images[ind],xconv2,xconv1),out=images[ind])
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].permute((1,2,0))
            #print(images[ind].shape)
            #print(images[ind][0,0])
            images[ind] = images[ind].reshape([H*W,12])
            #print(images[ind][0])
            #print(images[ind][0].shape)
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([H,W,12])
            images[ind] = images[ind].permute((2,0,1))
            #masker = masker.reshape([1,H,W])
            # if self.train:
            #     losser = torch.nn.MSELoss()
            #     #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
            #     color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #     print(color_loss)
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            #images[ind] = (images[ind]*0.7 + kek)/1.7  
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        # if self.train:
        #     print("color_loss",color_loss.shape)
        #     #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        return ret
123/54:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v4(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load)
#model.load_state_dict(torch.load('jsr_v4_2000.torch'))
#optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-5},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
123/55:
test = 0
for i in range(1000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
            # if i%200==0:
            #     scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v4.4_"+str(+test+i)+".torch")
123/56:
class jsrcnn_v4(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v4, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(12, 12,bias=False),
            nn.ReLU(),
            nn.Linear(12,128,bias=False),
            nn.ReLU(),
            nn.Linear(128,12,bias=False),
            nn.ReLU(),
            nn.Linear(12,3,bias=False),
            nn.Sigmoid(),
        )
        self.trans_layer_1 = torch.nn.Conv2d(3,3,(50,50),stride=1,padding='same',padding_mode='replicate')
        self.trans_layer_2 = torch.nn.Conv2d(3,6,(80,80),stride=1,padding='same',padding_mode='replicate')
        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            xconv1 = self.trans_layer_1(torch.stack([images[ind]]))
            xconv2 = self.trans_layer_2(xconv1)
            torch.cat((images[ind],xconv2,xconv1),out=images[ind])
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].permute((1,2,0))
            #print(images[ind].shape)
            #print(images[ind][0,0])
            images[ind] = images[ind].reshape([H*W,12])
            #print(images[ind][0])
            #print(images[ind][0].shape)
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([H,W,12])
            images[ind] = images[ind].permute((2,0,1))
            #masker = masker.reshape([1,H,W])
            # if self.train:
            #     losser = torch.nn.MSELoss()
            #     #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
            #     color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #     print(color_loss)
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            #images[ind] = (images[ind]*0.7 + kek)/1.7  
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        # if self.train:
        #     print("color_loss",color_loss.shape)
        #     #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        return ret
123/57:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v4(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load)
#model.load_state_dict(torch.load('jsr_v4_2000.torch'))
#optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-5},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
123/58:
test = 0
for i in range(1000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
            # if i%200==0:
            #     scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v4.4_"+str(+test+i)+".torch")
123/59:
class jsrcnn_v4(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v4, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(12, 12,bias=False),
            nn.ReLU(),
            nn.Linear(12,128,bias=False),
            nn.ReLU(),
            nn.Linear(128,12,bias=False),
            nn.ReLU(),
            nn.Linear(12,3,bias=False),
            nn.Sigmoid(),
        )
        self.trans_layer_1 = torch.nn.Conv2d(3,3,(50,50),stride=1,padding='same',padding_mode='replicate')
        self.trans_layer_2 = torch.nn.Conv2d(3,6,(80,80),stride=1,padding='same',padding_mode='replicate')
        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            xconv1 = self.trans_layer_1(torch.stack([images[ind]]))
            xconv2 = self.trans_layer_2(xconv1)
            images[ind]=torch.cat((images[ind],xconv2,xconv1))
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].permute((1,2,0))
            #print(images[ind].shape)
            #print(images[ind][0,0])
            images[ind] = images[ind].reshape([H*W,12])
            #print(images[ind][0])
            #print(images[ind][0].shape)
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([H,W,12])
            images[ind] = images[ind].permute((2,0,1))
            #masker = masker.reshape([1,H,W])
            # if self.train:
            #     losser = torch.nn.MSELoss()
            #     #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
            #     color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #     print(color_loss)
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            #images[ind] = (images[ind]*0.7 + kek)/1.7  
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        # if self.train:
        #     print("color_loss",color_loss.shape)
        #     #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        return ret
123/60:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v4(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load)
#model.load_state_dict(torch.load('jsr_v4_2000.torch'))
#optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-5},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
123/61:
test = 0
for i in range(1000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
            # if i%200==0:
            #     scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v4.4_"+str(+test+i)+".torch")
123/62:
class jsrcnn_v4(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v4, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(12, 12,bias=False),
            nn.ReLU(),
            nn.Linear(12,128,bias=False),
            nn.ReLU(),
            nn.Linear(128,12,bias=False),
            nn.ReLU(),
            nn.Linear(12,3,bias=False),
            nn.Sigmoid(),
        )
        self.trans_layer_1 = torch.nn.Conv2d(3,3,(50,50),stride=1,padding='same',padding_mode='replicate')
        self.trans_layer_2 = torch.nn.Conv2d(3,6,(80,80),stride=1,padding='same',padding_mode='replicate')
        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            xconv1 = self.trans_layer_1(torch.stack([images[ind]]))
            xconv2 = self.trans_layer_2(xconv1)
            images[ind]=torch.cat((images[ind],xconv2[0],xconv1[0]))
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].permute((1,2,0))
            #print(images[ind].shape)
            #print(images[ind][0,0])
            images[ind] = images[ind].reshape([H*W,12])
            #print(images[ind][0])
            #print(images[ind][0].shape)
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([H,W,12])
            images[ind] = images[ind].permute((2,0,1))
            #masker = masker.reshape([1,H,W])
            # if self.train:
            #     losser = torch.nn.MSELoss()
            #     #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
            #     color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #     print(color_loss)
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            #images[ind] = (images[ind]*0.7 + kek)/1.7  
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        # if self.train:
        #     print("color_loss",color_loss.shape)
        #     #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        return ret
123/63:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v4(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load)
#model.load_state_dict(torch.load('jsr_v4_2000.torch'))
#optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-5},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
123/64:
test = 0
for i in range(1000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
            # if i%200==0:
            #     scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v4.4_"+str(+test+i)+".torch")
123/65:
class jsrcnn_v4(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v4, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(12, 12,bias=False),
            nn.ReLU(),
            nn.Linear(12,128,bias=False),
            nn.ReLU(),
            nn.Linear(128,12,bias=False),
            nn.ReLU(),
            nn.Linear(12,3,bias=False),
            nn.Sigmoid(),
        )
        self.trans_layer_1 = torch.nn.Conv2d(3,3,(50,50),stride=1,padding='same',padding_mode='replicate')
        self.trans_layer_2 = torch.nn.Conv2d(3,6,(80,80),stride=1,padding='same',padding_mode='replicate')
        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            xconv1 = self.trans_layer_1(torch.stack([images[ind]]))
            xconv2 = self.trans_layer_2(xconv1)
            images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]))
            print(images.shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].permute((1,2,0))
            #print(images[ind].shape)
            #print(images[ind][0,0])
            images[ind] = images[ind].reshape([H*W,12])
            #print(images[ind][0])
            #print(images[ind][0].shape)
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([H,W,12])
            images[ind] = images[ind].permute((2,0,1))
            #masker = masker.reshape([1,H,W])
            # if self.train:
            #     losser = torch.nn.MSELoss()
            #     #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
            #     color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #     print(color_loss)
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            #images[ind] = (images[ind]*0.7 + kek)/1.7  
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        # if self.train:
        #     print("color_loss",color_loss.shape)
        #     #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        return ret
123/66:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v4(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load)
#model.load_state_dict(torch.load('jsr_v4_2000.torch'))
#optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-5},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
123/67:
test = 0
for i in range(1000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
            # if i%200==0:
            #     scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v4.4_"+str(+test+i)+".torch")
123/68:
class jsrcnn_v4(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v4, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(12, 12,bias=False),
            nn.ReLU(),
            nn.Linear(12,128,bias=False),
            nn.ReLU(),
            nn.Linear(128,12,bias=False),
            nn.ReLU(),
            nn.Linear(12,3,bias=False),
            nn.Sigmoid(),
        )
        self.trans_layer_1 = torch.nn.Conv2d(3,3,(50,50),stride=1,padding='same',padding_mode='replicate')
        self.trans_layer_2 = torch.nn.Conv2d(3,6,(80,80),stride=1,padding='same',padding_mode='replicate')
        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            xconv1 = self.trans_layer_1(torch.stack([images[ind]]))
            xconv2 = self.trans_layer_2(xconv1)
            images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            print(images.shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].permute((1,2,0))
            #print(images[ind].shape)
            #print(images[ind][0,0])
            images[ind] = images[ind].reshape([H*W,12])
            #print(images[ind][0])
            #print(images[ind][0].shape)
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([H,W,12])
            images[ind] = images[ind].permute((2,0,1))
            #masker = masker.reshape([1,H,W])
            # if self.train:
            #     losser = torch.nn.MSELoss()
            #     #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
            #     color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #     print(color_loss)
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            #images[ind] = (images[ind]*0.7 + kek)/1.7  
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        # if self.train:
        #     print("color_loss",color_loss.shape)
        #     #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        return ret
123/69:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v4(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load)
#model.load_state_dict(torch.load('jsr_v4_2000.torch'))
#optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-5},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
123/70:
test = 0
for i in range(1000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
            # if i%200==0:
            #     scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v4.4_"+str(+test+i)+".torch")
123/71:
class jsrcnn_v4(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v4, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(12, 12,bias=False),
            nn.ReLU(),
            nn.Linear(12,128,bias=False),
            nn.ReLU(),
            nn.Linear(128,12,bias=False),
            nn.ReLU(),
            nn.Linear(12,3,bias=False),
            nn.Sigmoid(),
        )
        self.trans_layer_1 = torch.nn.Conv2d(3,3,(50,50),stride=1,padding='same',padding_mode='replicate')
        self.trans_layer_2 = torch.nn.Conv2d(3,6,(80,80),stride=1,padding='same',padding_mode='replicate')
        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            xconv1 = self.trans_layer_1(torch.stack([images[ind]]))
            xconv2 = self.trans_layer_2(xconv1)
            images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].permute((1,2,0))
            #print(images[ind].shape)
            #print(images[ind][0,0])
            images[ind] = images[ind].reshape([H*W,12])
            #print(images[ind][0])
            #print(images[ind][0].shape)
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([H,W,12])
            images[ind] = images[ind].permute((2,0,1))
            #masker = masker.reshape([1,H,W])
            # if self.train:
            #     losser = torch.nn.MSELoss()
            #     #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
            #     color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #     print(color_loss)
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            #images[ind] = (images[ind]*0.7 + kek)/1.7  
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        # if self.train:
        #     print("color_loss",color_loss.shape)
        #     #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        return ret
123/72:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v4(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load)
#model.load_state_dict(torch.load('jsr_v4_2000.torch'))
#optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-5},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
123/73:
test = 0
for i in range(1000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
            # if i%200==0:
            #     scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v4.4_"+str(+test+i)+".torch")
123/74:
class jsrcnn_v4(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v4, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(12, 12,bias=False),
            nn.ReLU(),
            nn.Linear(12,128,bias=False),
            nn.ReLU(),
            nn.Linear(128,12,bias=False),
            nn.ReLU(),
            nn.Linear(12,3,bias=False),
            nn.Sigmoid(),
        )
        self.trans_layer_1 = torch.nn.Conv2d(3,3,(50,50),stride=1,padding='same',padding_mode='replicate')
        self.trans_layer_2 = torch.nn.Conv2d(3,6,(80,80),stride=1,padding='same',padding_mode='replicate')
        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            xconv1 = self.trans_layer_1(torch.stack([images[ind]]))
            xconv2 = self.trans_layer_2(xconv1)
            print(images[ind].shape)
            images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].permute((1,2,0))
            #print(images[ind].shape)
            #print(images[ind][0,0])
            images[ind] = images[ind].reshape([H*W,12])
            #print(images[ind][0])
            #print(images[ind][0].shape)
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([H,W,12])
            images[ind] = images[ind].permute((2,0,1))
            #masker = masker.reshape([1,H,W])
            # if self.train:
            #     losser = torch.nn.MSELoss()
            #     #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
            #     color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #     print(color_loss)
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            #images[ind] = (images[ind]*0.7 + kek)/1.7  
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        # if self.train:
        #     print("color_loss",color_loss.shape)
        #     #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        return ret
123/75:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v4(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load)
#model.load_state_dict(torch.load('jsr_v4_2000.torch'))
#optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-5},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
123/76:
test = 0
for i in range(1000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
            # if i%200==0:
            #     scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v4.4_"+str(+test+i)+".torch")
123/77:
class jsrcnn_v4(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v4, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(12, 12,bias=False),
            nn.ReLU(),
            nn.Linear(12,128,bias=False),
            nn.ReLU(),
            nn.Linear(128,12,bias=False),
            nn.ReLU(),
            nn.Linear(12,3,bias=False),
            nn.Sigmoid(),
        )
        self.trans_layer_1 = torch.nn.Conv2d(3,3,(50,50),stride=1,padding='same',padding_mode='replicate')
        self.trans_layer_2 = torch.nn.Conv2d(3,6,(80,80),stride=1,padding='same',padding_mode='replicate')
        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            xconv1 = self.trans_layer_1(torch.stack([images[ind]]))
            xconv2 = self.trans_layer_2(xconv1)
            print(images[ind].shape)
            images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].permute((1,2,0))
            #print(images[ind].shape)
            #print(images[ind][0,0])
            images[ind] = images[ind].reshape([H*W,12])
            #print(images[ind][0])
            #print(images[ind][0].shape)
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([H,W,3])
            images[ind] = images[ind].permute((2,0,1))
            #masker = masker.reshape([1,H,W])
            # if self.train:
            #     losser = torch.nn.MSELoss()
            #     #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
            #     color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #     print(color_loss)
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            #images[ind] = (images[ind]*0.7 + kek)/1.7  
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        # if self.train:
        #     print("color_loss",color_loss.shape)
        #     #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        return ret
123/78:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v4(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load)
#model.load_state_dict(torch.load('jsr_v4_2000.torch'))
#optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-5},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
123/79:
test = 0
for i in range(1000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
            # if i%200==0:
            #     scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v4.4_"+str(+test+i)+".torch")
123/80:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
123/81:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
123/82:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

# for im_name in os.listdir("../dataset/test"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)
123/83:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
123/84:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
123/85:
class jsrcnn_v4(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v4, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(12, 12,bias=False),
            nn.ReLU(),
            nn.Linear(12,128,bias=False),
            nn.ReLU(),
            nn.Linear(128,12,bias=False),
            nn.ReLU(),
            nn.Linear(12,3,bias=False),
            nn.Sigmoid(),
        )
        self.trans_layer_1 = torch.nn.Conv2d(3,3,(50,50),stride=1,padding='same',padding_mode='replicate')
        self.trans_layer_2 = torch.nn.Conv2d(3,6,(80,80),stride=1,padding='same',padding_mode='replicate')
        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            xconv1 = self.trans_layer_1(torch.stack([images[ind]]))
            xconv2 = self.trans_layer_2(xconv1)
            #print(images[ind].shape)
            images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].permute((1,2,0))
            #print(images[ind].shape)
            #print(images[ind][0,0])
            images[ind] = images[ind].reshape([H*W,12])
            #print(images[ind][0])
            #print(images[ind][0].shape)
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([H,W,3])
            images[ind] = images[ind].permute((2,0,1))
            #masker = masker.reshape([1,H,W])
            # if self.train:
            #     losser = torch.nn.MSELoss()
            #     #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
            #     color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #     print(color_loss)
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            #images[ind] = (images[ind]*0.7 + kek)/1.7  
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        # if self.train:
        #     print("color_loss",color_loss.shape)
        #     #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        return ret
123/86:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v4(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load)
#model.load_state_dict(torch.load('jsr_v4_2000.torch'))
#optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-5},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
123/87:
test = 0
for i in range(1000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
            # if i%200==0:
            #     scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v4.4_"+str(+test+i)+".torch")
124/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
124/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
124/3:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

# for im_name in os.listdir("../dataset/test"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)
124/4:
batchSize = 1
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    #iaa.Fliplr(0.5), # horizontal flips
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        #print("instance_toimage[idx].shape : ",instance_toimage[idx].shape)
        patch_stack = seq(images=instance_toimage[idx])
        #patch_stack = instance_toimage[idx]
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        #print("patch_img.shape",patch_img.shape)
        #print(patch_img.shape)
        #patch_img = patch_img.transpose(1,2,0)
        instances = instances.transpose(1,2,0)
        #print(instances.shape)
        # cv2.imshow("jsr",patch_img)
        # cv2.waitKey(0) 
        # cv2.destroyAllWindows() 
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            #dispim = np.expand_dims(instances[:,:,a], axis=2)
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            #dispim = dispim.transpose(2,0,1)
            #print(dispim.shape)
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        #     cv2.imshow("jsr1",dispim)
        #     cv2.waitKey(0)  
        # cv2.destroyAllWindows() 
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    #batch_Imgs = torch.Tensor(batch_Imgs)
    #batch_Imgs.to("cuda")
    #batch_Imgs = batch_Imgs.swapaxes(1, 3).swapaxes(2, 3)
    #print("Load")
    #for i in range(len(batch_Imgs)):
    #    print(batch_Imgs[i].shape)
    #    print(batch_Data[i]["masks"].shape)
    return batch_Imgs, batch_Data
#loadBatch()
124/5:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
124/6:
class jsrcnn_v4(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v4, self).__init__()
        #self.param = param
        self.channel_trans = nn.Sequential(
            nn.Linear(12, 12,bias=False),
            nn.ReLU(),
            nn.Linear(12,128,bias=False),
            nn.ReLU(),
            nn.Linear(128,12,bias=False),
            nn.ReLU(),
            nn.Linear(12,3,bias=False),
            nn.Sigmoid(),
        )
        self.trans_layer_1 = torch.nn.Conv2d(3,3,(50,50),stride=1,padding='same',padding_mode='replicate')
        self.trans_layer_2 = torch.nn.Conv2d(3,6,(80,80),stride=1,padding='same',padding_mode='replicate')
        #torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            #print(images[ind].shape)
            #img = np.array(img.cpu())
            #images[ind] = images[ind].permute(1,2,0)
            #images[ind].flatten()
            #print(images[ind].shape)
            xconv1 = self.trans_layer_1(torch.stack([images[ind]]))
            xconv2 = self.trans_layer_2(xconv1)
            #print(images[ind].shape)
            images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            #print(images[ind].shape)
            H = images[ind].shape[1]
            W = images[ind].shape[2]
            kek = images[ind].to(device)
            images[ind] = images[ind].permute((1,2,0))
            #print(images[ind].shape)
            #print(images[ind][0,0])
            images[ind] = images[ind].reshape([H*W,12])
            #print(images[ind][0])
            #print(images[ind][0].shape)
            #print(images[ind].shape)
            images[ind] = self.channel_trans(images[ind])
            #masker = self.mask_maker(images[ind])
            images[ind] = images[ind].reshape([H,W,3])
            images[ind] = images[ind].permute((2,0,1))
            #masker = masker.reshape([1,H,W])
            # if self.train:
            #     losser = torch.nn.MSELoss()
            #     #print("jsr ",type(targets[ind]["sem_mask"][0:1]))
            #     color_loss += losser(masker,targets[ind]["sem_mask"][0:1])
            #     print(color_loss)
            #    sem_val = torch.mul(images[ind],targets[ind]["sem_mask"]).sum((1,2))
            #    print(sem_val)
            #    neg_val = torch.mul(images[ind],targets[ind]["neg_mask"]).sum((1,2))
            #    color_loss = torch.mul(sem_val,neg_val).sum()/(((torch.square(sem_val).sum())**0.5)*((torch.square(neg_val).sum())**0.5))
            #images[ind] = (images[ind]*0.7 + kek)/1.7  
            #images[ind] = (images[ind] + kek)/2
               #color_loss = (neg_val/sem_val).sum()
               #print(color_loss)
            #print(images[ind].shape)
            # for i in range(images[ind].shape[0]):
            #     for j in range(images[ind].shape[1]):
            #         images[ind][i][j] = self.channel_trans(images[ind][i][j])
            #images[ind] = images[ind].permute(2,0,1)
            #img = torch.Tensor(img).to(device)
            #trniml.append(img)
        #trniml.to(device)
        ret = self.maskrcnn(images,targets)
        # if self.train:
        #     print("color_loss",color_loss.shape)
        #     #color_loss = torch.as_tensor(color_loss,dtype=torch.float16)
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        return ret
124/7:

# model = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT)  # load an instance segmentation model pre-trained pre-trained on COCO
# in_features = model.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
# model.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one
# model.to(device)# move model to the right devic

model = jsrcnn_v4(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load)
#model.load_state_dict(torch.load('jsr_v4_2000.torch'))
#optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-5},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
124/8:
test = 0
for i in range(1000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            # for loss in loss_dict.keys():
            #     print(loss,loss_dict[loss].item())
            if(losses.item()<minloss):
                #torch.save(model.state_dict(), "jsr_v2_minloss_"+str(losses.item())+".torch")
                minloss = losses.item()
            if i%200==0:
                scheduler.step()
            # if i%200==0:
            #     scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
                #optimizer = torch.optim.AdamW(params=model.parameters(), lr=optimizer.)
                torch.save(model.state_dict(), "jsr_v4.4_"+str(+test+i)+".torch")
127/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
127/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
127/3:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

# for im_name in os.listdir("../dataset/test"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)
129/1:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = [
            torch.nn.Conv2d(3,12,(1,1),stride=1,padding='same',padding_mode='replicate'),
            nn.ReLU(),
            torch.nn.Conv2d(12,128,(1,1),stride=1,padding='same',padding_mode='replicate'),
            nn.ReLU(),
            torch.nn.Conv2d(128,12,(1,1),stride=1,padding='same',padding_mode='replicate'),
            nn.ReLU(),
            torch.nn.Conv2d(128,128,(80,80),stride=1,padding='same',padding_mode='replicate'),
            nn.ReLU(),
            torch.nn.Conv2d(128,12,(50,50),stride=1,padding='same',padding_mode='replicate'),       
            nn.Sigmoid(),
        ]

        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            img = self.channel_trans[0](img)
            img = self.channel_trans[0](img)
            img = self.channel_trans[0](img)
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(images,targets)
        return ret
129/2:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
129/3:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
129/4:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

# for im_name in os.listdir("../dataset/test"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)
131/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
131/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
131/3:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

# for im_name in os.listdir("../dataset/test"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)
131/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
131/5:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
131/6:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = [
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            nn.ReLU(),
            torch.nn.Conv2d(12,128,(3,3),stride=1,padding='same',padding_mode='replicate'),
            nn.ReLU(),
            torch.nn.Conv2d(128,12,(1,1),stride=1,padding='same',padding_mode='replicate'),
            nn.ReLU(),
            torch.nn.Conv2d(128,128,(72,72),stride=1,padding='same',padding_mode='replicate'),
            nn.ReLU(),
            torch.nn.Conv2d(128,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
            nn.Sigmoid(),
        ]
        self.maskmaker = nn.Sequential(
            torch.nn.Conv2d(143,143,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(143,64,(24,24),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(64,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,1,(5,5),stride=1,padding='same',padding_mode='replicate'),
        )

        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.channel_trans[1](img)
            img = self.channel_trans[2](img)
            img = self.channel_trans[3](img)
            img12c = img
            img = self.channel_trans[4](img)
            imgmask = torch.concat((imgog,img12c,img),1)
            mk = self.maskmaker(imgmask)

            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(img12c,targets)
        return ret
131/7:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-5},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
131/8:
test = 0
for i in range(1000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
131/9:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = [
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            nn.ReLU(),
            torch.nn.Conv2d(12,128,(3,3),stride=1,padding='same',padding_mode='replicate'),
            nn.ReLU(),
            torch.nn.Conv2d(128,12,(1,1),stride=1,padding='same',padding_mode='replicate'),
            nn.ReLU(),
            torch.nn.Conv2d(128,128,(72,72),stride=1,padding='same',padding_mode='replicate'),
            nn.ReLU(),
            torch.nn.Conv2d(128,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
            nn.Sigmoid(),
        ]
        self.maskmaker = nn.Sequential(
            torch.nn.Conv2d(143,143,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(143,64,(24,24),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(64,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,1,(5,5),stride=1,padding='same',padding_mode='replicate'),
        )

        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]]).to(device)
            imgog = img
            img = self.channel_trans[0](img)
            img = self.channel_trans[1](img)
            img = self.channel_trans[2](img)
            img = self.channel_trans[3](img)
            img12c = img
            img = self.channel_trans[4](img)
            imgmask = torch.concat((imgog,img12c,img),1)
            mk = self.maskmaker(imgmask)

            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(img12c,targets)
        return ret
131/10:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-5},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
131/11:
test = 0
for i in range(1000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
131/12:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = [
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            nn.ReLU(),
            torch.nn.Conv2d(12,128,(3,3),stride=1,padding='same',padding_mode='replicate'),
            nn.ReLU(),
            torch.nn.Conv2d(128,12,(1,1),stride=1,padding='same',padding_mode='replicate'),
            nn.ReLU(),
            torch.nn.Conv2d(128,128,(72,72),stride=1,padding='same',padding_mode='replicate'),
            nn.ReLU(),
            torch.nn.Conv2d(128,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
            nn.Sigmoid(),
        ]
        self.maskmaker = nn.Sequential(
            torch.nn.Conv2d(143,143,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(143,64,(24,24),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(64,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,1,(5,5),stride=1,padding='same',padding_mode='replicate'),
        )

        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]]).to(device)
            imgog = img.to(device)
            img = self.channel_trans[0](img)
            img = self.channel_trans[1](img)
            img = self.channel_trans[2](img)
            img = self.channel_trans[3](img)
            img12c = img
            img = self.channel_trans[4](img)
            imgmask = torch.concat((imgog,img12c,img),1)
            mk = self.maskmaker(imgmask)

            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(img12c,targets)
        return ret
131/13:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-5},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
131/14:
test = 0
for i in range(1000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
131/15:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = [
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            nn.ReLU(),
            torch.nn.Conv2d(12,128,(3,3),stride=1,padding='same',padding_mode='replicate'),
            nn.ReLU(),
            torch.nn.Conv2d(128,12,(1,1),stride=1,padding='same',padding_mode='replicate'),
            nn.ReLU(),
            torch.nn.Conv2d(128,128,(72,72),stride=1,padding='same',padding_mode='replicate'),
            nn.ReLU(),
            torch.nn.Conv2d(128,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
            nn.Sigmoid(),
        ]
        self.maskmaker = nn.Sequential(
            torch.nn.Conv2d(143,143,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(143,64,(24,24),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(64,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,1,(5,5),stride=1,padding='same',padding_mode='replicate'),
        )

        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = (torch.stack([images[ind]])).to(device)
            imgog = img.to(device)
            img = self.channel_trans[0](img)
            img = self.channel_trans[1](img)
            img = self.channel_trans[2](img)
            img = self.channel_trans[3](img)
            img12c = img
            img = self.channel_trans[4](img)
            imgmask = torch.concat((imgog,img12c,img),1)
            mk = self.maskmaker(imgmask)

            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(img12c,targets)
        return ret
131/16:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-5},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
131/17:
test = 0
for i in range(1000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
131/18:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = [
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            nn.ReLU(),
            torch.nn.Conv2d(12,128,(3,3),stride=1,padding='same',padding_mode='replicate'),
            nn.ReLU(),
            torch.nn.Conv2d(128,12,(1,1),stride=1,padding='same',padding_mode='replicate'),
            nn.ReLU(),
            torch.nn.Conv2d(128,128,(72,72),stride=1,padding='same',padding_mode='replicate'),
            nn.ReLU(),
            torch.nn.Conv2d(128,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
            nn.Sigmoid(),
        ].to(device)
        self.maskmaker = nn.Sequential(
            torch.nn.Conv2d(143,143,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(143,64,(24,24),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(64,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,1,(5,5),stride=1,padding='same',padding_mode='replicate'),
        )

        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.channel_trans[1](img)
            img = self.channel_trans[2](img)
            img = self.channel_trans[3](img)
            img12c = img
            img = self.channel_trans[4](img)
            imgmask = torch.concat((imgog,img12c,img),1)
            mk = self.maskmaker(imgmask)

            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(img12c,targets)
        return ret
131/19:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-5},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
131/20:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            nn.ReLU(),
            torch.nn.Conv2d(12,128,(3,3),stride=1,padding='same',padding_mode='replicate'),
            nn.ReLU(),
            torch.nn.Conv2d(128,12,(1,1),stride=1,padding='same',padding_mode='replicate'),
            nn.ReLU(),
            torch.nn.Conv2d(128,128,(72,72),stride=1,padding='same',padding_mode='replicate'),
            nn.ReLU(),
            torch.nn.Conv2d(128,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
            nn.Sigmoid(),
        ])
        self.maskmaker = nn.Sequential(
            torch.nn.Conv2d(143,143,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(143,64,(24,24),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(64,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,1,(5,5),stride=1,padding='same',padding_mode='replicate'),
        )

        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.channel_trans[1](img)
            img = self.channel_trans[2](img)
            img = self.channel_trans[3](img)
            img12c = img
            img = self.channel_trans[4](img)
            imgmask = torch.concat((imgog,img12c,img),1)
            mk = self.maskmaker(imgmask)

            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(img12c,targets)
        return ret
131/21:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-5},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
131/22:
test = 0
for i in range(1000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
132/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
132/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
132/3:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

# for im_name in os.listdir("../dataset/test"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)
132/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
132/5:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
132/6:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            nn.ReLU(),
            torch.nn.Conv2d(12,128,(3,3),stride=1,padding='same',padding_mode='replicate'),
            nn.ReLU(),
            torch.nn.Conv2d(128,12,(1,1),stride=1,padding='same',padding_mode='replicate'),
            nn.ReLU(),
            torch.nn.Conv2d(128,128,(72,72),stride=1,padding='same',padding_mode='replicate'),
            nn.ReLU(),
            torch.nn.Conv2d(128,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
            nn.Sigmoid(),
        ])
        self.maskmaker = nn.Sequential(
            torch.nn.Conv2d(143,143,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(143,64,(24,24),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(64,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,1,(5,5),stride=1,padding='same',padding_mode='replicate'),
        )

        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.channel_trans[1](img)
            img = self.channel_trans[2](img)
            img = self.channel_trans[3](img)
            img12c = img
            img = self.channel_trans[4](img)
            imgmask = torch.concat((imgog,img12c,img),1)
            mk = self.maskmaker(imgmask)

            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(img12c,targets)
        return ret
132/7:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-5},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
132/8:
test = 0
for i in range(1000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
132/9:
batchSize = 1
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
132/10:
test = 0
for i in range(1000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
133/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
133/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
133/3:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

# for im_name in os.listdir("../dataset/test"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)
133/4:
batchSize = 1
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
133/5:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
133/6:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            nn.ReLU(),
            torch.nn.Conv2d(12,128,(3,3),stride=1,padding='same',padding_mode='replicate'),
            nn.ReLU(),
            torch.nn.Conv2d(128,12,(1,1),stride=1,padding='same',padding_mode='replicate'),
            nn.ReLU(),
            torch.nn.Conv2d(128,128,(72,72),stride=1,padding='same',padding_mode='replicate'),
            nn.ReLU(),
            torch.nn.Conv2d(128,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
            nn.Sigmoid(),
        ])
        self.maskmaker = nn.Sequential(
            torch.nn.Conv2d(143,143,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(143,64,(24,24),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(64,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,1,(5,5),stride=1,padding='same',padding_mode='replicate'),
        )

        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.channel_trans[1](img)
            img = self.channel_trans[2](img)
            img = self.channel_trans[3](img)
            img12c = img
            img = self.channel_trans[4](img)
            imgmask = torch.concat((imgog,img12c,img),1)
            mk = self.maskmaker(imgmask)

            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(img12c,targets)
        return ret
133/7:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-5},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
133/8:
test = 0
for i in range(1000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
133/9:
test = 0
for i in range(1000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
134/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
134/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
134/3:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

# for im_name in os.listdir("../dataset/test"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)
134/4:
batchSize = 1
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
134/5:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
134/6:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            nn.ReLU(),
            # torch.nn.Conv2d(12,128,(3,3),stride=1,padding='same',padding_mode='replicate'),
            # nn.ReLU(),
            # torch.nn.Conv2d(128,12,(1,1),stride=1,padding='same',padding_mode='replicate'),
            # nn.ReLU(),
            # torch.nn.Conv2d(128,128,(72,72),stride=1,padding='same',padding_mode='replicate'),
            # nn.ReLU(),
            torch.nn.Conv2d(128,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
            nn.Sigmoid(),
        ])
        self.maskmaker = nn.Sequential(
            torch.nn.Conv2d(143,143,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(143,64,(24,24),stride=1,padding='same',padding_mode='replicate'),
            # torch.nn.Conv2d(64,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(64,1,(5,5),stride=1,padding='same',padding_mode='replicate'),
        )

        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.channel_trans[1](img)
            img = self.channel_trans[2](img)
            img = self.channel_trans[3](img)
            img12c = img
            img = self.channel_trans[4](img)
            imgmask = torch.concat((imgog,img12c,img),1)
            mk = self.maskmaker(imgmask)

            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(img12c,targets)
        return ret
134/7:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-5},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
134/8:
test = 0
for i in range(1000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
134/9:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        self.maskbone = nn.ModuleList([
            torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        ])
        self.tobi = torch.nn.Conv2d(66,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = nn.ReLU(img)
            img = self.channel_trans[1](img)
            img = nn.ReLU(img)
            img3 = self.channel_trans[2](img)
            img3 = nn.ReLU(img3)
            img4 = self.channel_trans[3](img3)
            img4 = nn.ReLU(img4)
            img5 = self.channel_trans[4](img4)
            img5 = nn.ReLU(img5)
            imgmask = torch.concat((img5,img4,imgog),1)
            mk = self.maskbone(imgmask)
            segmask = self.tobi(torch.concat(imgmask,mk,imgog,img3))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(torch.concat(img5,imgog,1),targets)
        return ret
134/10:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-5},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
134/11:
test = 0
for i in range(1000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
134/12:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        self.maskbone = nn.ModuleList([
            torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        ])
        self.ReLU = nn.ReLU
        self.tobi = torch.nn.Conv2d(66,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img3 = self.channel_trans[2](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[3](img3)
            img4 = self.ReLU(img4)
            img5 = self.channel_trans[4](img4)
            img5 = self.ReLU(img5)
            imgmask = torch.concat((img5,img4,imgog),1)
            mk = self.maskbone(imgmask)
            segmask = self.tobi(torch.concat(imgmask,mk,imgog,img3))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(torch.concat(img5,imgog,1),targets)
        return ret
134/13:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-5},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
134/14:
test = 0
for i in range(1000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
134/15:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        self.maskbone = nn.ModuleList([
            torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        ])
        self.ReLU = nn.ReLU
        self.tobi = torch.nn.Conv2d(66,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img3 = self.channel_trans[2](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[3](img3)
            img4 = self.ReLU(img4)
            img5 = self.channel_trans[4](img4)
            img5 = self.ReLU(img5)
            imgmask = torch.concat((img5,img4,imgog),1)
            mk = self.maskbone(imgmask)
            segmask = self.tobi(torch.concat(imgmask,mk,imgog,img3))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(torch.concat(img5,imgog,1),targets)
        return ret
134/16:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-5},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
134/17:
test = 0
for i in range(1000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
134/18:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        self.maskbone = nn.ModuleList([
            torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        ])
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(66,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img3 = self.channel_trans[2](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[3](img3)
            img4 = self.ReLU(img4)
            img5 = self.channel_trans[4](img4)
            img5 = self.ReLU(img5)
            imgmask = torch.concat((img5,img4,imgog),1)
            mk = self.maskbone(imgmask)
            segmask = self.tobi(torch.concat(imgmask,mk,imgog,img3))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(torch.concat(img5,imgog,1),targets)
        return ret
134/19:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-5},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
134/20:
test = 0
for i in range(1000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
134/21:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        self.maskbone = nn.Sequential([
            torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        ])
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(66,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img3 = self.channel_trans[2](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[3](img3)
            img4 = self.ReLU(img4)
            img5 = self.channel_trans[4](img4)
            img5 = self.ReLU(img5)
            imgmask = torch.concat((img5,img4,imgog),1)
            mk = self.maskbone(imgmask)
            segmask = self.tobi(torch.concat(imgmask,mk,imgog,img3))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(torch.concat(img5,imgog,1),targets)
        return ret
134/22:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-5},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
134/23:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        self.maskbone = nn.Sequential(
            torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        ])
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(66,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img3 = self.channel_trans[2](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[3](img3)
            img4 = self.ReLU(img4)
            img5 = self.channel_trans[4](img4)
            img5 = self.ReLU(img5)
            imgmask = torch.concat((img5,img4,imgog),1)
            mk = self.maskbone(imgmask)
            segmask = self.tobi(torch.concat(imgmask,mk,imgog,img3))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(torch.concat(img5,imgog,1),targets)
        return ret
134/24:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        self.maskbone = nn.Sequential(
            torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        )
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(66,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img3 = self.channel_trans[2](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[3](img3)
            img4 = self.ReLU(img4)
            img5 = self.channel_trans[4](img4)
            img5 = self.ReLU(img5)
            imgmask = torch.concat((img5,img4,imgog),1)
            mk = self.maskbone(imgmask)
            segmask = self.tobi(torch.concat(imgmask,mk,imgog,img3))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(torch.concat(img5,imgog,1),targets)
        return ret
134/25:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-5},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
134/26:
test = 0
for i in range(1000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
134/27:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        self.maskbone = nn.Sequential(
            torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        )
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(66,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img3 = self.channel_trans[2](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[3](img3)
            img4 = self.ReLU(img4)
            img5 = self.channel_trans[4](img4)
            img5 = self.ReLU(img5)
            imgmask = torch.concat((img5,img4,imgog),1)
            mk = self.maskbone(imgmask)
            segmask = self.tobi(torch.concat(imgmask,mk,imgog,img3))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(torch.concat((img5,imgog),1),targets)
        return ret
134/28:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-5},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
134/29:
test = 0
for i in range(1000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
134/30:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        self.maskbone = nn.Sequential(
            torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        )
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(66,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img3 = self.channel_trans[2](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[3](img3)
            img4 = self.ReLU(img4)
            img5 = self.channel_trans[4](img4)
            img5 = self.ReLU(img5)
            imgmask = torch.concat((img5,img4,imgog),1)
            mk = self.maskbone(imgmask)
            segmask = self.tobi(torch.concat((imgmask,mk,imgog,img3),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(torch.concat((img5,imgog),1),targets)
        return ret
134/31:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-5},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
134/32:
test = 0
for i in range(1000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
134/33:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        self.maskbone = nn.Sequential(
            torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        )
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(66,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img3 = self.channel_trans[2](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[3](img3)
            img4 = self.ReLU(img4)
            img5 = self.channel_trans[4](img4)
            img5 = self.ReLU(img5)
            imgmask = torch.concat((img5,img4,imgog),1)
            mk = self.maskbone(imgmask)
            segmask = self.tobi(torch.concat((imgmask,mk,imgog,img3),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(self.rko(torch.concat((img5,imgog),1),targets))
        return ret
134/34:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-5},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
134/35:
test = 0
for i in range(1000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
134/36:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
134/37:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
134/38:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

# for im_name in os.listdir("../dataset/test"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)
134/39:
batchSize = 1
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
134/40:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
134/41:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        self.maskbone = nn.Sequential(
            torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        )
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(66,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img3 = self.channel_trans[2](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[3](img3)
            img4 = self.ReLU(img4)
            img5 = self.channel_trans[4](img4)
            img5 = self.ReLU(img5)
            imgmask = torch.concat((img5,img4,imgog),1)
            mk = self.maskbone(imgmask)
            segmask = self.tobi(torch.concat((imgmask,mk,imgog,img3),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(self.rko(torch.concat((img5,imgog),1),targets))
        return ret
134/42:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-5},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
134/43:
test = 0
for i in range(1000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
134/44:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        self.maskbone = nn.Sequential(
            torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        )
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(42,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            #img3 = self.channel_trans[2](img)
            #img3 = self.ReLU(img3)
            img3 = img
            img4 = self.channel_trans[3](img3)
            img4 = self.ReLU(img4)
            img5 = self.channel_trans[4](img4)
            img5 = self.ReLU(img5)
            imgmask = torch.concat((img5,img4,imgog),1)
            mk = self.maskbone(imgmask)
            segmask = self.tobi(torch.concat((imgmask,mk,imgog,img3),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(self.rko(torch.concat((img5,imgog),1),targets))
        return ret
134/45:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-5},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
134/46:
test = 0
for i in range(1000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
134/47:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        self.maskbone = nn.Sequential(
            torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        )
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(42,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            #img3 = self.channel_trans[2](img)
            #img3 = self.ReLU(img3)
            img3 = img
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img5 = self.channel_trans[3](img4)
            img5 = self.ReLU(img5)
            imgmask = torch.concat((img5,img4,imgog),1)
            mk = self.maskbone(imgmask)
            segmask = self.tobi(torch.concat((imgmask,mk,imgog,img3),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(self.rko(torch.concat((img5,imgog),1),targets))
        return ret
134/48:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-5},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
134/49:
test = 0
for i in range(1000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
134/50:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        self.maskbone = nn.Sequential(
            torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        )
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(42,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            #img3 = self.channel_trans[2](img)
            #img3 = self.ReLU(img3)
            img3 = img
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img5 = self.channel_trans[3](img4)
            img5 = self.ReLU(img5)
            imgmask = torch.concat((img5,img4,imgog),1)
            mk = self.maskbone(imgmask)
            segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(self.rko(torch.concat((img5,imgog),1),targets))
        return ret
134/51:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-5},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
134/52:
test = 0
for i in range(1000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
134/53:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        self.maskbone = nn.Sequential(
            torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        )
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(42,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            #img3 = self.channel_trans[2](img)
            #img3 = self.ReLU(img3)
            img3 = img
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img5 = self.channel_trans[3](img4)
            img5 = self.ReLU(img5)
            imgmask = torch.concat((img5,img4,imgog),1)
            mk = self.maskbone(imgmask)
            segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(self.rko(torch.concat((img5,imgog),1)),targets)
        return ret
134/54:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-5},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
134/55:
test = 0
for i in range(1000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
135/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
135/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
135/3:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

# for im_name in os.listdir("../dataset/test"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)
135/4:
batchSize = 1
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
135/5:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
135/6:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        self.maskbone = nn.Sequential(
            torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        )
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(42,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            #img3 = self.channel_trans[2](img)
            #img3 = self.ReLU(img3)
            img3 = img
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img5 = self.channel_trans[3](img4)
            img5 = self.ReLU(img5)
            imgmask = torch.concat((img5,img4,imgog),1)
            mk = self.maskbone(imgmask)
            segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(self.rko(torch.concat((img5,imgog),1)),targets)
        return ret
135/7:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-5},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
135/8:
test = 0
for i in range(1000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
135/9:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
135/10:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
135/11:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

# for im_name in os.listdir("../dataset/test"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)
135/12:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
135/13:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
135/14:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        self.maskbone = nn.Sequential(
            torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        )
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(42,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            #img3 = self.channel_trans[2](img)
            #img3 = self.ReLU(img3)
            img3 = img
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img5 = self.channel_trans[3](img4)
            img5 = self.ReLU(img5)
            imgmask = torch.concat((img5,img4,imgog),1)
            mk = self.maskbone(imgmask)
            segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(self.rko(torch.concat((img5,imgog),1)),targets)
        return ret
135/15:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-1}], lr=1e-5)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
135/16:
test = 0
for i in range(1000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
135/17:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
135/18:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
135/19:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

# for im_name in os.listdir("../dataset/test"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)
135/20:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
135/21:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
135/22:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.ReLU = nn.ReLU()
        # self.tobi = torch.nn.Conv2d(42,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        # self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        #for ind in range(len(images)):
        img = torch.stack(images)
        imgog = img
        img = self.channel_trans[0](img)
        img = self.ReLU(img)
        img = self.channel_trans[1](img)
        img = self.ReLU(img)
            #img3 = self.channel_trans[2](img)
            #img3 = self.ReLU(img3)
            # img3 = img
            # img4 = self.channel_trans[2](img3)
            # img4 = self.ReLU(img4)
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(img),targets)
        return ret
135/23:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,3,(1,1),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.ReLU = nn.ReLU()
        # self.tobi = torch.nn.Conv2d(42,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        # self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        #for ind in range(len(images)):
        img = torch.stack(images)
        imgog = img
        img = self.channel_trans[0](img)
        img = self.ReLU(img)
        img = self.channel_trans[1](img)
        img = self.ReLU(img)
        img = self.channel_trans[2](img)
        img = self.sigma(img)
            #img3 = self.channel_trans[2](img)
            #img3 = self.ReLU(img3)
            # img3 = img
            # img4 = self.channel_trans[2](img3)
            # img4 = self.ReLU(img4)
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn((img),targets)
        return ret
135/24:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-1}], lr=1e-5)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
135/25:
test = 0
for i in range(1000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
135/26:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,3,(1,1),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.ReLU = nn.ReLU()
        # self.tobi = torch.nn.Conv2d(42,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        # self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img = self.channel_trans[2](img)
            img = self.sigma(img)
            #img3 = self.channel_trans[2](img)
            #img3 = self.ReLU(img3)
            # img3 = img
            # img4 = self.channel_trans[2](img3)
            # img4 = self.ReLU(img4)
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn((img),targets)
        return ret
135/27:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-1}], lr=1e-5)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
135/28:
test = 0
for i in range(1000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
135/29:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
135/30:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
135/31:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

# for im_name in os.listdir("../dataset/test"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)
135/32:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
135/33:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
135/34:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,3,(1,1),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.ReLU = nn.ReLU()
        # self.tobi = torch.nn.Conv2d(42,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        # self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img = self.channel_trans[2](img)
            img = self.sigma(img)
            images[ind] = img
            #img3 = self.channel_trans[2](img)
            #img3 = self.ReLU(img3)
            # img3 = img
            # img4 = self.channel_trans[2](img3)
            # img4 = self.ReLU(img4)
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(images,targets)
        return ret
135/35:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-1}], lr=1e-5)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
135/36:
test = 0
for i in range(10000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
135/37:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
135/38:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
135/39:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

# for im_name in os.listdir("../dataset/test"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)
135/40:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
135/41:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
135/42:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,3,(1,1),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.ReLU = nn.ReLU()
        # self.tobi = torch.nn.Conv2d(42,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        # self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img = self.channel_trans[2](img)
            img = self.sigma(img)
            images[ind] = img[0]
            #img3 = self.channel_trans[2](img)
            #img3 = self.ReLU(img3)
            # img3 = img
            # img4 = self.channel_trans[2](img3)
            # img4 = self.ReLU(img4)
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(images,targets)
        return ret
135/43:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-1}], lr=1e-5)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
135/44:
test = 0
for i in range(10000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
135/45:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,3,(1,1),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.ReLU = nn.ReLU()
        # self.tobi = torch.nn.Conv2d(42,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        # self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img = self.channel_trans[2](img)
            img = self.sigma(img)
            images[ind] = img[0]
            #img3 = self.channel_trans[2](img)
            #img3 = self.ReLU(img3)
            # img3 = img
            # img4 = self.channel_trans[2](img3)
            # img4 = self.ReLU(img4)
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(images,targets)
        return ret
135/46:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-2}], lr=1e-5)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
135/47:
test = 0
for i in range(10000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
135/48:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,3,(1,1),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.ReLU = nn.ReLU()
        # self.tobi = torch.nn.Conv2d(42,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        # self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img = self.channel_trans[2](img)
            img = self.sigma(img)
            images[ind] = img[0]
            #img3 = self.channel_trans[2](img)
            #img3 = self.ReLU(img3)
            # img3 = img
            # img4 = self.channel_trans[2](img3)
            # img4 = self.ReLU(img4)
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(images,targets)
        return ret
135/49:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-1}], lr=1e-5)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
135/50:
test = 0
for i in range(10000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
136/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
136/2:
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
device = 'cpu'
136/3:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
136/4:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

# for im_name in os.listdir("../dataset/test"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)
136/5:
batchSize = 2
img_siz = [512,512]
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
136/6:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
136/7:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(51,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img3 = self.channel_trans[2](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[3](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4,img),1)
            img_ot = self.sigma(self.tobi(img_comb))
            images[ind] = img_ot[0]
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(images,targets)
        return ret
136/8:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-1}], lr=1e-5)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
136/9:
test = 0
for i in range(10000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
138/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
138/2:
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
device = 'cpu'
138/3:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
138/4:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

# for im_name in os.listdir("../dataset/test"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)
138/5:
batchSize = 2
img_siz = [512,512]
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
138/6:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
138/7:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(51,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img3 = self.channel_trans[2](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[3](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4,img),1)
            img_ot = self.sigma(self.tobi(img_comb))
            images[ind] = img_ot[0]
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(images,targets)
        return ret
138/8:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-1}], lr=1e-5)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
138/9:
test = 0
for i in range(10000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device=device)
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
135/51:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
135/52:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
135/53:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

# for im_name in os.listdir("../dataset/test"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)
135/54:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
135/55:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
135/56:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,3,(1,1),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.ReLU = nn.ReLU()
        # self.tobi = torch.nn.Conv2d(42,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        # self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img = self.channel_trans[2](img)
            img = self.sigma(img)
            images[ind] = img[0]
            #img3 = self.channel_trans[2](img)
            #img3 = self.ReLU(img3)
            # img3 = img
            # img4 = self.channel_trans[2](img3)
            # img4 = self.ReLU(img4)
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(images,targets)
        return ret
135/57:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-4}], lr=1e-5)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
135/58:
test = 0
for i in range(10000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device='cuda:0')
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
139/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
139/2:
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
device = 'cuda'
139/3:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
139/4:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

# for im_name in os.listdir("../dataset/test"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)
139/5:
batchSize = 2
img_siz = [512,512]
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
139/6:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
139/7:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(51,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img3 = self.channel_trans[2](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[3](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4,img),1)
            img_ot = self.sigma(self.tobi(img_comb))
            images[ind] = img_ot[0]
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(images,targets)
        return ret
139/8:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-1}], lr=1e-5)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
139/9:
test = 0
for i in range(10000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device=device)
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
141/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
141/2:
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
device = 'cuda'
141/3:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
141/4:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

# for im_name in os.listdir("../dataset/test"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)
141/5:
batchSize = 2
img_siz = [512,512]
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
141/6:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
141/7:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(51,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img3 = self.channel_trans[2](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[3](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4,img),1)
            img_ot = self.sigma(self.tobi(img_comb))
            images[ind] = img_ot[0]
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(images,targets)
        return ret
141/8:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
141/9:
test = 0
for i in range(10000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device=device)
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
140/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
140/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
140/3:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

# for im_name in os.listdir("../dataset/test"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)
140/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
140/5:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
140/6:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,3,(1,1),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.ReLU = nn.ReLU()
        # self.tobi = torch.nn.Conv2d(42,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        # self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img = self.channel_trans[2](img)
            img = self.sigma(img)
            images[ind] = img[0]
            #img3 = self.channel_trans[2](img)
            #img3 = self.ReLU(img3)
            # img3 = img
            # img4 = self.channel_trans[2](img3)
            # img4 = self.ReLU(img4)
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(images,targets)
        return ret
140/7:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
model.load_state_dict(torch.load('jsr_v5_1350.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.maskrcnn.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
140/8:
test = 1350
for i in range(10000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device=device)
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
140/9:
test = 1350
for i in range(10000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%20==0 and i!=0:
                evaluate(model, test_dataloader, device=device)
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
140/10:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
model.load_state_dict(torch.load('jsr_v5_1350.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-4}], lr=1e-5)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.maskrcnn.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
140/11:
test = 1350
for i in range(10000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            if i%50==0:
                scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device=device)
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
140/12:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
model.load_state_dict(torch.load('jsr_v5_1600.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-0}], lr=1e-5)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.maskrcnn.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
140/13:
test = 1600
for i in range(10000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            if i%20==0:
                scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device=device)
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
140/14:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
model.load_state_dict(torch.load('jsr_v5_1600.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-1}], lr=1e-5)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.maskrcnn.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
140/15:
test = 1600
for i in range(10000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            if i%20==0:
                scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device=device)
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
140/16:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
model.load_state_dict(torch.load('jsr_v5_1600.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-2}], lr=1e-5)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.maskrcnn.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
140/17:
test = 1600
for i in range(10000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            if i%30==0:
                scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device=device)
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
140/18:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
model.load_state_dict(torch.load('jsr_v5_1600.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-4}], lr=1e-5)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.maskrcnn.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
140/19:
test = 1600
for i in range(10000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            if i%30==0:
                scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device=device)
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
140/20:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
140/21:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
140/22:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("../dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
140/23:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
140/24:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
140/25:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,3,(1,1),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.ReLU = nn.ReLU()
        # self.tobi = torch.nn.Conv2d(42,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        # self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img = self.channel_trans[2](img)
            img = self.sigma(img)
            images[ind] = img[0]
            #img3 = self.channel_trans[2](img)
            #img3 = self.ReLU(img3)
            # img3 = img
            # img4 = self.channel_trans[2](img3)
            # img4 = self.ReLU(img4)
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(images,targets)
        return ret
140/26:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
model.load_state_dict(torch.load('jsr_v5_1750.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-4}], lr=1e-5)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.channel_trans.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
140/27:
test = 1600
for i in range(10000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            if i%30==0:
                scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device=device)
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
143/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
143/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
143/3:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

# for im_name in os.listdir("../dataset/test"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)
143/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        #k_w = patch_stack.shape[2]
        #k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
143/5:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
143/6:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,3,(1,1),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.ReLU = nn.ReLU()
        # self.tobi = torch.nn.Conv2d(42,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        # self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img = self.channel_trans[2](img)
            img = self.sigma(img)
            images[ind] = img[0]
            #img3 = self.channel_trans[2](img)
            #img3 = self.ReLU(img3)
            # img3 = img
            # img4 = self.channel_trans[2](img3)
            # img4 = self.ReLU(img4)
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(images,targets)
        return ret
143/7:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load('jsr_v5_1750.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-4}], lr=1e-5)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.channel_trans.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
143/8:
test = 0
for i in range(10000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            if i%30==0:
                scheduler.step()
            if i%50==0 and i!=0:
                evaluate(model, test_dataloader, device=device)
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
149/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
149/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
149/3:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

# for im_name in os.listdir("../dataset/test"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)
149/4:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
149/5:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
149/6:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

# for im_name in os.listdir("../dataset/test"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)
149/7:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        #k_w = patch_stack.shape[2]
        #k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
149/8:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
149/9:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,3,(1,1),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.ReLU = nn.ReLU()
        # self.tobi = torch.nn.Conv2d(42,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        # self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img = self.channel_trans[2](img)
            img = self.sigma(img)
            images[ind] = img[0]
            #img3 = self.channel_trans[2](img)
            #img3 = self.ReLU(img3)
            # img3 = img
            # img4 = self.channel_trans[2](img3)
            # img4 = self.ReLU(img4)
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(images,targets)
        return ret
149/10:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
model.load_state_dict(torch.load('jsr_v5_1750.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.channel_trans.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
149/11:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load('jsr_v5_1750.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.channel_trans.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
149/12:
test = 1600
for i in range(10000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%100==0 and i!=0:
                evaluate(model, test_dataloader, device=device)
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
149/13:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load('jsr_v5_1750.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.channel_trans.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
149/14:
test = 0
for i in range(10000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%100==0 and i!=0:
                evaluate(model, test_dataloader, device=device)
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
150/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
150/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
150/3:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("../dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
150/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        #k_w = patch_stack.shape[2]
        #k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
150/5:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
150/6:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,3,(1,1),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.ReLU = nn.ReLU()
        # self.tobi = torch.nn.Conv2d(42,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        # self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img = self.channel_trans[2](img)
            img = self.sigma(img)
            images[ind] = img[0]
            #img3 = self.channel_trans[2](img)
            #img3 = self.ReLU(img3)
            # img3 = img
            # img4 = self.channel_trans[2](img3)
            # img4 = self.ReLU(img4)
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(images,targets)
        return ret
150/7:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load('jsr_v5_1750.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.channel_trans.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
150/8:
test = 0
for i in range(10000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%100==0 and i!=0:
                evaluate(model, test_dataloader, device=device)
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
150/9:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
150/10:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
150/11:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

# for im_name in os.listdir("../dataset/test"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)
150/12:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        #k_w = patch_stack.shape[2]
        #k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
150/13:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
150/14:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,3,(1,1),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.ReLU = nn.ReLU()
        # self.tobi = torch.nn.Conv2d(42,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        # self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img = self.channel_trans[2](img)
            img = self.sigma(img)
            images[ind] = img[0]
            #img3 = self.channel_trans[2](img)
            #img3 = self.ReLU(img3)
            # img3 = img
            # img4 = self.channel_trans[2](img3)
            # img4 = self.ReLU(img4)
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(images,targets)
        return ret
150/15:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load('jsr_v5_1750.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.channel_trans.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
150/16:
test = 0
for i in range(10000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%100==0 and i!=0:
                evaluate(model, test_dataloader, device=device)
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
150/17:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
model.load_state_dict(torch.load('jsr_v5_700.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-5},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.channel_trans.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
150/18:
test = 700
for i in range(10000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%100==0 and i!=0:
                evaluate(model, test_dataloader, device=device)
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
150/19:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
model.load_state_dict(torch.load('jsr_v5_700.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-5},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.channel_trans.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
150/20:
test = 700
for i in range(10000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            if i%50==0:
                scheduler.step()
            if i%20==0 and i!=0:
                evaluate(model, test_dataloader, device=device)
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
150/21:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
model.load_state_dict(torch.load('jsr_v5_800.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-5},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-4}], lr=1e-5)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.maskrcnn.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
150/22:
test = 800
for i in range(10000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            if i%50==0:
                scheduler.step()
            if i%20==0 and i!=0:
                evaluate(model, test_dataloader, device=device)
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
150/23:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
model.load_state_dict(torch.load('jsr_v5_800.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-5},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-4}], lr=1e-5)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.maskrcnn.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
150/24:
test = 800
for i in range(10000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            if i%20==0:
                scheduler.step()
            if i%20==0 and i!=0:
                evaluate(model, test_dataloader, device=device)
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
150/25:
test = 800
for i in range(10000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())

            print(scheduler.get_lr())
            if i%1==0:
                scheduler.step()
            if i%20==0 and i!=0:
                evaluate(model, test_dataloader, device=device)
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
150/26:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
model.load_state_dict(torch.load('jsr_v5_800.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-5},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-4}], lr=1e-5)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.maskrcnn.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
150/27:
test = 800
for i in range(10000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())

            print(scheduler.get_lr())
            if i%1==0:
                scheduler.step()
            if i%20==0 and i!=0:
                evaluate(model, test_dataloader, device=device)
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
151/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
151/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
151/3:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("../dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
151/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
151/5:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
151/6:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,3,(1,1),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.ReLU = nn.ReLU()
        # self.tobi = torch.nn.Conv2d(42,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        # self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img = self.channel_trans[2](img)
            img = self.sigma(img)
            images[ind] = img[0]
            #img3 = self.channel_trans[2](img)
            #img3 = self.ReLU(img3)
            # img3 = img
            # img4 = self.channel_trans[2](img3)
            # img4 = self.ReLU(img4)
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(images,targets)
        return ret
151/7:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
model.load_state_dict(torch.load('jsr_v5_1060.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-6},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-5}], lr=1e-5)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
151/8:
test = 800
for i in range(10000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())

            print(scheduler.get_lr())
            if i%40==0:
                scheduler.step()
            if i%20==0 and i!=0:
                evaluate(model, test_dataloader, device=device)
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
151/9:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
model.load_state_dict(torch.load('jsr_v5_1060.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-6},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-5}], lr=1e-5)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
151/10:
test = 800
for i in range(10000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())

            
            if i%40==0:
                scheduler.step()
                print(scheduler.get_lr())
            if i%20==0 and i!=0:
                evaluate(model, test_dataloader, device=device)
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
151/11:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
model.load_state_dict(torch.load('jsr_v5_1060.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-6},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-5}], lr=1e-5)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
151/12:
test = 1060
for i in range(10000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())

            
            if i%40==0:
                scheduler.step()
                print(scheduler.get_lr())
            if i%20==0 and i!=0:
                evaluate(model, test_dataloader, device=device)
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
151/13:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
model.load_state_dict(torch.load('jsr_v5_1060.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-7},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-6}], lr=1e-5)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
151/14:
test = 1060
for i in range(10000):
    if i ==0:
        evaluate(model, test_dataloader, device=device)
        model.train()
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())

    
    if i%40==0:
        scheduler.step()
        print(scheduler.get_lr())
    if i%20==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
151/15:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
model.load_state_dict(torch.load('jsr_v5_1060.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-9},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-10}], lr=1e-5)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
151/16:
test = 1060
for i in range(10000):
    if i ==0:
        evaluate(model, test_dataloader, device=device)
        model.train()
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())

    
    if i%40==0:
        scheduler.step()
        print(scheduler.get_lr())
    if i%20==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
151/17:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
model.load_state_dict(torch.load('jsr_v5_1060.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-11},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-12}], lr=1e-5)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
151/18:
test = 1060
for i in range(10000):
    if i ==0:
        evaluate(model, test_dataloader, device=device)
        model.train()
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())

    
    if i%20==0:
        scheduler.step()
        print(scheduler.get_lr())
    if i%20==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
151/19:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
model.load_state_dict(torch.load('jsr_v5_1060.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-12},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-13}], lr=1e-5)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
151/20:
test = 1060
for i in range(10000):
    if i ==0:
        evaluate(model, test_dataloader, device=device)
        model.train()
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())

    
    if i%80==0:
        scheduler.step()
        print(scheduler.get_lr())
    if i%20==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
151/21:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
model.load_state_dict(torch.load('jsr_v5_1060.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-12},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-13}], lr=1e-5)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
151/22:
test = 1060
for i in range(10000):
    if i ==0:
        evaluate(model, test_dataloader, device=device)
        model.train()
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())

    
    if i%80==0 and i!=0:
        scheduler.step()
        print(scheduler.get_lr())
    if i%20==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
151/23:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
151/24:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
151/25:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("../dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
151/26:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
151/27:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
151/28:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,3,(1,1),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.ReLU = nn.ReLU()
        # self.tobi = torch.nn.Conv2d(42,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        # self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img = self.channel_trans[2](img)
            img = self.sigma(img)
            images[ind] = img[0]
            #img3 = self.channel_trans[2](img)
            #img3 = self.ReLU(img3)
            # img3 = img
            # img4 = self.channel_trans[2](img3)
            # img4 = self.ReLU(img4)
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(images,targets)
        return ret
151/29:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
model.load_state_dict(torch.load('jsr_v5_1060.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-12},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-13}], lr=1e-5)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.channel_trans.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
151/30:
test = 1060
for i in range(10000):
    if i ==0:
        evaluate(model, test_dataloader, device=device)
        model.train()
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())

    
    if i%80==0 and i!=0:
        scheduler.step()
        print(scheduler.get_lr())
    if i%20==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
151/31:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
model.load_state_dict(torch.load('jsr_v5_1060.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-14},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-13}], lr=1e-5)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.channel_trans.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
151/32:
test = 1060
for i in range(10000):
    if i ==0:
        evaluate(model, test_dataloader, device=device)
        model.train()
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())

    
    if i%80==0 and i!=0:
        scheduler.step()
        print(scheduler.get_lr())
    if i%20==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
151/33:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
model.load_state_dict(torch.load('jsr_v5_1060.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-20},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-13}], lr=1e-5)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.channel_trans.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
151/34:
test = 1060
for i in range(10000):
    if i ==0:
        evaluate(model, test_dataloader, device=device)
        model.train()
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())

    
    if i%80==0 and i!=0:
        scheduler.step()
        print(scheduler.get_lr())
    if i%20==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
151/35:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
model.load_state_dict(torch.load('jsr_v5_1060.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-20},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-13}], lr=1e-5)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.channel_trans.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
151/36:
test = 1060
for i in range(10000):
    if i ==0:
        print(scheduler.get_lr())
        evaluate(model, test_dataloader, device=device)
        model.train()
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())

    
    if i%80==0 and i!=0:
        scheduler.step()
        print(scheduler.get_lr())
    if i%20==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
151/37:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
model.load_state_dict(torch.load('jsr_v5_1060.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-25},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-13}], lr=1e-5)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.channel_trans.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
151/38:
test = 1060
for i in range(10000):
    if i ==0:
        print(scheduler.get_lr())
        evaluate(model, test_dataloader, device=device)
        model.train()
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())

    
    if i%80==0 and i!=0:
        scheduler.step()
        print(scheduler.get_lr())
    if i%20==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
142/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
142/2:
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
device = 'cuda'
142/3:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
142/4:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

# for im_name in os.listdir("../dataset/test"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)
142/5:
batchSize = 2
img_siz = [512,512]
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        #k_w = patch_stack.shape[2]
        #k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
142/6:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        #k_w = patch_stack.shape[2]
        #k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
142/7:
def dice_loss(input, target):
    smooth = 1.
    iflat = input.view(-1)
    tflat = target.view(-1)
    intersection = (iflat * tflat).sum()
    
    return 1 - ((2. * intersection + smooth) /
              (iflat.sum() + tflat.sum() + smooth))
142/8:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(51,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(51,1,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img3 = self.channel_trans[2](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[3](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4,img),1)
            img_ot = self.sigma(self.tobi(img_comb))
            images[ind] = img_ot[0]
            ghost = self.sigma(self.obito(img_comb))[0]
            ghost = ghost>0.5
            if self.train:
                seg_loss = dice_loss(ghost,targets[ind]["sem_mask"])
                color_loss[0] += seg_loss
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
            ret = self.maskrcnn(images,targets)
        if self.train:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
142/9:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
142/10:
test = 0
for i in range(10000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            if i%100==0:
                scheduler.step()
            if i%50==0 and i!=0:
                #evaluate(model, test_dataloader, device=device)
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
142/11:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(51,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(51,1,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img3 = self.channel_trans[2](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[3](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4,img),1)
            img_ot = self.sigma(self.tobi(img_comb))
            images[ind] = img_ot[0]
            ghost = self.sigma(self.obito(img_comb))[0]
            ghost = ghost>0.5
            print(ghost.shape)
            print(targets[ind]["sem_mask"].shape)
            if self.train:
                seg_loss = dice_loss(ghost,targets[ind]["sem_mask"])
                color_loss[0] += seg_loss
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
            ret = self.maskrcnn(images,targets)
        if self.train:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
142/12:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
142/13:
test = 0
for i in range(10000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            if i%100==0:
                scheduler.step()
            if i%50==0 and i!=0:
                #evaluate(model, test_dataloader, device=device)
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
142/14:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(51,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(51,1,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img3 = self.channel_trans[2](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[3](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4,img),1)
            img_ot = self.sigma(self.tobi(img_comb))
            images[ind] = img_ot[0]
            ghost = self.sigma(self.obito(img_comb))[0]
            ghost = ghost>0.5
            print(ghost.shape)
            print(targets[ind]["sem_mask"][0].shape)
            if self.train:
                seg_loss = dice_loss(ghost,targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
            ret = self.maskrcnn(images,targets)
        if self.train:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
142/15:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
142/16:
test = 0
for i in range(10000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            if i%100==0:
                scheduler.step()
            if i%50==0 and i!=0:
                #evaluate(model, test_dataloader, device=device)
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
142/17:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(51,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(51,1,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img3 = self.channel_trans[2](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[3](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4,img),1)
            img_ot = self.sigma(self.tobi(img_comb))
            images[ind] = img_ot[0]
            ghost = self.sigma(self.obito(img_comb))[0]
            ghost = ghost>0.5
            #print(ghost.shape)
            #print(targets[ind]["sem_mask"][0].shape)
            if self.train:
                seg_loss = dice_loss(ghost,targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
            ret = self.maskrcnn(images,targets)
        if self.train:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
142/18:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
142/19:
test = 0
for i in range(10000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
            if i%100==0:
                scheduler.step()
            if i%50==0 and i!=0:
                #evaluate(model, test_dataloader, device=device)
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
142/20:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(51,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(51,1,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img3 = self.channel_trans[2](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[3](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4,img),1)
            img_ot = self.sigma(self.tobi(img_comb))
            images[ind] = img_ot[0]
            ghost = self.sigma(self.obito(img_comb))[0]
            ghost = ghost>0.5
            #print(ghost.shape)
            #print(targets[ind]["sem_mask"][0].shape)
            if self.train:
                seg_loss = dice_loss(ghost,targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
            ret = self.maskrcnn(images,targets)
        if self.train:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
142/21:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-5)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
142/22:
test = 0
for i in range(10000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%50==0 and i!=0:
                #evaluate(model, test_dataloader, device=device)
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
142/23:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(51,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(51,1,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img3 = self.channel_trans[2](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[3](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4,img),1)
            img_ot = self.sigma(self.tobi(img_comb))
            images[ind] = img_ot[0]
            ghost = self.sigma(self.obito(img_comb))[0]
            ghost = ghost>0.5
            #print(ghost.shape)
            #print(targets[ind]["sem_mask"][0].shape)
            if self.train:
                seg_loss = dice_loss(ghost,targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
            ret = self.maskrcnn(images,targets)
        if self.train:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
142/24:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-4)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
142/25:
test = 0
for i in range(10000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%50==0 and i!=0:
                #evaluate(model, test_dataloader, device=device)
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
153/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
153/2:
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
device = 'cuda'
153/3:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
153/4:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

# for im_name in os.listdir("../dataset/test"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)
153/5:
batchSize = 2
img_siz = [512,512]
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        #k_w = patch_stack.shape[2]
        #k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
153/6:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        #k_w = patch_stack.shape[2]
        #k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
153/7:
def dice_loss(input, target):
    smooth = 1.
    iflat = input.view(-1)
    tflat = target.view(-1)
    intersection = (iflat * tflat).sum()
    
    return 1 - ((2. * intersection + smooth) /
              (iflat.sum() + tflat.sum() + smooth))
153/8:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(39,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(39,1,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img3 = self.channel_trans[2](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[3](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.sigma(self.tobi(img_comb))
            images[ind] = img_ot[0]
            ghost = self.sigma(self.obito(img_comb))[0]
            ghost = ghost>0.5
            #print(ghost.shape)
            #print(targets[ind]["sem_mask"][0].shape)
            if self.train:
                seg_loss = dice_loss(ghost,targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
            ret = self.maskrcnn(images,targets)
        if self.train:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
153/9:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-4)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
153/10:
test = 0
for i in range(10000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%50==0 and i!=0:
                #evaluate(model, test_dataloader, device=device)
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
153/11:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
153/12:
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
device = 'cuda'
153/13:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
153/14:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

# for im_name in os.listdir("../dataset/test"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)
153/15:
batchSize = 2
img_siz = [512,512]
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        #k_w = patch_stack.shape[2]
        #k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
153/16:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        #k_w = patch_stack.shape[2]
        #k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
153/17:
def dice_loss(input, target):
    smooth = 1.
    iflat = input.view(-1)
    tflat = target.view(-1)
    intersection = (iflat * tflat).sum()
    
    return 1 - ((2. * intersection + smooth) /
              (iflat.sum() + tflat.sum() + smooth))
153/18:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(39,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(39,1,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img3 = self.channel_trans[2](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[3](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.sigma(self.tobi(img_comb))
            images[ind] = img_ot[0]
            ghost = self.sigma(self.obito(img_comb))[0]
            ghost = ghost>0.5
            #print(ghost.shape)
            #print(targets[ind]["sem_mask"][0].shape)
            if self.train:
                seg_loss = dice_loss(ghost,targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
            ret = self.maskrcnn(images,targets)
        if self.train:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
153/19:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-4)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
153/20:
test = 0
for i in range(10000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%50==0 and i!=0:
                #evaluate(model, test_dataloader, device=device)
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
153/21:
def dice_loss(input, target):
    smooth = 1.
    iflat = input.view(-1)
    tflat = target.view(-1)
    intersection = (iflat * tflat).sum()
    
    return 1 - ((2. * intersection + smooth) /
              (iflat.sum() + tflat.sum() + smooth))
153/22:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(39,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(39,1,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img3 = self.channel_trans[2](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[3](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.sigma(self.tobi(img_comb[0]))
            images[ind] = img_ot
            ghost = self.sigma(self.obito(img_comb))[0]
            ghost = ghost>0.5
            #print(ghost.shape)
            #print(targets[ind]["sem_mask"][0].shape)
            if self.train:
                seg_loss = dice_loss(ghost,targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
            ret = self.maskrcnn(images,targets)
        if self.train:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
153/23:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-4)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
153/24:
test = 0
for i in range(10000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%50==0 and i!=0:
                #evaluate(model, test_dataloader, device=device)
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
153/25:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(27,1,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img3 = self.channel_trans[2](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[3](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.sigma(self.tobi(img_comb[0]))
            images[ind] = img_ot
            ghost = self.sigma(self.obito(img_comb))[0]
            ghost = ghost>0.5
            #print(ghost.shape)
            #print(targets[ind]["sem_mask"][0].shape)
            if self.train:
                seg_loss = dice_loss(ghost,targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
            ret = self.maskrcnn(images,targets)
        if self.train:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
153/26:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-4)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
153/27:
test = 0
for i in range(10000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%50==0 and i!=0:
                #evaluate(model, test_dataloader, device=device)
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
153/28:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
153/29:
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
device = 'cuda'
153/30:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
153/31:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

# for im_name in os.listdir("../dataset/test"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)
153/32:
batchSize = 2
img_siz = [512,512]
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        #k_w = patch_stack.shape[2]
        #k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
153/33:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        #k_w = patch_stack.shape[2]
        #k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
153/34:
def dice_loss(input, target):
    smooth = 1.
    iflat = input.view(-1)
    tflat = target.view(-1)
    intersection = (iflat * tflat).sum()
    
    return 1 - ((2. * intersection + smooth) /
              (iflat.sum() + tflat.sum() + smooth))
153/35:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(27,1,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.sigma(self.tobi(img_comb[0]))
            images[ind] = img_ot
            ghost = self.sigma(self.obito(img_comb))[0]
            #ghost = ghost>0.5
            #print(ghost.shape)
            #print(targets[ind]["sem_mask"][0].shape)
            if self.train:
                seg_loss = dice_loss(ghost,targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
            ret = self.maskrcnn(images,targets)
        if self.train:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
153/36:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-3)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
153/37:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
153/38:
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
device = 'cuda'
153/39:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
153/40:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

# for im_name in os.listdir("../dataset/test"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)
153/41:
batchSize = 2
img_siz = [512,512]
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        #k_w = patch_stack.shape[2]
        #k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
153/42:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        #k_w = patch_stack.shape[2]
        #k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
153/43:
def dice_loss(input, target):
    smooth = 1.
    iflat = input.view(-1)
    tflat = target.view(-1)
    intersection = (iflat * tflat).sum()
    
    return 1 - ((2. * intersection + smooth) /
              (iflat.sum() + tflat.sum() + smooth))
153/44:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(27,1,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.sigma(self.tobi(img_comb[0]))
            images[ind] = img_ot
            ghost = self.sigma(self.obito(img_comb))[0]
            #ghost = ghost>0.5
            #print(ghost.shape)
            #print(targets[ind]["sem_mask"][0].shape)
            if self.train:
                seg_loss = dice_loss(ghost,targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
            ret = self.maskrcnn(images,targets)
        if self.train:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
153/45:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-2}], lr=1e-2)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.maskrcnn.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
154/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
154/2:
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
device = 'cuda'
154/3:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
154/4:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

# for im_name in os.listdir("../dataset/test"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)
154/5:
batchSize = 2
img_siz = [512,512]
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        #k_w = patch_stack.shape[2]
        #k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
154/6:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        #k_w = patch_stack.shape[2]
        #k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
154/7:
def dice_loss(input, target):
    smooth = 1.
    iflat = input.view(-1)
    tflat = target.view(-1)
    intersection = (iflat * tflat).sum()
    
    return 1 - ((2. * intersection + smooth) /
              (iflat.sum() + tflat.sum() + smooth))
154/8:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(27,1,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.sigma(self.tobi(img_comb[0]))
            images[ind] = img_ot
            ghost = self.sigma(self.obito(img_comb))[0]
            #ghost = ghost>0.5
            #print(ghost.shape)
            #print(targets[ind]["sem_mask"][0].shape)
            if self.train:
                seg_loss = dice_loss(ghost,targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
            ret = self.maskrcnn(images,targets)
        if self.train:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
154/9:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-2}], lr=1e-2)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.maskrcnn.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
154/10:
test = 0
for i in range(10000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%200==0 and i!=0:
                #evaluate(model, test_dataloader, device=device)
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
154/11:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(27,1,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.sigma(self.tobi(img_comb[0]))
            images[ind] = img_ot
            ghost = self.sigma(self.obito(img_comb))[0]
            #ghost = ghost>0.5
            #print(ghost.shape)
            #print(targets[ind]["sem_mask"][0].shape)
            if self.train:
                seg_loss = dice_loss(ghost,targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
            ret = self.maskrcnn(images,targets)
        if self.train:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
154/12:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-2}], lr=1e-2)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.maskrcnn.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
154/13:
test = 0
for i in range(10000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%200==0 and i!=0:
                #evaluate(model, test_dataloader, device=device)
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
155/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
155/2:
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
device = 'cuda'
155/3:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
155/4:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

# for im_name in os.listdir("../dataset/test"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)
155/5:
batchSize = 1
img_siz = [512,512]
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        #k_w = patch_stack.shape[2]
        #k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
155/6:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
155/7:
def dice_loss(input, target):
    smooth = 1.
    iflat = input.view(-1)
    tflat = target.view(-1)
    intersection = (iflat * tflat).sum()
    
    return 1 - ((2. * intersection + smooth) /
              (iflat.sum() + tflat.sum() + smooth))
155/8:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(27,1,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.sigma(self.tobi(img_comb[0]))
            images[ind] = img_ot
            ghost = self.sigma(self.obito(img_comb))[0]
            #ghost = ghost>0.5
            #print(ghost.shape)
            #print(targets[ind]["sem_mask"][0].shape)
            if self.train:
                seg_loss = dice_loss(ghost,targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
            ret = self.maskrcnn(images,targets)
        if self.train:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
155/9:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 5e-3}], lr=5e-3)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.maskrcnn.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
155/10:
test = 0
for i in range(10000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%200==0 and i!=0:
                #evaluate(model, test_dataloader, device=device)
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
155/11:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-3)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.maskrcnn.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
155/12:
test = 0
for i in range(10000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%200==0 and i!=0:
                #evaluate(model, test_dataloader, device=device)
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
155/13:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 5e-4}], lr=5e-4)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.maskrcnn.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
155/14:
test = 0
for i in range(10000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%200==0 and i!=0:
                #evaluate(model, test_dataloader, device=device)
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
156/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
156/2:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
156/3:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
156/4:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
157/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
158/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
158/2:
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
device = 'cuda'
158/3:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
158/4:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

# for im_name in os.listdir("../dataset/test"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)
158/5:
batchSize = 1
img_siz = [512,512]
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        #k_w = patch_stack.shape[2]
        #k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
158/6:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
158/7:
def dice_loss(input, target):
    smooth = 1.
    iflat = input.view(-1)
    tflat = target.view(-1)
    intersection = (iflat * tflat).sum()
    
    return 1 - ((2. * intersection + smooth) /
              (iflat.sum() + tflat.sum() + smooth))
158/8:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(27,1,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.sigma(self.tobi(img_comb[0]))
            images[ind] = img_ot
            ghost = self.sigma(self.obito(img_comb))[0]
            ghost = ghost>0.5
            #print(ghost.shape)
            #print(targets[ind]["sem_mask"][0].shape)
            if self.train:
                seg_loss = dice_loss(ghost,targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
            ret = self.maskrcnn(images,targets)
        if self.train:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
158/9:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 5e-4}], lr=5e-4)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.maskrcnn.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
158/10:
test = 0
for i in range(10000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%200==0 and i!=0:
                #evaluate(model, test_dataloader, device=device)
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
158/11:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 5e-6}], lr=5e-6)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.maskrcnn.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
158/12:
test = 0
for i in range(10000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%200==0 and i!=0:
                #evaluate(model, test_dataloader, device=device)
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
158/13:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm = m = nn.BatchNorm2d(100)
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(27,1,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.norm(self.tobi(img_comb[0]))
            images[ind] = img_ot
            ghost = self.norm(self.obito(img_comb))[0]
            ghost = ghost>0.5
            #print(ghost.shape)
            #print(targets[ind]["sem_mask"][0].shape)
            if self.train:
                seg_loss = dice_loss(ghost,targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
            ret = self.maskrcnn(images,targets)
        if self.train:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
158/14:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 5e-6}], lr=5e-6)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.maskrcnn.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
158/15:
test = 0
for i in range(10000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%200==0 and i!=0:
                #evaluate(model, test_dataloader, device=device)
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
158/16:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm = m = nn.BatchNorm2d(100)
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(27,1,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.norm(self.tobi(img_comb))
            images[ind] = img_ot[0]
            ghost = self.norm(self.obito(img_comb))[0]
            ghost = ghost>0.5
            #print(ghost.shape)
            #print(targets[ind]["sem_mask"][0].shape)
            if self.train:
                seg_loss = dice_loss(ghost,targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
            ret = self.maskrcnn(images,targets)
        if self.train:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
158/17:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 5e-6}], lr=5e-6)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.maskrcnn.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
158/18:
test = 0
for i in range(10000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%200==0 and i!=0:
                #evaluate(model, test_dataloader, device=device)
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
158/19:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm = m = nn.BatchNorm2d(3)
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(27,1,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.norm(self.tobi(img_comb))
            images[ind] = img_ot[0]
            ghost = self.norm(self.obito(img_comb))[0]
            ghost = ghost>0.5
            #print(ghost.shape)
            #print(targets[ind]["sem_mask"][0].shape)
            if self.train:
                seg_loss = dice_loss(ghost,targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
            ret = self.maskrcnn(images,targets)
        if self.train:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
158/20:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 5e-6}], lr=5e-6)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.maskrcnn.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
158/21:
test = 0
for i in range(10000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%200==0 and i!=0:
                #evaluate(model, test_dataloader, device=device)
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
158/22:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3c = m = nn.BatchNorm2d(3)
        self.norm1c = m = nn.BatchNorm2d(1)
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(27,1,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.norm3c(self.tobi(img_comb))
            images[ind] = img_ot[0]
            ghost = self.norm1c(self.obito(img_comb))[0]
            ghost = ghost>0.5
            #print(ghost.shape)
            #print(targets[ind]["sem_mask"][0].shape)
            if self.train:
                seg_loss = dice_loss(ghost,targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
            ret = self.maskrcnn(images,targets)
        if self.train:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
158/23:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 5e-6}], lr=5e-6)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.maskrcnn.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
158/24:
test = 0
for i in range(10000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = sum(loss for loss in loss_dict.values())

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%200==0 and i!=0:
                #evaluate(model, test_dataloader, device=device)
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
160/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
163/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
164/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
164/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
164/3:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("../dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
164/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
164/5:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
164/6:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        self.vis = False
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,3,(1,1),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm = nn.BatchNorm2d(3)
        self.ReLU = nn.ReLU()
        # self.tobi = torch.nn.Conv2d(42,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        # self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img = self.channel_trans[2](img)
            img = self.norm(img)
            images[ind] = img[0]
            if self.vis = True:
                cv2.imshow("jsr1",images[ind].detach().numpy().transpose(1,2,0))
                cv2.waitKey(0)
                cv2.destroyAllWindows() 
            #img3 = self.channel_trans[2](img)
            #img3 = self.ReLU(img3)
            # img3 = img
            # img4 = self.channel_trans[2](img3)
            # img4 = self.ReLU(img4)
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(images,targets)
        return ret
164/7:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        self.vis = False
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,3,(1,1),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm = nn.BatchNorm2d(3)
        self.ReLU = nn.ReLU()
        # self.tobi = torch.nn.Conv2d(42,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        # self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img = self.channel_trans[2](img)
            img = self.norm(img)
            images[ind] = img[0]
            if self.vis == True:
                cv2.imshow("jsr1",images[ind].detach().numpy().transpose(1,2,0))
                cv2.waitKey(0)
                cv2.destroyAllWindows() 
            #img3 = self.channel_trans[2](img)
            #img3 = self.ReLU(img3)
            # img3 = img
            # img4 = self.channel_trans[2](img3)
            # img4 = self.ReLU(img4)
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(images,targets)
        return ret
164/8:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load('jsr_v5_1060.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-4}], lr=1e-4)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.channel_trans.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
164/9:
test = 1060
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    model.vis = False
    
    if i%80==0 and i!=0:
        scheduler.step()
        print(scheduler.get_lr())
    if i%50==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
164/10:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        self.vis = False
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,3,(1,1),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm = nn.BatchNorm2d(3)
        self.ReLU = nn.ReLU()
        # self.tobi = torch.nn.Conv2d(42,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        # self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img = self.channel_trans[2](img)
            img = self.norm(img)
            images[ind] = img[0]
            if self.vis == True:
                cv2.imshow("jsr1",images[ind].detach().cpu().numpy().transpose(1,2,0))
                cv2.waitKey(0)
                cv2.destroyAllWindows() 
            #img3 = self.channel_trans[2](img)
            #img3 = self.ReLU(img3)
            # img3 = img
            # img4 = self.channel_trans[2](img3)
            # img4 = self.ReLU(img4)
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(images,targets)
        return ret
164/11:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load('jsr_v5_1060.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-4}], lr=1e-4)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.channel_trans.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
164/12:
test = 1060
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    model.vis = False
    
    if i%80==0 and i!=0:
        scheduler.step()
        print(scheduler.get_lr())
    if i%50==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
165/1:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        self.vis = False
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,3,(1,1),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm = nn.BatchNorm2d(3)
        self.ReLU = nn.ReLU()
        # self.tobi = torch.nn.Conv2d(42,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        # self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img = self.channel_trans[2](img)
            img = self.norm(img)
            images[ind] = img[0]
            if self.vis == True:
                cv2.imwrite("str_img",images[ind].detach().cpu().numpy().transpose(1,2,0))
                cv2.waitKey(0)
                cv2.destroyAllWindows() 
            #img3 = self.channel_trans[2](img)
            #img3 = self.ReLU(img3)
            # img3 = img
            # img4 = self.channel_trans[2](img3)
            # img4 = self.ReLU(img4)
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(images,targets)
        return ret
165/2:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
165/3:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
165/4:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("../dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
165/5:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
165/6:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
165/7:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        self.vis = False
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,3,(1,1),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm = nn.BatchNorm2d(3)
        self.ReLU = nn.ReLU()
        # self.tobi = torch.nn.Conv2d(42,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        # self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img = self.channel_trans[2](img)
            img = self.norm(img)
            images[ind] = img[0]
            if self.vis == True:
                cv2.imwrite("str_img.png",images[ind].detach().cpu().numpy().transpose(1,2,0))
                cv2.waitKey(0)
                cv2.destroyAllWindows() 
            #img3 = self.channel_trans[2](img)
            #img3 = self.ReLU(img3)
            # img3 = img
            # img4 = self.channel_trans[2](img3)
            # img4 = self.ReLU(img4)
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(images,targets)
        return ret
165/8:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load('jsr_v5_1060.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-4}], lr=1e-4)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.channel_trans.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
165/9:
test = 1060
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    model.vis = False
    
    if i%80==0 and i!=0:
        scheduler.step()
        print(scheduler.get_lr())
    if i%50==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
165/10:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        self.vis = False
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,3,(1,1),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm = nn.BatchNorm2d(3)
        self.ReLU = nn.ReLU()
        # self.tobi = torch.nn.Conv2d(42,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        # self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img = self.channel_trans[2](img)
            img = self.norm(img)
            images[ind] = img[0]
            if self.vis == True:
                cv2.imwrite("str_img.png",images[ind].detach().cpu().numpy().transpose(1,2,0))
            #img3 = self.channel_trans[2](img)
            #img3 = self.ReLU(img3)
            # img3 = img
            # img4 = self.channel_trans[2](img3)
            # img4 = self.ReLU(img4)
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(images,targets)
        return ret
165/11:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load('jsr_v5_1060.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-4}], lr=1e-4)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.channel_trans.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
165/12:
test = 1060
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    model.vis = False
    
    if i%80==0 and i!=0:
        scheduler.step()
        print(scheduler.get_lr())
    if i%50==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
165/13:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load('jsr_v5_1060.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-4}], lr=1e-4)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.channel_trans.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
165/14:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    model.vis = False
    
    if i%80==0 and i!=0:
        scheduler.step()
        print(scheduler.get_lr())
    if i%50==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
165/15:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        self.vis = False
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,3,(1,1),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3 = nn.BatchNorm2d(3)
        self.norm12 = nn.BatchNorm2d(12)
        self.ReLU = nn.ReLU()
        # self.tobi = torch.nn.Conv2d(42,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        # self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.norm12(img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img = self.channel_trans[2](img)
            img = self.norm(img)
            images[ind] = img[0]
            if self.vis == True:
                cv2.imwrite("mod_img.bmp",images[ind].detach().cpu().numpy().transpose(1,2,0))
                cv2.imwrite("og_img.bmp",images[ind].detach().cpu().numpy().transpose(1,2,0))
            #img3 = self.channel_trans[2](img)
            #img3 = self.ReLU(img3)
            # img3 = img
            # img4 = self.channel_trans[2](img3)
            # img4 = self.ReLU(img4)
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(images,targets)
        return ret
165/16:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load('jsr_v5_1060.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-4}], lr=1e-4)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.channel_trans.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
165/17:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    model.vis = False
    
    if i%80==0 and i!=0:
        scheduler.step()
        print(scheduler.get_lr())
    if i%50==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
165/18:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        self.vis = False
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,3,(1,1),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3 = nn.BatchNorm2d(3)
        self.norm12 = nn.BatchNorm2d(12)
        self.ReLU = nn.ReLU()
        # self.tobi = torch.nn.Conv2d(42,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        # self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.norm12(img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img = self.channel_trans[2](img)
            img = self.norm3(img)
            images[ind] = img[0]
            if self.vis == True:
                cv2.imwrite("mod_img.bmp",images[ind].detach().cpu().numpy().transpose(1,2,0))
                cv2.imwrite("og_img.bmp",images[ind].detach().cpu().numpy().transpose(1,2,0))
            #img3 = self.channel_trans[2](img)
            #img3 = self.ReLU(img3)
            # img3 = img
            # img4 = self.channel_trans[2](img3)
            # img4 = self.ReLU(img4)
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(images,targets)
        return ret
165/19:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load('jsr_v5_1060.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-4}], lr=1e-4)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.channel_trans.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
165/20:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    model.vis = False
    
    if i%80==0 and i!=0:
        scheduler.step()
        print(scheduler.get_lr())
    if i%50==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
165/21:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        self.vis = False
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,3,(1,1),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3 = nn.BatchNorm2d(3)
        self.norm12 = nn.BatchNorm2d(12)
        self.ReLU = nn.ReLU()
        # self.tobi = torch.nn.Conv2d(42,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        # self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.norm12(img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img = self.channel_trans[2](img)
            img = self.norm3(img)
            images[ind] = img[0]
            if self.vis == True:
                cv2.imwrite("mod_img.bmp",images[ind].copy().cpu().numpy().transpose(1,2,0))
                cv2.imwrite("og_img.bmp",imgog[0].copy().cpu().numpy().transpose(1,2,0))
            #img3 = self.channel_trans[2](img)
            #img3 = self.ReLU(img3)
            # img3 = img
            # img4 = self.channel_trans[2](img3)
            # img4 = self.ReLU(img4)
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(images,targets)
        return ret
165/22:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load('jsr_v5_1060.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-4}], lr=1e-4)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.channel_trans.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
165/23:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    model.vis = False
    
    if i%80==0 and i!=0:
        scheduler.step()
        print(scheduler.get_lr())
    if i%50==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
165/24:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        self.vis = False
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,3,(1,1),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3 = nn.BatchNorm2d(3)
        self.norm12 = nn.BatchNorm2d(12)
        self.ReLU = nn.ReLU()
        # self.tobi = torch.nn.Conv2d(42,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        # self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.norm12(img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img = self.channel_trans[2](img)
            img = self.norm3(img)
            images[ind] = img[0]
            if self.vis == True:
                cv2.imwrite("mod_img.bmp",images[ind].detach().cpu().numpy().transpose(1,2,0))
                cv2.imwrite("og_img.bmp",imgog[0].detach().cpu().numpy().transpose(1,2,0))
            #img3 = self.channel_trans[2](img)
            #img3 = self.ReLU(img3)
            # img3 = img
            # img4 = self.channel_trans[2](img3)
            # img4 = self.ReLU(img4)
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(images,targets)
        return ret
165/25:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load('jsr_v5_1060.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-4}], lr=1e-4)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.channel_trans.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
165/26:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    model.vis = False
    
    if i%80==0 and i!=0:
        scheduler.step()
        print(scheduler.get_lr())
    if i%50==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
165/27:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        self.vis = False
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,3,(1,1),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3 = nn.BatchNorm2d(3)
        self.norm12 = nn.BatchNorm2d(12)
        self.ReLU = nn.ReLU()
        # self.tobi = torch.nn.Conv2d(42,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        # self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.norm12(img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img = self.channel_trans[2](img)
            img = self.norm3(img)
            images[ind] = img[0]
            if self.vis == True:
                cv2.imwrite("mod_img.bmp",img[0].cpu().numpy().transpose(1,2,0))
                cv2.imwrite("og_img.bmp",imgog[0].cpu().numpy().transpose(1,2,0))
            #img3 = self.channel_trans[2](img)
            #img3 = self.ReLU(img3)
            # img3 = img
            # img4 = self.channel_trans[2](img3)
            # img4 = self.ReLU(img4)
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(images,targets)
        return ret
165/28:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load('jsr_v5_1060.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-4}], lr=1e-4)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.channel_trans.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
165/29:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    model.vis = False
    
    if i%80==0 and i!=0:
        scheduler.step()
        print(scheduler.get_lr())
    if i%50==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
165/30:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        self.vis = False
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,3,(1,1),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3 = nn.BatchNorm2d(3)
        self.norm12 = nn.BatchNorm2d(12)
        self.ReLU = nn.ReLU()
        # self.tobi = torch.nn.Conv2d(42,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        # self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.norm12(img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img = self.channel_trans[2](img)
            img = self.norm3(img)
            images[ind] = img[0]
            if self.vis == True:
                cv2.imwrite("mod_img.bmp",img[0].clone().detach().cpu().numpy().transpose(1,2,0))
                cv2.imwrite("og_img.bmp",imgog[0].clone().detach().cpu().numpy().transpose(1,2,0))
            #img3 = self.channel_trans[2](img)
            #img3 = self.ReLU(img3)
            # img3 = img
            # img4 = self.channel_trans[2](img3)
            # img4 = self.ReLU(img4)
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(images,targets)
        return ret
165/31:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load('jsr_v5_1060.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-4}], lr=1e-4)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.channel_trans.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
165/32:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    model.vis = False
    
    if i%80==0 and i!=0:
        scheduler.step()
        print(scheduler.get_lr())
    if i%50==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
165/33:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        self.vis = False
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,3,(1,1),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3 = nn.BatchNorm2d(3)
        self.norm12 = nn.BatchNorm2d(12)
        self.ReLU = nn.ReLU()
        # self.tobi = torch.nn.Conv2d(42,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        # self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.norm12(img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img = self.channel_trans[2](img)
            img = self.norm3(img)
            images[ind] = img[0]
            if self.vis == True:
                cv2.imwrite("mod_img.png",img[0].clone().detach().cpu().numpy().transpose(1,2,0))
                cv2.imwrite("og_img.png",imgog[0].clone().detach().cpu().numpy().transpose(1,2,0))
            #img3 = self.channel_trans[2](img)
            #img3 = self.ReLU(img3)
            # img3 = img
            # img4 = self.channel_trans[2](img3)
            # img4 = self.ReLU(img4)
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(images,targets)
        return ret
165/34:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load('jsr_v5_1060.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-4}], lr=1e-4)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.channel_trans.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
165/35:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    model.vis = False
    
    if i%80==0 and i!=0:
        scheduler.step()
        print(scheduler.get_lr())
    if i%50==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
165/36:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        self.vis = False
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,3,(1,1),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3 = nn.BatchNorm2d(3)
        self.norm12 = nn.BatchNorm2d(12)
        self.ReLU = nn.ReLU()
        # self.tobi = torch.nn.Conv2d(42,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        # self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.norm12(img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img = self.channel_trans[2](img)
            img = self.norm3(img)
            images[ind] = img[0]
            if self.vis == True:
                cv2.imwrite("mod_img.png",img[0].clone().detach().cpu().numpy()*255.transpose(1,2,0))
                cv2.imwrite("og_img.png",imgog[0].clone().detach().cpu().numpy()*255.transpose(1,2,0))
            #img3 = self.channel_trans[2](img)
            #img3 = self.ReLU(img3)
            # img3 = img
            # img4 = self.channel_trans[2](img3)
            # img4 = self.ReLU(img4)
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(images,targets)
        return ret
165/37:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        self.vis = False
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,3,(1,1),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3 = nn.BatchNorm2d(3)
        self.norm12 = nn.BatchNorm2d(12)
        self.ReLU = nn.ReLU()
        # self.tobi = torch.nn.Conv2d(42,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        # self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.norm12(img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img = self.channel_trans[2](img)
            img = self.norm3(img)
            images[ind] = img[0]
            if self.vis == True:
                cv2.imwrite("mod_img.png",img[0].clone().detach().cpu().numpy()*255.transpose(1,2,0))
                cv2.imwrite("og_img.png",imgog[0].clone().detach().cpu().numpy()*255.transpose(1,2,0))
            #img3 = self.channel_trans[2](img)
            #img3 = self.ReLU(img3)
            # img3 = img
            # img4 = self.channel_trans[2](img3)
            # img4 = self.ReLU(img4)
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(images,targets)
        return ret
165/38:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        self.vis = False
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,3,(1,1),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3 = nn.BatchNorm2d(3)
        self.norm12 = nn.BatchNorm2d(12)
        self.ReLU = nn.ReLU()
        # self.tobi = torch.nn.Conv2d(42,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        # self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.norm12(img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img = self.channel_trans[2](img)
            img = self.norm3(img)
            images[ind] = img[0]
            if self.vis == True:
                cv2.imwrite("mod_img.png",(img[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            #img3 = self.channel_trans[2](img)
            #img3 = self.ReLU(img3)
            # img3 = img
            # img4 = self.channel_trans[2](img3)
            # img4 = self.ReLU(img4)
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(images,targets)
        return ret
165/39:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load('jsr_v5_1060.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-4}], lr=1e-4)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.channel_trans.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
165/40:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    model.vis = False
    
    if i%80==0 and i!=0:
        scheduler.step()
        print(scheduler.get_lr())
    if i%50==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
165/41:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        self.vis = False
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,3,(1,1),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3 = nn.BatchNorm2d(3)
        self.norm12 = nn.BatchNorm2d(12)
        self.ReLU = nn.ReLU()
        # self.tobi = torch.nn.Conv2d(42,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        # self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.norm12(img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img = self.channel_trans[2](img)
            img = self.norm3(img)
            images[ind] = img[0]
            if self.vis == True:
                cv2.imwrite("mod_img.png",(img[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            #img3 = self.channel_trans[2](img)
            #img3 = self.ReLU(img3)
            # img3 = img
            # img4 = self.channel_trans[2](img3)
            # img4 = self.ReLU(img4)
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(images,targets)
        return ret
165/42:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load('jsr_v5_1060.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-4}], lr=1e-4)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.channel_trans.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
165/43:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    model.vis = False
    
    if i%80==0 and i!=0:
        scheduler.step()
        print(scheduler.get_lr())
    if i%50==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5.1_"+str(+test+i)+".torch")
166/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
166/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
166/3:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

# for im_name in os.listdir("../dataset/test"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)
166/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        #k_w = patch_stack.shape[2]
        #k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
166/5:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
166/6:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        self.vis = False
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,3,(1,1),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3 = nn.BatchNorm2d(3)
        self.norm12 = nn.BatchNorm2d(12)
        self.ReLU = nn.ReLU()
        # self.tobi = torch.nn.Conv2d(42,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        # self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.norm12(img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img = self.channel_trans[2](img)
            img = self.norm3(img)
            images[ind] = img[0]
            if self.vis == True:
                cv2.imwrite("mod_img.png",(img[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            #img3 = self.channel_trans[2](img)
            #img3 = self.ReLU(img3)
            # img3 = img
            # img4 = self.channel_trans[2](img3)
            # img4 = self.ReLU(img4)
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(images,targets)
        return ret
166/7:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load('jsr_v5_1060.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-4}], lr=1e-4)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.channel_trans.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
166/8:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    model.vis = False
    
    if i%80==0 and i!=0:
        scheduler.step()
        print(scheduler.get_lr())
    if i%50==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5.1_"+str(+test+i)+".torch")
167/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
167/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
167/3:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

# for im_name in os.listdir("../dataset/test"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)
167/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        #k_w = patch_stack.shape[2]
        #k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
167/5:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
167/6:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        self.vis = False
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,3,(1,1),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3 = nn.BatchNorm2d(3)
        self.norm12 = nn.BatchNorm2d(12)
        self.ReLU = nn.ReLU()
        # self.tobi = torch.nn.Conv2d(42,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        # self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.norm12(img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img = self.channel_trans[2](img)
            img = self.norm3(img)
            images[ind] = img[0]
            if self.vis == True:
                cv2.imwrite("mod_img.png",(img[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            #img3 = self.channel_trans[2](img)
            #img3 = self.ReLU(img3)
            # img3 = img
            # img4 = self.channel_trans[2](img3)
            # img4 = self.ReLU(img4)
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(images,targets)
        return ret
167/7:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load('jsr_v5_1060.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-4}], lr=1e-4)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.channel_trans.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
167/8:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    model.vis = False
    
    if i%80==0 and i!=0:
        scheduler.step()
        print(scheduler.get_lr())
    if i%50==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5.1_"+str(+test+i)+".torch")
167/9:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
model.load_state_dict(torch.load('jsr_v5.1_800.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-5},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-6}], lr=1e-6)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.channel_trans.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
167/10:
test = 800
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    model.vis = False
    
    if i%30==0 and i!=0:
        scheduler.step()
        print(scheduler.get_lr())
    if i%50==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5.1_"+str(+test+i)+".torch")
169/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
169/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
169/3:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

# for im_name in os.listdir("../dataset/test"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)
169/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        #k_w = patch_stack.shape[2]
        #k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
169/5:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
169/6:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        self.vis = False
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,3,(1,1),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3 = nn.BatchNorm2d(3)
        self.norm12 = nn.BatchNorm2d(12)
        self.ReLU = nn.ReLU()
        # self.tobi = torch.nn.Conv2d(42,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        # self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.norm12(img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img = self.channel_trans[2](img)
            img = self.norm3(img)
            images[ind] = img[0]
            if self.vis == True:
                cv2.imwrite("mod_img.png",(img[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            #img3 = self.channel_trans[2](img)
            #img3 = self.ReLU(img3)
            # img3 = img
            # img4 = self.channel_trans[2](img3)
            # img4 = self.ReLU(img4)
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(images,targets)
        return ret
169/7:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load('jsr_v5.1_800.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-4}], lr=1e-4)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.95
# for param in model.channel_trans.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
169/8:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    model.vis = False
    
    if i%10==0 and i!=0:
        scheduler.step()
        print(scheduler.get_lr())
    if i%50==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5.1_"+str(+test+i)+".torch")
169/9:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
model.load_state_dict(torch.load('jsr_v5.1_700.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-6},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-10}], lr=1e-10)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.channel_trans.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
169/10:
test = 700
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    model.vis = False
    
    if i%30==0 and i!=0:
        scheduler.step()
        print(scheduler.get_lr())
    if i%50==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5.1_"+str(+test+i)+".torch")
169/11:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
model.load_state_dict(torch.load('jsr_v5.1_700.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-6},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-10}], lr=1e-10)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.9
for param in model.maskrcnn.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
169/12:
test = 700
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    model.vis = False
    
    if i%10==0 and i!=0:
        scheduler.step()
        print(scheduler.get_lr())
    if i%50==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5.1_"+str(+test+i)+".torch")
169/13:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
model.load_state_dict(torch.load('jsr_v5.1_700.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-6},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-10}], lr=1e-10)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.9
for param in model.maskrcnn.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
169/14:
test = 700
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    model.vis = False
    
    if i%10==0 and i!=0:
        scheduler.step()
        print(scheduler.get_lr())
    if i%50==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5.1_"+str(+test+i)+".torch")
170/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
170/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
170/3:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

# for im_name in os.listdir("../dataset/test"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)
170/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        #k_w = patch_stack.shape[2]
        #k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
170/5:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
170/6:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        self.vis = False
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,3,(1,1),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3 = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm12 = nn.BatchNorm2d(12,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        # self.tobi = torch.nn.Conv2d(42,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        # self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.norm12(img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img = self.channel_trans[2](img)
            img = self.norm3(img)
            images[ind] = img[0]
            if self.vis == True:
                cv2.imwrite("mod_img.png",(img[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            #img3 = self.channel_trans[2](img)
            #img3 = self.ReLU(img3)
            # img3 = img
            # img4 = self.channel_trans[2](img3)
            # img4 = self.ReLU(img4)
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(images,targets)
        return ret
170/7:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load('jsr_v5.1_700.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-4}], lr=1e-4)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.9
for param in model.maskrcnn.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
170/8:
test = 700
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    model.vis = False
    
    if i%40==0 and i!=0:
        scheduler.step()
        print(scheduler.get_lr())
    if i%50==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5.1_"+str(+test+i)+".torch")
172/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
172/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
172/3:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

# for im_name in os.listdir("../dataset/test"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)
172/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        #k_w = patch_stack.shape[2]
        #k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
172/5:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
172/6:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        self.vis = False
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,3,(1,1),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3 = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm12 = nn.BatchNorm2d(12,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        # self.tobi = torch.nn.Conv2d(42,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        # self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.norm12(img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img = self.channel_trans[2](img)
            img = self.norm3(img)
            images[ind] = img[0]
            if self.vis == True:
                cv2.imwrite("mod_img.png",(img[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            #img3 = self.channel_trans[2](img)
            #img3 = self.ReLU(img3)
            # img3 = img
            # img4 = self.channel_trans[2](img3)
            # img4 = self.ReLU(img4)
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(images,targets)
        return ret
172/7:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load('jsr_v5.1_700.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-4}], lr=1e-4)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.9
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
172/8:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    model.vis = False
    
    if i%40==0 and i!=0:
        scheduler.step()
        print(scheduler.get_lr())
    if i%50==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5.1_"+str(+test+i)+".torch")
175/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
175/2:
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
device = 'cuda'
175/3:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
175/4:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

# for im_name in os.listdir("../dataset/test"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)
175/5:
batchSize = 1
img_siz = [512,512]
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        #k_w = patch_stack.shape[2]
        #k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
175/6:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
175/7:
def dice_loss(input, target):
    smooth = 1.
    iflat = input.view(-1)
    tflat = target.view(-1)
    intersection = (iflat * tflat).sum()
    
    return 1 - ((2. * intersection + smooth) /
              (iflat.sum() + tflat.sum() + smooth))
175/8:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3c = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm1c = nn.BatchNorm2d(1,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(27,1,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.norm3c(self.tobi(img_comb))
            images[ind] = img_ot[0]
            ghost = self.norm1c(self.obito(img_comb))[0]
            #ghost = ghost>0.5
            #print(ghost.shape)
            #print(targets[ind]["sem_mask"][0].shape)
            if self.train:
                seg_loss = dice_loss(ghost,targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            if self.vis == True:
                cv2.imwrite("ghost_img.png",(ghost.clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))

        ret = self.maskrcnn(images,targets)
        if self.train:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
175/9:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-4}], lr=1e-4)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.maskrcnn.parameters():
    param.requires_grad = False
for param in model.tobi.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
175/10:
test = 0
for i in range(10000):
            images, targets = loadBatch()
            images = list(image.to(device) for image in images)
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            optimizer.zero_grad()
            loss_dict = model(images, targets)
            # for losst in loss_dict.keys():
            #     print(losst,loss_dict[losst])
            losses = loss_dict["color_loss"].values()

            losses.backward()
            optimizer.step()
            print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
            if i%200==0:
                scheduler.step()
            if i%200==0 and i!=0:
                #evaluate(model, test_dataloader, device=device)
                model.train()
                torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
175/11:
def dice_loss(input, target):
    smooth = 1.
    iflat = input.view(-1)
    tflat = target.view(-1)
    intersection = (iflat * tflat).sum()
    
    return 1 - ((2. * intersection + smooth) /
              (iflat.sum() + tflat.sum() + smooth))
175/12:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.vis = False
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3c = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm1c = nn.BatchNorm2d(1,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(27,1,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.norm3c(self.tobi(img_comb))
            images[ind] = img_ot[0]
            ghost = self.norm1c(self.obito(img_comb))[0]
            #ghost = ghost>0.5
            #print(ghost.shape)
            #print(targets[ind]["sem_mask"][0].shape)
            if self.train:
                seg_loss = dice_loss(ghost,targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            if self.vis == True:
                cv2.imwrite("ghost_img.png",(ghost.clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))

        ret = self.maskrcnn(images,targets)
        if self.train:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
175/13:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-4}], lr=1e-4)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.maskrcnn.parameters():
    param.requires_grad = False
for param in model.tobi.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
175/14:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = loss_dict["color_loss"].values()

    losses.backward()
    optimizer.step()
    model.vis = False
    print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
    if i%200==0:
        scheduler.step()
    if i%200==0 and i!=0:
        #evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
175/15:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
175/16:
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
device = 'cuda'
175/17:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
175/18:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

# for im_name in os.listdir("../dataset/test"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)
175/19:
batchSize = 1
img_siz = [512,512]
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        #k_w = patch_stack.shape[2]
        #k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
175/20:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
175/21:
def dice_loss(input, target):
    smooth = 1.
    iflat = input.view(-1)
    tflat = target.view(-1)
    intersection = (iflat * tflat).sum()
    
    return 1 - ((2. * intersection + smooth) /
              (iflat.sum() + tflat.sum() + smooth))
175/22:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.vis = False
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3c = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm1c = nn.BatchNorm2d(1,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(27,1,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.norm3c(self.tobi(img_comb))
            images[ind] = img_ot[0]
            ghost = self.norm1c(self.obito(img_comb))[0]
            #ghost = ghost>0.5
            #print(ghost.shape)
            #print(targets[ind]["sem_mask"][0].shape)
            if self.train:
                seg_loss = dice_loss(ghost,targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            if self.vis == True:
                cv2.imwrite("ghost_img.png",(ghost.clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))

        ret = self.maskrcnn(images,targets)
        if self.train:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
175/23:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-4}], lr=1e-4)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.maskrcnn.parameters():
    param.requires_grad = False
for param in model.tobi.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
175/24:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = loss_dict["color_loss"].item()

    losses.backward()
    optimizer.step()
    model.vis = False
    print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
    if i%200==0:
        scheduler.step()
    if i%200==0 and i!=0:
        #evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
175/25:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.vis = False
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3c = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm1c = nn.BatchNorm2d(1,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(27,1,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.norm3c(self.tobi(img_comb))
            images[ind] = img_ot[0]
            ghost = self.norm1c(self.obito(img_comb))[0]
            #ghost = ghost>0.5
            #print(ghost.shape)
            #print(targets[ind]["sem_mask"][0].shape)
            if self.train:
                seg_loss = dice_loss(ghost,targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            if self.vis == True:
                cv2.imwrite("ghost_img.png",(ghost.clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))

        ret = self.maskrcnn(images,targets)
        if self.train:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
175/26:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-4}], lr=1e-4)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.maskrcnn.parameters():
    param.requires_grad = False
for param in model.tobi.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
175/27:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())
    losses = loss_dict["color_loss"]

    losses.backward()
    optimizer.step()
    model.vis = False
    print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
    if i%200==0:
        scheduler.step()
    if i%200==0 and i!=0:
        #evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
175/28:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.vis = False
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3c = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm1c = nn.BatchNorm2d(1,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(27,1,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.norm3c(self.tobi(img_comb))
            images[ind] = img_ot[0]
            ghost = self.norm1c(self.obito(img_comb))[0]
            #ghost = ghost>0.5
            #print(ghost.shape)
            #print(targets[ind]["sem_mask"][0].shape)
            if self.train:
                seg_loss = dice_loss(ghost,targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            if self.vis == True:
                cv2.imwrite("ghost_img.png",(ghost.clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("ot_img.png",(img_ot[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))

        ret = self.maskrcnn(images,targets)
        if self.train:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
175/29:
#model = jsrcnn_v5(box_nms_thresh=0.5)
#model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-6}], lr=1e-6)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
# for param in model.tobi.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
175/30:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    loss_dict.pop("color_loss")
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())
    # losses = loss_dict["color_loss"]

    losses.backward()
    optimizer.step()
    model.vis = False
    print(i,'loss:', losses.item()," color_loss:",loss_dict["color_loss"].item())
    if i%200==0:
        scheduler.step()
    if i%200==0 and i!=0:
        #evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
175/31:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    loss_dict.pop("color_loss")
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())
    # losses = loss_dict["color_loss"]

    losses.backward()
    optimizer.step()
    model.vis = False
    print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    if i%200==0:
        scheduler.step()
    if i%200==0 and i!=0:
        #evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
177/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
177/2:
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
device = 'cuda'
177/3:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
177/4:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

# for im_name in os.listdir("../dataset/test"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)
177/5:
batchSize = 1
img_siz = [512,512]
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        #k_w = patch_stack.shape[2]
        #k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
177/6:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
177/7:
def dice_loss(input, target):
    smooth = 1.
    iflat = input.view(-1)
    tflat = target.view(-1)
    intersection = (iflat * tflat).sum()
    
    return 1 - ((2. * intersection + smooth) /
              (iflat.sum() + tflat.sum() + smooth))
177/8:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.vis = False
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        self.BC = torch.nn.BCELoss()
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3c = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm1c = nn.BatchNorm2d(1,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(27,1,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.norm3c(self.tobi(img_comb))
            images[ind] = img_ot[0]
            ghost = self.norm1c(self.obito(img_comb))[0]
            #ghost = ghost>0.5
            print(ghost.shape)
            print(targets[ind]["sem_mask"][0].shape)
            if self.train:
                seg_loss = self.BC(ghost,targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            if self.vis == True:
                cv2.imwrite("ghost_img.png",(ghost.clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("ot_img.png",(img_ot[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))

        ret = self.maskrcnn(images,targets)
        if self.train:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
177/9:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-6}], lr=1e-6)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
# for param in model.tobi.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
177/10:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    loss_dict.pop("color_loss")
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())
    # losses = loss_dict["color_loss"]

    losses.backward()
    optimizer.step()
    model.vis = False
    print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    if i%200==0:
        scheduler.step()
    if i%200==0 and i!=0:
        #evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
177/11:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.vis = False
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        self.BC = torch.nn.BCELoss()
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3c = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm1c = nn.BatchNorm2d(1,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(27,1,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.norm3c(self.tobi(img_comb))
            print(img_ot[0].shape)
            images[ind] = img_ot[0]
            ghost = self.norm1c(self.obito(img_comb))[0]
            #ghost = ghost>0.5
            print(ghost[0].shape)
            print(targets[ind]["sem_mask"][0].shape)
            if self.train:
                seg_loss = self.BC(ghost,targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            if self.vis == True:
                cv2.imwrite("ghost_img.png",(ghost.clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("ot_img.png",(img_ot[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))

        ret = self.maskrcnn(images,targets)
        if self.train:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
177/12:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-6}], lr=1e-6)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
# for param in model.tobi.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
177/13:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    loss_dict.pop("color_loss")
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())
    # losses = loss_dict["color_loss"]

    losses.backward()
    optimizer.step()
    model.vis = False
    print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    if i%200==0:
        scheduler.step()
    if i%200==0 and i!=0:
        #evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
177/14:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
177/15:
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
device = 'cuda'
177/16:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
177/17:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

# for im_name in os.listdir("../dataset/test"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)
177/18:
batchSize = 1
img_siz = [512,512]
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        #k_w = patch_stack.shape[2]
        #k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
177/19:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
177/20:
def dice_loss(input, target):
    smooth = 1.
    iflat = input.view(-1)
    tflat = target.view(-1)
    intersection = (iflat * tflat).sum()
    
    return 1 - ((2. * intersection + smooth) /
              (iflat.sum() + tflat.sum() + smooth))
177/21:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.vis = False
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        self.BC = torch.nn.BCELoss()
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3c = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm1c = nn.BatchNorm2d(1,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(27,1,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.norm3c(self.tobi(img_comb))
            #print(img_ot[0].shape)
            images[ind] = img_ot[0]
            ghost = self.norm1c(self.obito(img_comb))
            #ghost = ghost>0.5
            print(ghost[0].shape)
            print(targets[ind]["sem_mask"][0].shape)
            if self.train:
                seg_loss = self.BC(ghost[0],targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            if self.vis == True:
                cv2.imwrite("ghost_img.png",(ghost.clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("ot_img.png",(img_ot[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))

        ret = self.maskrcnn(images,targets)
        if self.train:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
177/22:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-6}], lr=1e-6)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
# for param in model.tobi.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
177/23:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    loss_dict.pop("color_loss")
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())
    # losses = loss_dict["color_loss"]

    losses.backward()
    optimizer.step()
    model.vis = False
    print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    if i%200==0:
        scheduler.step()
    if i%200==0 and i!=0:
        #evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
177/24:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.vis = False
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        self.BC = torch.nn.BCELoss()
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3c = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm1c = nn.BatchNorm2d(1,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(27,1,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.norm3c(self.tobi(img_comb))
            #print(img_ot[0].shape)
            images[ind] = img_ot[0]
            ghost = self.norm1c(self.obito(img_comb))
            #ghost = ghost>0.5
            print(ghost[0].shape)
            print(targets[ind]["sem_mask"][0].shape)
            if self.train:
                seg_loss = self.BC(ghost,targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            if self.vis == True:
                cv2.imwrite("ghost_img.png",(ghost.clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("ot_img.png",(img_ot[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))

        ret = self.maskrcnn(images,targets)
        if self.train:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
177/25:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-6}], lr=1e-6)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
# for param in model.tobi.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
177/26:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    loss_dict.pop("color_loss")
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())
    # losses = loss_dict["color_loss"]

    losses.backward()
    optimizer.step()
    model.vis = False
    print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    if i%200==0:
        scheduler.step()
    if i%200==0 and i!=0:
        #evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
177/27:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.vis = False
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        self.BC = torch.nn.BCELoss()
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3c = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm1c = nn.BatchNorm2d(1,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(27,1,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.norm3c(self.tobi(img_comb))
            #print(img_ot[0].shape)
            images[ind] = img_ot[0]
            ghost = self.norm1c(self.obito(img_comb))
            #ghost = ghost>0.5
            print(ghost[0])
            print(targets[ind]["sem_mask"][0].shape)
            if self.train:
                seg_loss = self.BC(ghost,targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            if self.vis == True:
                cv2.imwrite("ghost_img.png",(ghost.clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("ot_img.png",(img_ot[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))

        ret = self.maskrcnn(images,targets)
        if self.train:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
177/28:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-6}], lr=1e-6)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
# for param in model.tobi.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
177/29:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    loss_dict.pop("color_loss")
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())
    # losses = loss_dict["color_loss"]

    losses.backward()
    optimizer.step()
    model.vis = False
    print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    if i%200==0:
        scheduler.step()
    if i%200==0 and i!=0:
        #evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
177/30:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.vis = False
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        self.BC = torch.nn.BCELoss()
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3c = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm1c = nn.BatchNorm2d(1,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(27,1,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.ReLU(self.norm3c(self.tobi(img_comb)))
            #print(img_ot[0].shape)
            images[ind] = img_ot[0]
            ghost = self.ReLU(self.norm1c(self.obito(img_comb)))
            #ghost = ghost>0.5
            print(ghost[0].shape)
            print(targets[ind]["sem_mask"][0].shape)
            if self.train:
                seg_loss = self.BC(ghost,targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            if self.vis == True:
                cv2.imwrite("ghost_img.png",(ghost.clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("ot_img.png",(img_ot[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))

        ret = self.maskrcnn(images,targets)
        if self.train:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
177/31:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-6}], lr=1e-6)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
# for param in model.tobi.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
177/32:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    loss_dict.pop("color_loss")
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())
    # losses = loss_dict["color_loss"]

    losses.backward()
    optimizer.step()
    model.vis = False
    print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    if i%200==0:
        scheduler.step()
    if i%200==0 and i!=0:
        #evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
177/33:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.vis = False
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        self.BC = torch.nn.BCELoss()
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3c = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm1c = nn.BatchNorm2d(1,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(27,1,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.ReLU(self.norm3c(self.tobi(img_comb)))
            #print(img_ot[0].shape)
            images[ind] = img_ot[0]
            ghost = self.ReLU(self.norm1c(self.obito(img_comb)))
            #ghost = ghost>0.5
            print(ghost[0].shape)
            print(targets[ind]["sem_mask"][0].shape)
            if self.train:
                seg_loss = self.BC(ghost[0][0],targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            if self.vis == True:
                cv2.imwrite("ghost_img.png",(ghost.clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("ot_img.png",(img_ot[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))

        ret = self.maskrcnn(images,targets)
        if self.train:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
177/34:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-6}], lr=1e-6)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
# for param in model.tobi.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
177/35:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    loss_dict.pop("color_loss")
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())
    # losses = loss_dict["color_loss"]

    losses.backward()
    optimizer.step()
    model.vis = False
    print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    if i%200==0:
        scheduler.step()
    if i%200==0 and i!=0:
        #evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
177/36:
batchSize = 1
img_siz = [512,512]
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        #k_w = patch_stack.shape[2]
        #k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.float32)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.float32)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
177/37:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
177/38:
def dice_loss(input, target):
    smooth = 1.
    iflat = input.view(-1)
    tflat = target.view(-1)
    intersection = (iflat * tflat).sum()
    
    return 1 - ((2. * intersection + smooth) /
              (iflat.sum() + tflat.sum() + smooth))
177/39:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.vis = False
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        self.BC = torch.nn.BCELoss()
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3c = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm1c = nn.BatchNorm2d(1,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(27,1,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.ReLU(self.norm3c(self.tobi(img_comb)))
            #print(img_ot[0].shape)
            images[ind] = img_ot[0]
            ghost = self.ReLU(self.norm1c(self.obito(img_comb)))
            #ghost = ghost>0.5
            print(ghost[0].shape)
            print(targets[ind]["sem_mask"][0].shape)
            if self.train:
                seg_loss = self.BC(ghost[0][0],targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            if self.vis == True:
                cv2.imwrite("ghost_img.png",(ghost.clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("ot_img.png",(img_ot[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))

        ret = self.maskrcnn(images,targets)
        if self.train:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
177/40:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-6}], lr=1e-6)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
# for param in model.tobi.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
177/41:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    loss_dict.pop("color_loss")
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())
    # losses = loss_dict["color_loss"]

    losses.backward()
    optimizer.step()
    model.vis = False
    print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    if i%200==0:
        scheduler.step()
    if i%200==0 and i!=0:
        #evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
178/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
178/2:
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
device = 'cuda'
178/3:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
178/4:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

# for im_name in os.listdir("../dataset/test"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)
178/5:
batchSize = 1
img_siz = [512,512]
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        #k_w = patch_stack.shape[2]
        #k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.float32)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.float32)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
178/6:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
178/7:
def dice_loss(input, target):
    smooth = 1.
    iflat = input.view(-1)
    tflat = target.view(-1)
    intersection = (iflat * tflat).sum()
    
    return 1 - ((2. * intersection + smooth) /
              (iflat.sum() + tflat.sum() + smooth))
178/8:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.vis = False
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        self.BC = torch.nn.BCELoss()
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3c = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm1c = nn.BatchNorm2d(1,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(27,1,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.ReLU(self.norm3c(self.tobi(img_comb)))
            #print(img_ot[0].shape)
            images[ind] = img_ot[0]
            ghost = self.ReLU(self.norm1c(self.obito(img_comb)))
            #ghost = ghost>0.5
            print(ghost[0].shape)
            print(targets[ind]["sem_mask"][0].shape)
            if self.train:
                seg_loss = self.BC(ghost[0][0],targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            if self.vis == True:
                cv2.imwrite("ghost_img.png",(ghost.clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("ot_img.png",(img_ot[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))

        ret = self.maskrcnn(images,targets)
        if self.train:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
178/9:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-6}], lr=1e-6)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
# for param in model.tobi.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
178/10:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    loss_dict.pop("color_loss")
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())
    # losses = loss_dict["color_loss"]

    losses.backward()
    optimizer.step()
    model.vis = False
    print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    if i%200==0:
        scheduler.step()
    if i%200==0 and i!=0:
        #evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
178/11:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.vis = False
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        self.BC = torch.nn.BCELoss()
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3c = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm1c = nn.BatchNorm2d(1,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(27,1,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.ReLU(self.norm3c(self.tobi(img_comb)))
            #print(img_ot[0].shape)
            images[ind] = img_ot[0]
            ghost = self.ReLU(self.norm1c(self.obito(img_comb)))
            #ghost = ghost>0.5
            print(ghost[0].shape)
            print(targets[ind]["sem_mask"][0].shape)
            if self.train:
                seg_loss = self.BC(ghost[0][0],targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            if self.vis == True:
                cv2.imwrite("ghost_img.png",(ghost[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("ot_img.png",(img_ot[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))

        ret = self.maskrcnn(images,targets)
        if self.train:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
178/12:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-6}], lr=1e-6)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
# for param in model.tobi.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
179/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
179/2:
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
device = 'cuda'
179/3:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
179/4:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

# for im_name in os.listdir("../dataset/test"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)
179/5:
batchSize = 1
img_siz = [512,512]
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        #k_w = patch_stack.shape[2]
        #k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.float32)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.float32)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
179/6:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
179/7:
def dice_loss(input, target):
    smooth = 1.
    iflat = input.view(-1)
    tflat = target.view(-1)
    intersection = (iflat * tflat).sum()
    
    return 1 - ((2. * intersection + smooth) /
              (iflat.sum() + tflat.sum() + smooth))
179/8:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.vis = False
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        self.BC = torch.nn.BCELoss()
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3c = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm1c = nn.BatchNorm2d(1,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(27,1,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.ReLU(self.norm3c(self.tobi(img_comb)))
            #print(img_ot[0].shape)
            images[ind] = img_ot[0]
            ghost = self.ReLU(self.norm1c(self.obito(img_comb)))
            #ghost = ghost>0.5
            print(ghost[0].shape)
            print(targets[ind]["sem_mask"][0].shape)
            if self.train:
                seg_loss = self.BC(ghost[0][0],targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            if self.vis == True:
                cv2.imwrite("ghost_img.png",(ghost[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("ot_img.png",(img_ot[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))

        ret = self.maskrcnn(images,targets)
        if self.train:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
179/9:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-6}], lr=1e-6)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
# for param in model.tobi.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
179/10:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    loss_dict.pop("color_loss")
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())
    # losses = loss_dict["color_loss"]

    losses.backward()
    optimizer.step()
    model.vis = False
    print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    if i%200==0:
        scheduler.step()
    if i%200==0 and i!=0:
        #evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
180/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
180/2:
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
device = 'cuda'
180/3:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
180/4:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

# for im_name in os.listdir("../dataset/test"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)
180/5:
batchSize = 1
img_siz = [512,512]
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        #k_w = patch_stack.shape[2]
        #k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.float32)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.float32)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
180/6:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
180/7:
def dice_loss(input, target):
    smooth = 1.
    iflat = input.view(-1)
    tflat = target.view(-1)
    intersection = (iflat * tflat).sum()
    
    return 1 - ((2. * intersection + smooth) /
              (iflat.sum() + tflat.sum() + smooth))
180/8:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.vis = False
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        self.BC = torch.nn.BCELoss()
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3c = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm1c = nn.BatchNorm2d(1,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(27,1,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.ReLU(self.norm3c(self.tobi(img_comb)))
            #print(img_ot[0].shape)
            images[ind] = img_ot[0]
            ghost = self.ReLU(self.norm1c(self.obito(img_comb)))[0]
            #ghost = ghost>0.5
            print(ghost[0].shape)
            print(targets[ind]["sem_mask"][0].shape)
            if self.train:
                seg_loss = self.BC(ghost[0][0],targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            if self.vis == True:
                cv2.imwrite("ghost_img.png",(ghost.clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("ot_img.png",(img_ot[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))

        ret = self.maskrcnn(images,targets)
        if self.train:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
180/9:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-6}], lr=1e-6)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
# for param in model.tobi.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
180/10:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    loss_dict.pop("color_loss")
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())
    # losses = loss_dict["color_loss"]

    losses.backward()
    optimizer.step()
    model.vis = False
    print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    if i%200==0:
        scheduler.step()
    if i%200==0 and i!=0:
        #evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
180/11:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.vis = False
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        self.BC = torch.nn.BCELoss()
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3c = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm1c = nn.BatchNorm2d(1,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(27,1,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.ReLU(self.norm3c(self.tobi(img_comb)))
            #print(img_ot[0].shape)
            images[ind] = img_ot[0]
            ghost = self.ReLU(self.norm1c(self.obito(img_comb)))[0]
            #ghost = ghost>0.5
            print(ghost[0].shape)
            print(targets[ind]["sem_mask"][0].shape)
            if self.train:
                seg_loss = self.BC(ghost[0],targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            if self.vis == True:
                cv2.imwrite("ghost_img.png",(ghost.clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("ot_img.png",(img_ot[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))

        ret = self.maskrcnn(images,targets)
        if self.train:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
180/12:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-6}], lr=1e-6)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
# for param in model.tobi.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
180/13:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    loss_dict.pop("color_loss")
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())
    # losses = loss_dict["color_loss"]

    losses.backward()
    optimizer.step()
    model.vis = False
    print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    if i%200==0:
        scheduler.step()
    if i%200==0 and i!=0:
        #evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
180/14:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.vis = False
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        self.BC = torch.nn.BCELoss()
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3c = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm1c = nn.BatchNorm2d(1,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(27,1,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.ReLU(self.norm3c(self.tobi(img_comb)))
            #print(img_ot[0].shape)
            images[ind] = img_ot[0]
            ghost = self.ReLU(self.norm1c(self.obito(img_comb)))[0]
            #ghost = ghost>0.5
            print(ghost[0].shape)
            print(targets[ind]["sem_mask"][0].shape)
            if self.train:
                seg_loss = self.BC(ghost[0],targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            if self.vis == True:
                # cv2.imwrite("ghost_img.png",(ghost.clone().detach().cpu().numpy()*255).transpose(1,2,0))
                # cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                # cv2.imwrite("ot_img.png",(img_ot[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))

        ret = self.maskrcnn(images,targets)
        if self.train:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
180/15:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.vis = False
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        self.BC = torch.nn.BCELoss()
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3c = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm1c = nn.BatchNorm2d(1,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(27,1,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.ReLU(self.norm3c(self.tobi(img_comb)))
            #print(img_ot[0].shape)
            images[ind] = img_ot[0]
            ghost = self.ReLU(self.norm1c(self.obito(img_comb)))[0]
            #ghost = ghost>0.5
            print(ghost[0].shape)
            print(targets[ind]["sem_mask"][0].shape)
            if self.train:
                seg_loss = self.BC(ghost[0],targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            if self.vis == True:
                # cv2.imwrite("ghost_img.png",(ghost.clone().detach().cpu().numpy()*255).transpose(1,2,0))
                # cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                # cv2.imwrite("ot_img.png",(img_ot[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))

        ret = self.maskrcnn(images,targets)
        if self.train:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
180/16:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.vis = False
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        self.BC = torch.nn.BCELoss()
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3c = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm1c = nn.BatchNorm2d(1,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(27,1,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.ReLU(self.norm3c(self.tobi(img_comb)))
            #print(img_ot[0].shape)
            images[ind] = img_ot[0]
            ghost = self.ReLU(self.norm1c(self.obito(img_comb)))[0]
            #ghost = ghost>0.5
            print(ghost[0].shape)
            print(targets[ind]["sem_mask"][0].shape)
            if self.train:
                seg_loss = self.BC(ghost[0],targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            if self.vis == True:
                pass
                # cv2.imwrite("ghost_img.png",(ghost.clone().detach().cpu().numpy()*255).transpose(1,2,0))
                # cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                # cv2.imwrite("ot_img.png",(img_ot[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))

        ret = self.maskrcnn(images,targets)
        if self.train:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
180/17:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-6}], lr=1e-6)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
# for param in model.tobi.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
181/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
181/2:
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
device = 'cuda'
181/3:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
181/4:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

# for im_name in os.listdir("../dataset/test"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)
181/5:
batchSize = 1
img_siz = [512,512]
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        #k_w = patch_stack.shape[2]
        #k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.float32)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.float32)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
181/6:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
181/7:
def dice_loss(input, target):
    smooth = 1.
    iflat = input.view(-1)
    tflat = target.view(-1)
    intersection = (iflat * tflat).sum()
    
    return 1 - ((2. * intersection + smooth) /
              (iflat.sum() + tflat.sum() + smooth))
181/8:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.vis = False
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        self.BC = torch.nn.BCELoss()
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3c = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm1c = nn.BatchNorm2d(1,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(27,1,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.ReLU(self.norm3c(self.tobi(img_comb)))
            #print(img_ot[0].shape)
            images[ind] = img_ot[0]
            ghost = self.ReLU(self.norm1c(self.obito(img_comb)))[0]
            #ghost = ghost>0.5
            print(ghost[0].shape)
            print(targets[ind]["sem_mask"][0].shape)
            if self.train:
                seg_loss = self.BC(ghost[0],targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            if self.vis == True:
                pass
                # cv2.imwrite("ghost_img.png",(ghost.clone().detach().cpu().numpy()*255).transpose(1,2,0))
                # cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                # cv2.imwrite("ot_img.png",(img_ot[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))

        ret = self.maskrcnn(images,targets)
        if self.train:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
181/9:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-6}], lr=1e-6)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
# for param in model.tobi.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
181/10:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    loss_dict.pop("color_loss")
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())
    # losses = loss_dict["color_loss"]

    losses.backward()
    optimizer.step()
    model.vis = False
    print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    if i%200==0:
        scheduler.step()
    if i%200==0 and i!=0:
        #evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
181/11:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.vis = False
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        self.BC = torch.nn.BCELoss()
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3c = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm1c = nn.BatchNorm2d(1,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(27,1,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.ReLU(self.norm3c(self.tobi(img_comb)))
            print(img_ot[0].shape)
            images[ind] = img_ot[0]
            ghost = self.ReLU(self.norm1c(self.obito(img_comb)))[0]
            #ghost = ghost>0.5
            print(ghost[0].shape)
            print(targets[ind]["sem_mask"][0].shape)
            if self.train:
                seg_loss = self.BC(ghost[0],targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            if self.vis == True:
                pass
                # cv2.imwrite("ghost_img.png",(ghost.clone().detach().cpu().numpy()*255).transpose(1,2,0))
                # cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                # cv2.imwrite("ot_img.png",(img_ot[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))

        ret = self.maskrcnn(images,targets)
        if self.train:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
181/12:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-6}], lr=1e-6)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
# for param in model.tobi.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
181/13:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.vis = False
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        self.BC = torch.nn.BCELoss()
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3c = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm1c = nn.BatchNorm2d(1,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(27,1,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.ReLU(self.norm3c(self.tobi(img_comb)))
            print(img_ot[0].shape)
            images[ind] = img_ot[0]
            ghost = self.ReLU(self.norm1c(self.obito(img_comb)))[0]
            #ghost = ghost>0.5
            print(ghost[0].shape)
            print(targets[ind]["sem_mask"][0].shape)
            if self.train:
                seg_loss = self.BC(ghost[0],targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            if self.vis == True:
                pass
                # cv2.imwrite("ghost_img.png",(ghost.clone().detach().cpu().numpy()*255).transpose(1,2,0))
                # cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                # cv2.imwrite("ot_img.png",(img_ot[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))

        ret = self.maskrcnn(images,targets)
        if self.train:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
181/14:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-6}], lr=1e-6)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
# for param in model.tobi.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
182/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
182/2:
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
device = 'cuda'
182/3:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
182/4:
instance_toimage = []

# for im_name in os.listdir("../dataset/images"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)

for im_name in os.listdir("../dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
182/5:
batchSize = 1
img_siz = [512,512]
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        #k_w = patch_stack.shape[2]
        #k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.float32)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.float32)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
182/6:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
182/7:
def dice_loss(input, target):
    smooth = 1.
    iflat = input.view(-1)
    tflat = target.view(-1)
    intersection = (iflat * tflat).sum()
    
    return 1 - ((2. * intersection + smooth) /
              (iflat.sum() + tflat.sum() + smooth))
182/8:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.vis = False
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        self.BC = torch.nn.BCELoss()
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3c = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm1c = nn.BatchNorm2d(1,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(27,1,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.ReLU(self.norm3c(self.tobi(img_comb)))
            print(img_ot[0].shape)
            images[ind] = img_ot[0]
            ghost = self.ReLU(self.norm1c(self.obito(img_comb)))[0]
            #ghost = ghost>0.5
            print(ghost[0].shape)
            print(targets[ind]["sem_mask"][0].shape)
            if self.train:
                seg_loss = self.BC(ghost[0],targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            if self.vis == True:
                pass
                # cv2.imwrite("ghost_img.png",(ghost.clone().detach().cpu().numpy()*255).transpose(1,2,0))
                # cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                # cv2.imwrite("ot_img.png",(img_ot[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))

        ret = self.maskrcnn(images,targets)
        if self.train:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
182/9:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-6}], lr=1e-6)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
# for param in model.tobi.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
182/10:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    loss_dict.pop("color_loss")
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())
    # losses = loss_dict["color_loss"]

    losses.backward()
    optimizer.step()
    model.vis = False
    print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    if i%200==0:
        scheduler.step()
    if i%200==0 and i!=0:
        #evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
183/1:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.vis = False
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        self.BC = torch.nn.BCELoss()
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3c = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm1c = nn.BatchNorm2d(1,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(27,1,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.ReLU(self.norm3c(self.tobi(img_comb)))
            print(img_ot[0].shape)
            images[ind] = img_ot[0]
            ghost = self.ReLU(self.norm1c(self.obito(img_comb)))[0]
            #ghost = ghost>0.5
            print(ghost[0].shape)
            print(targets[ind]["sem_mask"][0].shape)
            if self.train:
                seg_loss = dice_loss(ghost[0],targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            if self.vis == True:
                pass
                # cv2.imwrite("ghost_img.png",(ghost.clone().detach().cpu().numpy()*255).transpose(1,2,0))
                # cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                # cv2.imwrite("ot_img.png",(img_ot[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))

        ret = self.maskrcnn(images,targets)
        if self.train:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
183/2:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
183/3:
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
device = 'cuda'
183/4:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
183/5:
instance_toimage = []

# for im_name in os.listdir("../dataset/images"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)

for im_name in os.listdir("../dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
183/6:
batchSize = 1
img_siz = [512,512]
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        #k_w = patch_stack.shape[2]
        #k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.float32)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.float32)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
183/7:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
183/8:
def dice_loss(input, target):
    smooth = 1.
    iflat = input.view(-1)
    tflat = target.view(-1)
    intersection = (iflat * tflat).sum()
    
    return 1 - ((2. * intersection + smooth) /
              (iflat.sum() + tflat.sum() + smooth))
183/9:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.vis = False
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        self.BC = torch.nn.BCELoss()
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3c = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm1c = nn.BatchNorm2d(1,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(27,1,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.ReLU(self.norm3c(self.tobi(img_comb)))
            print(img_ot[0].shape)
            images[ind] = img_ot[0]
            ghost = self.ReLU(self.norm1c(self.obito(img_comb)))[0]
            #ghost = ghost>0.5
            print(ghost[0].shape)
            print(targets[ind]["sem_mask"][0].shape)
            if self.train:
                seg_loss = dice_loss(ghost[0],targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            if self.vis == True:
                pass
                # cv2.imwrite("ghost_img.png",(ghost.clone().detach().cpu().numpy()*255).transpose(1,2,0))
                # cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                # cv2.imwrite("ot_img.png",(img_ot[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))

        ret = self.maskrcnn(images,targets)
        if self.train:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
183/10:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-6}], lr=1e-6)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
# for param in model.tobi.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
183/11:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    loss_dict.pop("color_loss")
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())
    # losses = loss_dict["color_loss"]

    losses.backward()
    optimizer.step()
    model.vis = False
    print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    if i%200==0:
        scheduler.step()
    if i%200==0 and i!=0:
        #evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
183/12:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
183/13:
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
device = 'cuda'
183/14:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
183/15:
instance_toimage = []

# for im_name in os.listdir("../dataset/images"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)

for im_name in os.listdir("../dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
183/16:
batchSize = 1
img_siz = [512,512]
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        #k_w = patch_stack.shape[2]
        #k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.float32)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.float32)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
183/17:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
183/18:
def dice_loss(input, target):
    smooth = 1.
    iflat = input.view(-1)
    tflat = target.view(-1)
    intersection = (iflat * tflat).sum()
    
    return 1 - ((2. * intersection + smooth) /
              (iflat.sum() + tflat.sum() + smooth))
183/19:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.vis = False
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        self.BC = torch.nn.BCELoss()
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3c = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm1c = nn.BatchNorm2d(1,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(27,1,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.ReLU(self.norm3c(self.tobi(img_comb)))
            #print(img_ot[0].shape)
            images[ind] = img_ot[0]
            ghost = self.ReLU(self.norm1c(self.obito(img_comb)))[0]
            #ghost = ghost>0.5
            #print(ghost[0].shape)
            #print(targets[ind]["sem_mask"][0].shape)
            if self.train:
                seg_loss = dice_loss(ghost[0],targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            if self.vis == True:
                pass
                # cv2.imwrite("ghost_img.png",(ghost.clone().detach().cpu().numpy()*255).transpose(1,2,0))
                # cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                # cv2.imwrite("ot_img.png",(img_ot[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))

        ret = self.maskrcnn(images,targets)
        if self.train:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
183/20:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-6}], lr=1e-6)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
# for param in model.tobi.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
183/21:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    loss_dict.pop("color_loss")
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())
    # losses = loss_dict["color_loss"]

    losses.backward()
    optimizer.step()
    model.vis = False
    print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    if i%200==0:
        scheduler.step()
    if i%200==0 and i!=0:
        #evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
184/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
184/2:
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
device = 'cuda'
184/3:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
184/4:
instance_toimage = []

# for im_name in os.listdir("../dataset/images"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)

for im_name in os.listdir("../dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
184/5:
batchSize = 1
img_siz = [512,512]
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        #k_w = patch_stack.shape[2]
        #k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.float32)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.float32)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
184/6:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.float32)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.float32)
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
184/7:
def dice_loss(input, target):
    smooth = 1.
    iflat = input.view(-1)
    tflat = target.view(-1)
    intersection = (iflat * tflat).sum()
    
    return 1 - ((2. * intersection + smooth) /
              (iflat.sum() + tflat.sum() + smooth))
184/8:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.vis = False
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        self.BC = torch.nn.BCELoss()
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3c = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm1c = nn.BatchNorm2d(1,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(27,1,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.ReLU(self.norm3c(self.tobi(img_comb)))
            #print(img_ot[0].shape)
            images[ind] = img_ot[0]
            ghost = self.ReLU(self.norm1c(self.obito(img_comb)))[0]
            #ghost = ghost>0.5
            #print(ghost[0].shape)
            #print(targets[ind]["sem_mask"][0].shape)
            if self.train:
                seg_loss = dice_loss(ghost[0],targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            if self.vis == True:
                cv2.imwrite("ghost_img.png",(ghost.clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("ot_img.png",(img_ot[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))

        ret = self.maskrcnn(images,targets)
        if self.train:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
184/9:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-6}], lr=1e-6)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.9
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
# for param in model.tobi.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
184/10:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    loss_dict.pop("color_loss")
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())
    # losses = loss_dict["color_loss"]

    losses.backward()
    optimizer.step()
    model.vis = False
    print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    if i%30==0:
        scheduler.step()
    if i%20==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
185/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
185/2:
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
device = 'cuda'
185/3:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
185/4:
instance_toimage = []

# for im_name in os.listdir("../dataset/images"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)

for im_name in os.listdir("../dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
185/5:
batchSize = 1
img_siz = [512,512]
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        #k_w = patch_stack.shape[2]
        #k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.float32)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.float32)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
185/6:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.float32)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.float32)
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
185/7:
def dice_loss(input, target):
    smooth = 1.
    iflat = input.view(-1)
    tflat = target.view(-1)
    intersection = (iflat * tflat).sum()
    
    return 1 - ((2. * intersection + smooth) /
              (iflat.sum() + tflat.sum() + smooth))
185/8:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.vis = False
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        self.BC = torch.nn.BCELoss()
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3c = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm1c = nn.BatchNorm2d(1,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(27,1,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.ReLU(self.norm3c(self.tobi(img_comb)))
            #print(img_ot[0].shape)
            images[ind] = img_ot[0]
            ghost = self.ReLU(self.norm1c(self.obito(img_comb)))[0]
            #ghost = ghost>0.5
            #print(ghost[0].shape)
            #print(targets[ind]["sem_mask"][0].shape)
            if self.train:
                seg_loss = dice_loss(ghost[0],targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            if self.vis == True:
                cv2.imwrite("ghost_img.png",(ghost.clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("ot_img.png",(img_ot[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))

        ret = self.maskrcnn(images,targets)
        if self.train:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
185/9:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-6}], lr=1e-6)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.9
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
# for param in model.tobi.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
185/10:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    #loss_dict.pop("color_loss")
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())
    # losses = loss_dict["color_loss"]

    losses.backward()
    optimizer.step()
    model.vis = False
    print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    if i%30==0:
        scheduler.step()
    if i%20==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
186/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
186/2:
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
device = 'cuda'
186/3:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
186/4:
instance_toimage = []

# for im_name in os.listdir("../dataset/images"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)

for im_name in os.listdir("../dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
186/5:
batchSize = 1
img_siz = [512,512]
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        #k_w = patch_stack.shape[2]
        #k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.float32)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.float32)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
186/6:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.float32)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.float32)
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
186/7:
def dice_loss(input, target):
    smooth = 1.
    iflat = input.view(-1)
    tflat = target.view(-1)
    intersection = (iflat * tflat).sum()
    
    return 1 - ((2. * intersection + smooth) /
              (iflat.sum() + tflat.sum() + smooth))
186/8:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.vis = False
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        self.BC = torch.nn.BCELoss()
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3c = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm1c = nn.BatchNorm2d(1,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(27,1,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.ReLU(self.norm3c(self.tobi(img_comb)))
            #print(img_ot[0].shape)
            images[ind] = img_ot[0]
            ghost = self.ReLU(self.norm1c(self.obito(img_comb)))[0]
            #ghost = ghost>0.5
            #print(ghost[0].shape)
            #print(targets[ind]["sem_mask"][0].shape)
            if self.train:
                seg_loss = dice_loss(ghost[0],targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            if self.vis == True:
                cv2.imwrite("ghost_img.png",(ghost.clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("ot_img.png",(img_ot[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))

        ret = self.maskrcnn(images,targets)
        if self.train:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
186/9:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-6}], lr=1e-6)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.9
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
# for param in model.tobi.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
186/10:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    #loss_dict.pop("color_loss")
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())
    # losses = loss_dict["color_loss"]

    losses.backward()
    optimizer.step()
    model.vis = False
    print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    if i%30==0:
        scheduler.step()
    if i%20==0 and i!=0:
        model.train = False
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
186/11:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.vis = False
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        self.BC = torch.nn.BCELoss()
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3c = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm1c = nn.BatchNorm2d(1,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(27,1,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.ReLU(self.norm3c(self.tobi(img_comb)))
            #print(img_ot[0].shape)
            images[ind] = img_ot[0]
            ghost = self.ReLU(self.norm1c(self.obito(img_comb)))[0]
            #ghost = ghost>0.5
            #print(ghost[0].shape)
            #print(targets[ind]["sem_mask"][0].shape)
            if self.train():
                seg_loss = dice_loss(ghost[0],targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            if self.vis == True:
                cv2.imwrite("ghost_img.png",(ghost.clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("ot_img.png",(img_ot[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))

        ret = self.maskrcnn(images,targets)
        if self.train:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
186/12:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-6}], lr=1e-6)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.9
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
# for param in model.tobi.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
186/13:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    #loss_dict.pop("color_loss")
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())
    # losses = loss_dict["color_loss"]

    losses.backward()
    optimizer.step()
    model.vis = False
    print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    if i%30==0:
        scheduler.step()
    if i%20==0 and i!=0:
        model.eval()
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
186/14:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.vis = False
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        self.BC = torch.nn.BCELoss()
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3c = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm1c = nn.BatchNorm2d(1,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(27,1,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.ReLU(self.norm3c(self.tobi(img_comb)))
            #print(img_ot[0].shape)
            images[ind] = img_ot[0]
            ghost = self.ReLU(self.norm1c(self.obito(img_comb)))[0]
            #ghost = ghost>0.5
            #print(ghost[0].shape)
            #print(targets[ind]["sem_mask"][0].shape)
            if self.train():
                seg_loss = dice_loss(ghost[0],targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            if self.vis == True:
                cv2.imwrite("ghost_img.png",(ghost.clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("ot_img.png",(img_ot[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))

        ret = self.maskrcnn(images,targets)
        if self.train:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
186/15:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-6}], lr=1e-6)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.9
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
# for param in model.tobi.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
186/16:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    #loss_dict.pop("color_loss")
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())
    # losses = loss_dict["color_loss"]

    losses.backward()
    optimizer.step()
    model.vis = False
    print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    if i%30==0:
        scheduler.step()
    if i%20==0:
        model.eval()
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
186/17:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.vis = False
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        self.BC = torch.nn.BCELoss()
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3c = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm1c = nn.BatchNorm2d(1,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(27,1,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.ReLU(self.norm3c(self.tobi(img_comb)))
            #print(img_ot[0].shape)
            images[ind] = img_ot[0]
            ghost = self.ReLU(self.norm1c(self.obito(img_comb)))[0]
            #ghost = ghost>0.5
            #print(ghost[0].shape)
            #print(targets[ind]["sem_mask"][0].shape)
            if self.training:
                seg_loss = dice_loss(ghost[0],targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            if self.vis == True:
                cv2.imwrite("ghost_img.png",(ghost.clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("ot_img.png",(img_ot[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))

        ret = self.maskrcnn(images,targets)
        if self.train:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
186/18:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-6}], lr=1e-6)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.9
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
# for param in model.tobi.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
186/19:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    #loss_dict.pop("color_loss")
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())
    # losses = loss_dict["color_loss"]

    losses.backward()
    optimizer.step()
    model.vis = False
    print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    if i%30==0:
        scheduler.step()
    if i%20==0:
        model.eval()
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
186/20:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.vis = False
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        self.BC = torch.nn.BCELoss()
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3c = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm1c = nn.BatchNorm2d(1,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(27,1,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.ReLU(self.norm3c(self.tobi(img_comb)))
            #print(img_ot[0].shape)
            images[ind] = img_ot[0]
            ghost = self.ReLU(self.norm1c(self.obito(img_comb)))[0]
            #ghost = ghost>0.5
            #print(ghost[0].shape)
            #print(targets[ind]["sem_mask"][0].shape)
            if self.training:
                seg_loss = dice_loss(ghost[0],targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            if self.vis == True:
                cv2.imwrite("ghost_img.png",(ghost.clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("ot_img.png",(img_ot[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))

        ret = self.maskrcnn(images,targets)
        if self.training:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
186/21:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-6}], lr=1e-6)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.9
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
# for param in model.tobi.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
186/22:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    #loss_dict.pop("color_loss")
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())
    # losses = loss_dict["color_loss"]

    losses.backward()
    optimizer.step()
    model.vis = False
    print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    if i%30==0:
        scheduler.step()
    if i%20==0:
        model.eval()
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
176/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
176/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
176/3:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

# for im_name in os.listdir("../dataset/test"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)
176/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        #k_w = patch_stack.shape[2]
        #k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
176/5:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
176/6:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        self.vis = False
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,3,(1,1),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3 = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm12 = nn.BatchNorm2d(12,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        # self.tobi = torch.nn.Conv2d(42,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        # self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.norm12(img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img = self.channel_trans[2](img)
            img = self.ReLU(self.norm3(img+imgog))
            images[ind] = img[0]
            if self.vis == True:
                cv2.imwrite("mod_img.png",(img[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            #img3 = self.channel_trans[2](img)
            #img3 = self.ReLU(img3)
            # img3 = img
            # img4 = self.channel_trans[2](img3)
            # img4 = self.ReLU(img4)
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(images,targets)
        return ret
176/7:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load('jsr_v5.1_700.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-4}], lr=1e-4)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.9
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
176/8:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    model.vis = False
    
    if i%40==0 and i!=0:
        scheduler.step()
        print(scheduler.get_lr())
    if i%50==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5.2_"+str(+test+i)+".torch")
176/9:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load('jsr_v5.1_700.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-4}], lr=1e-4)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.9
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
176/10:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    model.vis = False
    
    if i%40==0 and i!=0:
        scheduler.step()
        print(scheduler.get_lr())
    if i%50==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5.2_"+str(+test+i)+".torch")
176/11:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
176/12:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
176/13:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

# for im_name in os.listdir("../dataset/test"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)
176/14:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
176/15:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
176/16:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        self.vis = False
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,3,(1,1),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3 = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm12 = nn.BatchNorm2d(12,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        # self.tobi = torch.nn.Conv2d(42,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        # self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.norm12(img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img = self.channel_trans[2](img)
            img = self.ReLU(self.norm3(img+imgog))
            images[ind] = img[0]
            if self.vis == True:
                cv2.imwrite("mod_img.png",(img[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            #img3 = self.channel_trans[2](img)
            #img3 = self.ReLU(img3)
            # img3 = img
            # img4 = self.channel_trans[2](img3)
            # img4 = self.ReLU(img4)
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(images,targets)
        return ret
176/17:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
model.load_state_dict(torch.load('jsr_v5.2_2000.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-7},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-7}], lr=1e-7)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.9
for param in model.channel_trans.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
176/18:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    model.vis = False
    
    if i%40==0 and i!=0:
        scheduler.step()
        print(scheduler.get_lr())
    if i%50==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5.2_"+str(+test+i)+".torch")
176/19:
test = 2000
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    model.vis = False
    
    if i%40==0 and i!=0:
        scheduler.step()
        print(scheduler.get_lr())
    if i%200==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5.2_"+str(+test+i)+".torch")
176/20:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
176/21:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
176/22:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

# for im_name in os.listdir("../dataset/test"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)
176/23:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
176/24:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
176/25:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        self.vis = False
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(24,3,(1,1),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3 = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm12 = nn.BatchNorm2d(12,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        # self.tobi = torch.nn.Conv2d(42,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        # self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.norm12(img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img = self.channel_trans[2](img)
            img = self.ReLU(self.norm3(img+imgog))
            images[ind] = img[0]
            if self.vis == True:
                cv2.imwrite("mod_img.png",(img[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            #img3 = self.channel_trans[2](img)
            #img3 = self.ReLU(img3)
            # img3 = img
            # img4 = self.channel_trans[2](img3)
            # img4 = self.ReLU(img4)
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(images,targets)
        return ret
176/26:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load('jsr_v5.2_2000.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 2e-4}], lr=1e-4)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.9
for param in model.channel_trans.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
176/27:
test = 2000
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    model.vis = False
    
    if i%50==0 and i!=0:
        scheduler.step()
        print(scheduler.get_lr())
    if i%200==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5.2_"+str(+test+i)+".torch")
176/28:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load('jsr_v5.2_2000.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 2e-4}], lr=1e-4)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.9
for param in model.channel_trans.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
176/29:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    model.vis = False
    
    if i%50==0 and i!=0:
        scheduler.step()
        print(scheduler.get_lr())
    if i%200==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5.2_"+str(+test+i)+".torch")
189/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
189/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
189/3:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

# for im_name in os.listdir("../dataset/test"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)
189/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
189/5:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
189/6:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        self.vis = False
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(1,1),stride=1,padding='same'),
            torch.nn.Conv2d(12,24,(1,1),stride=1,padding='same'),
            torch.nn.Conv2d(24,3,(1,1),stride=1,padding='same'),
            #torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3 = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm12 = nn.BatchNorm2d(12,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        # self.tobi = torch.nn.Conv2d(42,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        # self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.norm12(img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img = self.channel_trans[2](img)
            img = self.ReLU(self.norm3(img+imgog))
            images[ind] = img[0]
            if self.vis == True:
                cv2.imwrite("mod_img.png",(img[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            #img3 = self.channel_trans[2](img)
            #img3 = self.ReLU(img3)
            # img3 = img
            # img4 = self.channel_trans[2](img3)
            # img4 = self.ReLU(img4)
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(images,targets)
        return ret
189/7:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load('jsr_v5.2_2000.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 2e-6}], lr=1e-4)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.9
for param in model.channel_trans.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
189/8:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    model.vis = False
    
    if i%50==0 and i!=0:
        scheduler.step()
        print(scheduler.get_lr())
    if i%200==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5.2_"+str(+test+i)+".torch")
189/9:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load('jsr_v5.2_2000.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 2e-10}], lr=1e-4)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.9
for param in model.channel_trans.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
189/10:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    model.vis = False
    
    if i%50==0 and i!=0:
        scheduler.step()
        print(scheduler.get_lr())
    if i%200==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5.2_"+str(+test+i)+".torch")
189/11:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load('jsr_v5.2_2000.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 2e-8}], lr=1e-4)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.9
for param in model.channel_trans.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
189/12:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    model.vis = False
    
    if i%50==0 and i!=0:
        scheduler.step()
        print(scheduler.get_lr())
    if i%200==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5.2_"+str(+test+i)+".torch")
189/13:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
189/14:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
189/15:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

# for im_name in os.listdir("../dataset/test"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)
189/16:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        #k_w = patch_stack.shape[2]
        #k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
189/17:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
189/18:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        self.vis = False
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(1,1),stride=1,padding='same'),
            torch.nn.Conv2d(12,24,(1,1),stride=1,padding='same'),
            torch.nn.Conv2d(24,3,(1,1),stride=1,padding='same'),
            #torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3 = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm12 = nn.BatchNorm2d(12,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        # self.tobi = torch.nn.Conv2d(42,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        # self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.norm12(img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img = self.channel_trans[2](img)
            img = self.ReLU(self.norm3(img))
            images[ind] = img[0]
            if self.vis == True:
                cv2.imwrite("mod_img.png",(img[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            #img3 = self.channel_trans[2](img)
            #img3 = self.ReLU(img3)
            # img3 = img
            # img4 = self.channel_trans[2](img3)
            # img4 = self.ReLU(img4)
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(images,targets)
        return ret
189/19:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load('jsr_v5.2_2000.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 2e-7}], lr=1e-4)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.9
# for param in model.channel_trans.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
189/20:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    model.vis = False
    
    if i%50==0 and i!=0:
        scheduler.step()
        print(scheduler.get_lr())
    if i%200==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5.2_"+str(+test+i)+".torch")
189/21:
#model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load('jsr_v5.2_2000.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 2e-7}], lr=1e-4)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.channel_trans.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
189/22:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    model.vis = False
    
    if i%30==0 and i!=0:
        scheduler.step()
        print(scheduler.get_lr())
    if i%200==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5.2_"+str(+test+i)+".torch")
189/23:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
model.load_state_dict(torch.load('jsr_v5.2_1600.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 2e-7}], lr=1e-4)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.maskrcnn.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
189/24:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    model.vis = False
    
    if i%30==0 and i!=0:
        scheduler.step()
        print(scheduler.get_lr())
    if i%200==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5.2_"+str(+test+i)+".torch")
189/25:
test = 2000
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    model.vis = False
    
    if i%30==0 and i!=0:
        scheduler.step()
        print(scheduler.get_lr())
    if i%200==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5.2_"+str(+test+i)+".torch")
189/26:
#model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load('jsr_v5.2_1600.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 2e-4}], lr=1e-4)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.maskrcnn.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
189/27:
test = 4000
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    model.vis = False
    
    if i%10==0 and i!=0:
        scheduler.step()
        print(scheduler.get_lr())
    if i%200==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5.2_"+str(+test+i)+".torch")
189/28:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
189/29:
#model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load('jsr_v5.2_1600.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-7},{ 'params' : model.channel_trans.parameters(), 'lr' : 2e-7}], lr=1e-7)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
189/30:
test = 5000
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    model.vis = False
    
    if i%10==0 and i!=0:
        scheduler.step()
        print(scheduler.get_lr())
    if i%200==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5.2_"+str(+test+i)+".torch")
189/31:
#model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load('jsr_v5.2_1600.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-8},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-9}], lr=1e-9)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.channel_trans.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
189/32:
test = 5000
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    model.vis = False
    
    if i%10==0 and i!=0:
        scheduler.step()
        print(scheduler.get_lr())
    if i%200==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5.2_"+str(+test+i)+".torch")
189/33:
#model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load('jsr_v5.2_1600.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-10},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-9}], lr=1e-9)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.channel_trans.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
189/34:
test = 5000
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    model.vis = False
    
    if i%10==0 and i!=0:
        scheduler.step()
        print(scheduler.get_lr())
    if i%200==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5.2_"+str(+test+i)+".torch")
189/35:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
model.load_state_dict(torch.load('jsr_v5.2_4400.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-10},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-9}], lr=1e-9)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.channel_trans.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
189/36:
test = 5000
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    model.vis = False
    
    if i%10==0 and i!=0:
        scheduler.step()
        print(scheduler.get_lr())
        evaluate(model, test_dataloader, device=device)
    if i%200==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5.2_"+str(+test+i)+".torch")
189/37:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
model.load_state_dict(torch.load('jsr_v5.2_4400.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-15},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-9}], lr=1e-9)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.channel_trans.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
189/38:
test = 5000
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    model.vis = False
    
    if i%10==0 and i!=0:
        scheduler.step()
        print(scheduler.get_lr())
        evaluate(model, test_dataloader, device=device)
        model.train()
    if i%200==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5.2_"+str(+test+i)+".torch")
189/39:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
model.load_state_dict(torch.load('jsr_v5.2_4400.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-15},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-9}], lr=1e-9)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.channel_trans.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
189/40:
test = 5000
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    model.vis = False
    
    if i%10==0:
        scheduler.step()
        print(scheduler.get_lr())
        evaluate(model, test_dataloader, device=device)
        model.train()
    if i%200==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5.2_"+str(+test+i)+".torch")
189/41:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
model.load_state_dict(torch.load('jsr_v5.2_3200.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-15},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-9}], lr=1e-9)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.channel_trans.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
189/42:
test = 5000
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    model.vis = False
    
    if i%50==0:
        scheduler.step()
        print(scheduler.get_lr())
        evaluate(model, test_dataloader, device=device)
        model.train()
    if i%200==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5.2_"+str(+test+i)+".torch")
189/43:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
model.load_state_dict(torch.load('jsr_v5.2_16000.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-15},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-9}], lr=1e-9)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.channel_trans.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
189/44:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
model.load_state_dict(torch.load('jsr_v5.2_1600.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-15},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-9}], lr=1e-9)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.channel_trans.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
189/45:
test = 5000
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    model.vis = False
    
    if i%50==0:
        scheduler.step()
        print(scheduler.get_lr())
        evaluate(model, test_dataloader, device=device)
        model.train()
    if i%200==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5.2_"+str(+test+i)+".torch")
189/46:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
model.load_state_dict(torch.load('jsr_v5.2_2800.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-15},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-9}], lr=1e-9)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.channel_trans.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
189/47:
test = 5000
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    model.vis = False
    
    if i%50==0:
        scheduler.step()
        print(scheduler.get_lr())
        evaluate(model, test_dataloader, device=device)
        model.train()
    if i%200==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5.2_"+str(+test+i)+".torch")
189/48:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
model.load_state_dict(torch.load('jsr_v5.2_3200.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-15},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-9}], lr=1e-9)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.channel_trans.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
189/49:
test = 5000
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    model.vis = False
    
    if i%50==0:
        scheduler.step()
        print(scheduler.get_lr())
        evaluate(model, test_dataloader, device=device)
        model.train()
    if i%200==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5.2_"+str(+test+i)+".torch")
188/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
188/2:
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
device = 'cuda'
188/3:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
188/4:
instance_toimage = []

# for im_name in os.listdir("../dataset/images"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)

for im_name in os.listdir("../dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
188/5:
batchSize = 1
img_siz = [512,512]
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        #k_w = patch_stack.shape[2]
        #k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.float32)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.float32)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
188/6:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.float32)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.float32)
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
188/7:
def dice_loss(input, target):
    smooth = 1.
    iflat = input.view(-1)
    tflat = target.view(-1)
    intersection = (iflat * tflat).sum()
    
    return 1 - ((2. * intersection + smooth) /
              (iflat.sum() + tflat.sum() + smooth))
188/8:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.vis = False
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(1,1),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(8,8),stride=1,padding='same',padding_mode='replicate'),       
        ])
        #self.BC = torch.nn.BCELoss()
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3c = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm1c = nn.BatchNorm2d(1,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(27,1,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.ReLU(self.norm3c(self.tobi(img_comb)))
            #print(img_ot[0].shape)
            images[ind] = img_ot[0]
            ghost = self.ReLU(self.norm1c(self.obito(img_comb)))[0]
            #ghost = ghost>0.5
            #print(ghost[0].shape)
            #print(targets[ind]["sem_mask"][0].shape)
            if self.training:
                seg_loss = dice_loss(ghost[0],targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            if self.vis == True:
                cv2.imwrite("ghost_img.png",(ghost.clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("ot_img.png",(img_ot[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))

        ret = self.maskrcnn(images,targets)
        if self.training:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
188/9:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-30},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-2}], lr=1e-2)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.maskrcnn.parameters():
    param.requires_grad = False
# for param in model.tobi.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
188/10:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    #loss_dict.pop("color_loss")
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())
    # losses = loss_dict["color_loss"]

    losses.backward()
    optimizer.step()
    model.vis = False
    print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    if i%10==0:
        scheduler.step()
    if i%100==0:
        model.eval()
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_"+str(+test+i)+".torch")
188/11:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
188/12:
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
device = 'cuda'
188/13:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
188/14:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("../dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
188/15:
batchSize = 1
img_siz = [512,512]
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        #k_w = patch_stack.shape[2]
        #k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.float32)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.float32)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
188/16:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.float32)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.float32)
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
188/17:
def dice_loss(input, target):
    smooth = 1.
    iflat = input.view(-1)
    tflat = target.view(-1)
    intersection = (iflat * tflat).sum()
    
    return 1 - ((2. * intersection + smooth) /
              (iflat.sum() + tflat.sum() + smooth))
188/18:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.vis = False
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(1,1),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(8,8),stride=1,padding='same',padding_mode='replicate'),       
        ])
        #self.BC = torch.nn.BCELoss()
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3c = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm1c = nn.BatchNorm2d(1,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(27,1,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.ReLU(self.norm3c(self.tobi(img_comb)))
            #print(img_ot[0].shape)
            images[ind] = img_ot[0]
            ghost = self.ReLU(self.norm1c(self.obito(img_comb)))[0]
            #ghost = ghost>0.5
            #print(ghost[0].shape)
            #print(targets[ind]["sem_mask"][0].shape)
            if self.training:
                seg_loss = dice_loss(ghost[0],targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            if self.vis == True:
                cv2.imwrite("ghost_img.png",(ghost.clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("ot_img.png",(img_ot[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))

        ret = self.maskrcnn(images,targets)
        if self.training:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
188/19:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-30},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-2}], lr=1e-2)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.maskrcnn.parameters():
    param.requires_grad = False
# for param in model.tobi.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
188/20:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    #loss_dict.pop("color_loss")
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())
    # losses = loss_dict["color_loss"]

    losses.backward()
    optimizer.step()
    model.vis = False
    print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    if i%10==0:
        scheduler.step()
    if i%100==0:
        model.eval()
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_"+str(+test+i)+".torch")
191/1:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.vis = False
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(1,1),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(1,1),stride=1,padding='same',padding_mode='replicate'),       
        ])
        #self.BC = torch.nn.BCELoss()
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3c = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm1c = nn.BatchNorm2d(1,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(27,1,(12,12),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.ReLU(self.norm3c(self.tobi(img_comb)))
            #print(img_ot[0].shape)
            images[ind] = img_ot[0]
            ghost = self.ReLU(self.norm1c(self.obito(img_comb)))[0]
            #ghost = ghost>0.5
            #print(ghost[0].shape)
            #print(targets[ind]["sem_mask"][0].shape)
            if self.training:
                seg_loss = dice_loss(ghost[0],targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            if self.vis == True:
                cv2.imwrite("ghost_img.png",(ghost.clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("ot_img.png",(img_ot[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))

        ret = self.maskrcnn(images,targets)
        if self.training:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
191/2:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
191/3:
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
device = 'cuda'
191/4:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
191/5:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("../dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
191/6:
batchSize = 1
img_siz = [512,512]
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        #k_w = patch_stack.shape[2]
        #k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.float32)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.float32)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
191/7:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.float32)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.float32)
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
191/8:
def dice_loss(input, target):
    smooth = 1.
    iflat = input.view(-1)
    tflat = target.view(-1)
    intersection = (iflat * tflat).sum()
    
    return 1 - ((2. * intersection + smooth) /
              (iflat.sum() + tflat.sum() + smooth))
191/9:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.vis = False
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(1,1),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(1,1),stride=1,padding='same',padding_mode='replicate'),       
        ])
        #self.BC = torch.nn.BCELoss()
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3c = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm1c = nn.BatchNorm2d(1,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(27,1,(12,12),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.ReLU(self.norm3c(self.tobi(img_comb)))
            #print(img_ot[0].shape)
            images[ind] = img_ot[0]
            ghost = self.ReLU(self.norm1c(self.obito(img_comb)))[0]
            #ghost = ghost>0.5
            #print(ghost[0].shape)
            #print(targets[ind]["sem_mask"][0].shape)
            if self.training:
                seg_loss = dice_loss(ghost[0],targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            if self.vis == True:
                cv2.imwrite("ghost_img.png",(ghost.clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("ot_img.png",(img_ot[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))

        ret = self.maskrcnn(images,targets)
        if self.training:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
191/10:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-30},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-2}], lr=1e-2)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.maskrcnn.parameters():
    param.requires_grad = False
# for param in model.tobi.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
191/11:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    #loss_dict.pop("color_loss")
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())
    # losses = loss_dict["color_loss"]

    losses.backward()
    optimizer.step()
    model.vis = False
    print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    if i%10==0:
        scheduler.step()
    if i%100==0:
        model.eval()
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_"+str(+test+i)+".torch")
191/12:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.vis = False
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(1,1),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(1,1),stride=1,padding='same',padding_mode='replicate'),       
        ])
        #self.BC = torch.nn.BCELoss()
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3c = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm1c = nn.BatchNorm2d(1,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(27,1,(12,12),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.ReLU(self.norm3c(self.tobi(img_comb)))
            #print(img_ot[0].shape)
            images[ind] = img_ot[0]
            ghost = self.ReLU(self.norm1c(self.obito(img_comb)))[0]
            #ghost = ghost>0.5
            #print(ghost[0].shape)
            #print(targets[ind]["sem_mask"][0].shape)
            if self.training:
                seg_loss = dice_loss(ghost[0],targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            if self.vis == True:
                cv2.imwrite("ghost_img.png",(ghost.clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("ot_img.png",(img_ot[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))

        ret = self.maskrcnn(images,targets)
        if self.training:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
191/13:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
model.load_state_dict(torch.load('jsr_1800.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-3},{ 'params' : model.tobi.parameters(), 'lr' : 1e-2}], lr=1e-2)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.channel_trans.parameters():
    param.requires_grad = False
# for param in model.tobi.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
191/14:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    loss_dict.pop("color_loss")
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())
    # losses = loss_dict["color_loss"]

    losses.backward()
    optimizer.step()
    model.vis = False
    print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    if i%50==0:
        scheduler.step()
    if i%100==0:
        model.eval()
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_"+str(+test+i)+".torch")
191/15:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
model.load_state_dict(torch.load('jsr_1800.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-3},{ 'params' : model.tobi.parameters(), 'lr' : 1e-2}], lr=1e-2)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 1
for param in model.channel_trans.parameters():
    param.requires_grad = False
# for param in model.tobi.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
191/16:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    loss_dict.pop("color_loss")
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())
    # losses = loss_dict["color_loss"]

    losses.backward()
    optimizer.step()
    model.vis = False
    print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    if i%50==0:
        scheduler.step()
    if i%100==0:
        model.eval()
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_"+str(+test+i)+".torch")
191/17:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
model.load_state_dict(torch.load('jsr_1800.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.tobi.parameters(), 'lr' : 1e-2}], lr=1e-2)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.channel_trans.parameters():
    param.requires_grad = False
# for param in model.tobi.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
191/18:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    loss_dict.pop("color_loss")
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())
    # losses = loss_dict["color_loss"]

    losses.backward()
    optimizer.step()
    model.vis = False
    print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    if i%100==0:
        scheduler.step()
    if i%100==0:
        model.eval()
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_"+str(+test+i)+".torch")
190/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
190/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
190/3:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

# for im_name in os.listdir("../dataset/test"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)
190/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
190/5:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
190/6:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        self.vis = False
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(1,1),stride=1,padding='same'),
            #torch.nn.Conv2d(12,24,(1,1),stride=1,padding='same'),
            torch.nn.Conv2d(12,12,(1,1),stride=1,padding='same'),
            #torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3 = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm12 = nn.BatchNorm2d(12,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        # self.tobi = torch.nn.Conv2d(42,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        # self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.norm12(img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            img = self.ReLU(img)
            img = self.channel_trans[2](img)
            img = self.ReLU(self.norm3(img))
            images[ind] = img[0]
            if self.vis == True:
                cv2.imwrite("mod_img.png",(img[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            #img3 = self.channel_trans[2](img)
            #img3 = self.ReLU(img3)
            # img3 = img
            # img4 = self.channel_trans[2](img3)
            # img4 = self.ReLU(img4)
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(images,targets)
        return ret
190/7:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
model.load_state_dict(torch.load('jsr_v5.69_3200.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-15},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-9}], lr=1e-9)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.channel_trans.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
190/8:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load('jsr_v5.69_3200.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-4)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.channel_trans.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
190/9:
test = 5000
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    model.vis = False
    
    if i%50==0:
        scheduler.step()
        print(scheduler.get_lr())
        evaluate(model, test_dataloader, device=device)
        model.train()
    if i%200==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5.69_"+str(+test+i)+".torch")
190/10:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load('jsr_v5.69_3200.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-4)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.channel_trans.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
190/11:
test = 5000
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    model.vis = False
    
    if i%100==0:
        scheduler.step()
        print(scheduler.get_lr())
        evaluate(model, test_dataloader, device=device)
        model.train()
    if i%200==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5.69_"+str(+test+i)+".torch")
190/12:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        self.vis = False
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(1,1),stride=1,padding='same'),
            #torch.nn.Conv2d(12,24,(1,1),stride=1,padding='same'),
            torch.nn.Conv2d(12,3,(1,1),stride=1,padding='same'),
            #torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3 = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm12 = nn.BatchNorm2d(12,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        # self.tobi = torch.nn.Conv2d(42,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        # self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,trainable_backbone_layers=5,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.norm12(img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            #img = self.ReLU(img)
            #img = self.channel_trans[2](img)
            img = self.ReLU(self.norm3(img))
            images[ind] = img[0]
            if self.vis == True:
                cv2.imwrite("mod_img.png",(img[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            #img3 = self.channel_trans[2](img)
            #img3 = self.ReLU(img3)
            # img3 = img
            # img4 = self.channel_trans[2](img3)
            # img4 = self.ReLU(img4)
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(images,targets)
        return ret
190/13:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load('jsr_v5.69_3200.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-4)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.channel_trans.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
190/14:
test = 5000
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    model.vis = False
    
    if i%100==0:
        scheduler.step()
        print(scheduler.get_lr())
        evaluate(model, test_dataloader, device=device)
        model.train()
    if i%200==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5.69_"+str(+test+i)+".torch")
191/19:
test = 1800
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    loss_dict.pop("color_loss")
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())
    # losses = loss_dict["color_loss"]

    losses.backward()
    optimizer.step()
    model.vis = False
    print(i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    if i%100==0:
        scheduler.step()
    if i%100==0:
        model.eval()
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_"+str(+test+i)+".torch")
192/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
192/2:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
192/3:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

# for im_name in os.listdir("../dataset/test"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)
192/4:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
192/5:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            t=1
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
192/6:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        self.vis = False
        #self.param = param
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(1,1),stride=1,padding='same'),
            #torch.nn.Conv2d(12,24,(1,1),stride=1,padding='same'),
            torch.nn.Conv2d(12,3,(1,1),stride=1,padding='same'),
            #torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       
        ])
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3 = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm12 = nn.BatchNorm2d(12,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        # self.tobi = torch.nn.Conv2d(42,1,(5,5),stride=1,padding='same',padding_mode='replicate')
        # self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,trainable_backbone_layers=5,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.norm12(img)
            img = self.ReLU(img)
            img = self.channel_trans[1](img)
            #img = self.ReLU(img)
            #img = self.channel_trans[2](img)
            img = self.ReLU(self.norm3(img))
            images[ind] = img[0]
            if self.vis == True:
                cv2.imwrite("mod_img.png",(img[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            #img3 = self.channel_trans[2](img)
            #img3 = self.ReLU(img3)
            # img3 = img
            # img4 = self.channel_trans[2](img3)
            # img4 = self.ReLU(img4)
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))
        ret = self.maskrcnn(images,targets)
        return ret
192/7:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load('jsr_v5.69_3200.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-4)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.channel_trans.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
192/8:
test = 5000
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    model.vis = False
    
    if i%100==0:
        scheduler.step()
        print(scheduler.get_lr())
        evaluate(model, test_dataloader, device=device)
        model.train()
    if i%200==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5.69_"+str(+test+i)+".torch")
192/9:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    model.vis = False
    
    if i%100==0:
        scheduler.step()
        print(scheduler.get_lr())
        evaluate(model, test_dataloader, device=device)
        model.train()
    if i%200==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5.69_"+str(+test+i)+".torch")
192/10:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load('jsr_v5.69_3200.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-3}], lr=1e-4)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.channel_trans.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
192/11:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    model.vis = False
    
    if i%100==0:
        scheduler.step()
        print(scheduler.get_lr())
        evaluate(model, test_dataloader, device=device)
        model.train()
    if i%200==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5.69_"+str(+test+i)+".torch")
192/12:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load('jsr_v5.69_3200.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-11}], lr=1e-4)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.channel_trans.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
192/13:
test = 1800
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    model.vis = False
    
    if i%100==0:
        scheduler.step()
        print(scheduler.get_lr())
        evaluate(model, test_dataloader, device=device)
        model.train()
    if i%200==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5.69_"+str(+test+i)+".torch")
192/14:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
model.load_state_dict(torch.load('jsr_v5.69_1200.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-6},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-11}], lr=1e-4)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.channel_trans.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
192/15:
test = 1800
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    model.vis = False
    
    if i%100==0:
        scheduler.step()
        print(scheduler.get_lr())
        evaluate(model, test_dataloader, device=device)
        model.train()
    if i%200==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5.69_"+str(+test+i)+".torch")
192/16:
batchSize = 2
img_siz = [512,512]
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        #k_w = patch_stack.shape[2]
        #k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
192/17:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
model.load_state_dict(torch.load('jsr_v5.69_1200.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-6},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-11}], lr=1e-4)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.channel_trans.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
192/18:
test = 1800
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())

    losses.backward()
    optimizer.step()
    print(test+i,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    model.vis = False
    
    if i%100==0:
        scheduler.step()
        print(scheduler.get_lr())
        evaluate(model, test_dataloader, device=device)
        model.train()
    if i%200==0 and i!=0:
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_v5.69_"+str(+test+i)+".torch")
193/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
193/2:
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
device = 'cuda'
193/3:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
193/4:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("../dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
193/5:
batchSize = 1
img_siz = [512,512]
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(512,patch_stack.shape[2])
        k_h = random.randint(512,patch_stack.shape[1])
        #k_w = patch_stack.shape[2]
        #k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.float32)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.float32)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
193/6:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.float32)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.float32)
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
193/7:
def dice_loss(input, target):
    smooth = 1.
    iflat = input.view(-1)
    tflat = target.view(-1)
    intersection = (iflat * tflat).sum()
    
    return 1 - ((2. * intersection + smooth) /
              (iflat.sum() + tflat.sum() + smooth))
193/8:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.vis = False
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(1,1),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(1,1),stride=1,padding='same',padding_mode='replicate'),       
        ])
        #self.BC = torch.nn.BCELoss()
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3c = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm1c = nn.BatchNorm2d(1,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(27,1,(12,12),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,trainable_backbone_layers=5,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.ReLU(self.norm3c(self.tobi(img_comb)))
            #print(img_ot[0].shape)
            images[ind] = img_ot[0]
            ghost = self.ReLU(self.norm1c(self.obito(img_comb)))[0]
            #ghost = ghost>0.5
            #print(ghost[0].shape)
            #print(targets[ind]["sem_mask"][0].shape)
            if self.training:
                seg_loss = dice_loss(ghost[0],targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            if self.vis == True:
                cv2.imwrite("ghost_img.png",(ghost.clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("ot_img.png",(img_ot[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))

        ret = self.maskrcnn(images,targets)
        if self.training:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
193/9:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
model.load_state_dict(torch.load('jsr_1800.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.tobi.parameters(), 'lr' : 1e-3}], lr=1e-2)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.channel_trans.parameters():
    param.requires_grad = False
# for param in model.tobi.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
193/10:
test = 1800
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    loss_dict.pop("color_loss")
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())
    # losses = loss_dict["color_loss"]

    losses.backward()
    optimizer.step()
    model.vis = False
    print(i+test,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    if i%100==0:
        scheduler.step()
    if i%100==0:
        model.eval()
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_"+str(+test+i)+".torch")
193/11:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
model.load_state_dict(torch.load('jsr_5000.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-6},{ 'params' : model.tobi.parameters(), 'lr' : 1e-6}], lr=1e-6)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.channel_trans.parameters():
#     param.requires_grad = False
# for param in model.tobi.parameters():
#     param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
193/12:
test = 5000
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    #loss_dict.pop("color_loss")
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())
    # losses = loss_dict["color_loss"]

    losses.backward()
    optimizer.step()
    model.vis = False
    print(i+test,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    if i%100==0:
        scheduler.step()
    if i%100==0:
        model.eval()
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsr_"+str(+test+i)+".torch")
195/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
195/2:
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
device = 'cuda'
195/3:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
195/4:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("../dataset/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
195/5:
batchSize = 1
img_siz = [512,512]
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.8))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(1000,patch_stack.shape[2])
        k_h = random.randint(1000,patch_stack.shape[1])
        #k_w = patch_stack.shape[2]
        #k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.float32)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.float32)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
195/6:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.float32)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.float32)
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
195/7:
def dice_loss(input, target):
    smooth = 1.
    iflat = input.view(-1)
    tflat = target.view(-1)
    intersection = (iflat * tflat).sum()
    
    return 1 - ((2. * intersection + smooth) /
              (iflat.sum() + tflat.sum() + smooth))
195/8:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.vis = False
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(1,1),stride=1,padding='same'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(3,3),stride=1,padding='same'),
            torch.nn.Conv2d(12,12,(1,1),stride=1,padding='same'),       
        ])
        #self.BC = torch.nn.BCELoss()
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3c = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm1c = nn.BatchNorm2d(1,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(27,1,(12,12),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,trainable_backbone_layers=5,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.ReLU(self.norm3c(self.tobi(img_comb)))
            #print(img_ot[0].shape)
            #images[ind] = img_ot[0]
            ghost = self.ReLU(self.norm1c(self.obito(img_comb)))[0]
            #ghost = ghost>0.5
            #print(ghost[0].shape)
            #print(targets[ind]["sem_mask"][0].shape)
            if self.training:
                seg_loss = dice_loss(ghost[0],targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            if self.vis == True:
                cv2.imwrite("ghost_img.png",(ghost.clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("ot_img.png",(img_ot[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))

        ret = self.maskrcnn(images,targets)
        if self.training:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
195/9:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load('jsr_5000.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.tobi.parameters(), 'lr' : 1e-50}], lr=1e-50)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.channel_trans.parameters():
#     param.requires_grad = False
for param in model.tobi.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
195/10:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    #loss_dict.pop("color_loss")
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())
    # losses = loss_dict["color_loss"]

    losses.backward()
    optimizer.step()
    model.vis = False
    print(i+test,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    if i%100==0:
        scheduler.step()
    if i%100==0:
        model.eval()
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsrprabhu_"+str(+test+i)+".torch")
196/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
196/2:
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
device = 'cuda'
196/3:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
196/4:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

# for im_name in os.listdir("../dataset/test"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)
196/5:
batchSize = 1
img_siz = [512,512]
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.8))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(1000,patch_stack.shape[2])
        k_h = random.randint(1000,patch_stack.shape[1])
        #k_w = patch_stack.shape[2]
        #k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.float32)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.float32)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
196/6:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.float32)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.float32)
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
196/7:
def dice_loss(input, target):
    smooth = 1.
    iflat = input.view(-1)
    tflat = target.view(-1)
    intersection = (iflat * tflat).sum()
    
    return 1 - ((2. * intersection + smooth) /
              (iflat.sum() + tflat.sum() + smooth))
196/8:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.vis = False
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(1,1),stride=1,padding='same'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(3,3),stride=1,padding='same'),
            torch.nn.Conv2d(12,12,(1,1),stride=1,padding='same'),       
        ])
        #self.BC = torch.nn.BCELoss()
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3c = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm1c = nn.BatchNorm2d(1,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(27,1,(12,12),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,trainable_backbone_layers=5,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.ReLU(self.norm3c(self.tobi(img_comb)))
            #print(img_ot[0].shape)
            #images[ind] = img_ot[0]
            ghost = self.ReLU(self.norm1c(self.obito(img_comb)))[0]
            #ghost = ghost>0.5
            #print(ghost[0].shape)
            #print(targets[ind]["sem_mask"][0].shape)
            if self.training:
                seg_loss = dice_loss(ghost[0],targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            if self.vis == True:
                cv2.imwrite("ghost_img.png",(ghost.clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("ot_img.png",(img_ot[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))

        ret = self.maskrcnn(images,targets)
        if self.training:
            color_loss = color_loss[0]
            ret["color_loss"] = color_loss
        
        return ret
196/9:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load('jsr_5000.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.tobi.parameters(), 'lr' : 1e-50}], lr=1e-50)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.channel_trans.parameters():
#     param.requires_grad = False
for param in model.tobi.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
196/10:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    #loss_dict.pop("color_loss")
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())
    # losses = loss_dict["color_loss"]

    losses.backward()
    optimizer.step()
    model.vis = False
    print(i+test,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    if i%100==0:
        scheduler.step()
    if i%100==0:
        model.eval()
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsrprabhu_"+str(+test+i)+".torch")
196/11:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.vis = False
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(1,1),stride=1,padding='same'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(3,3),stride=1,padding='same'),
            torch.nn.Conv2d(12,12,(1,1),stride=1,padding='same'),       
        ])
        #self.BC = torch.nn.BCELoss()
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3c = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm1c = nn.BatchNorm2d(1,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(27,1,(12,12),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,trainable_backbone_layers=5,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.ReLU(self.norm3c(self.tobi(img_comb)))
            #print(img_ot[0].shape)
            #images[ind] = img_ot[0]
            ghost = self.ReLU(self.norm1c(self.obito(img_comb)))[0]
            #ghost = ghost>0.5
            #print(ghost[0].shape)
            #print(targets[ind]["sem_mask"][0].shape)
            if self.training:
                seg_loss = dice_loss(ghost[0],targets[ind]["sem_mask"][0])
                color_loss[0] += seg_loss
            if self.vis == True:
                cv2.imwrite("ghost_img.png",(ghost.clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("ot_img.png",(img_ot[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))

        ret = self.maskrcnn(images,targets)
        # if self.training:
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        
        return ret
196/12:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
#model.load_state_dict(torch.load('jsr_5000.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.tobi.parameters(), 'lr' : 1e-50}], lr=1e-50)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.channel_trans.parameters():
    param.requires_grad = False
for param in model.tobi.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
196/13:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    #loss_dict.pop("color_loss")
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())
    # losses = loss_dict["color_loss"]

    losses.backward()
    optimizer.step()
    model.vis = False
    print(i+test,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    if i%100==0:
        scheduler.step()
    if i%100==0:
        model.eval()
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsrprabhu_"+str(+test+i)+".torch")
196/14:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
model.load_state_dict(torch.load('jsr_1100.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.tobi.parameters(), 'lr' : 1e-50}], lr=1e-50)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.channel_trans.parameters():
    param.requires_grad = False
for param in model.tobi.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
196/15:
model = jsrcnn_v5(box_nms_thresh=0.5)
model.to(device)
model.load_state_dict(torch.load('jsrprabhu_1100.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.tobi.parameters(), 'lr' : 1e-50}], lr=1e-50)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.channel_trans.parameters():
    param.requires_grad = False
for param in model.tobi.parameters():
    param.requires_grad = False
scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
196/16:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    #loss_dict.pop("color_loss")
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())
    # losses = loss_dict["color_loss"]

    losses.backward()
    optimizer.step()
    model.vis = False
    print(i+test,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    if i%100==0:
        scheduler.step()
    if i%100==0:
        model.eval()
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsrprabhu_"+str(+test+i)+".torch")
196/17:
test = 800
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    #loss_dict.pop("color_loss")
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())
    # losses = loss_dict["color_loss"]

    losses.backward()
    optimizer.step()
    model.vis = False
    print(i+test,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    if i%100==0:
        scheduler.step()
    if i%100==0:
        model.eval()
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsrprabhu_"+str(+test+i)+".torch")
196/18:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.vis = False
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(1,1),stride=1,padding='same'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(3,3),stride=1,padding='same'),
            torch.nn.Conv2d(12,12,(1,1),stride=1,padding='same'),       
        ])
        #self.BC = torch.nn.BCELoss()
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3c = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm1c = nn.BatchNorm2d(1,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.obito = torch.nn.Conv2d(27,1,(12,12),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,trainable_backbone_layers=5,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.ReLU(self.norm3c(self.tobi(img_comb)))
            #print(img_ot[0].shape)
            images[ind] = img_ot[0]
            #ghost = self.ReLU(self.norm1c(self.obito(img_comb)))[0]
            #ghost = ghost>0.5
            #print(ghost[0].shape)
            #print(targets[ind]["sem_mask"][0].shape)
            # if self.training:
            #     seg_loss = dice_loss(ghost[0],targets[ind]["sem_mask"][0])
            #     color_loss[0] += seg_loss
            if self.vis == True:
                cv2.imwrite("ghost_img.png",(ghost.clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("ot_img.png",(img_ot[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))

        ret = self.maskrcnn(images,targets)
        # if self.training:
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        
        return ret
196/19:
model = jsrcnn_v5()
model.to(device)
model.load_state_dict(torch.load('jsrprabhu_1100.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.tobi.parameters(), 'lr' : 1e-3}], lr=1e-3)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.maskrcnn.parameters():
    param.requires_grad = False

scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
196/20:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.vis = False
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(1,1),stride=1,padding='same'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(3,3),stride=1,padding='same'),
            torch.nn.Conv2d(12,12,(1,1),stride=1,padding='same'),       
        ])
        #self.BC = torch.nn.BCELoss()
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3c = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm1c = nn.BatchNorm2d(1,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(27,1,(12,12),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,trainable_backbone_layers=5,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.ReLU(self.norm3c(self.tobi(img_comb)))
            #print(img_ot[0].shape)
            images[ind] = img_ot[0]
            #ghost = self.ReLU(self.norm1c(self.obito(img_comb)))[0]
            #ghost = ghost>0.5
            #print(ghost[0].shape)
            #print(targets[ind]["sem_mask"][0].shape)
            # if self.training:
            #     seg_loss = dice_loss(ghost[0],targets[ind]["sem_mask"][0])
            #     color_loss[0] += seg_loss
            if self.vis == True:
                cv2.imwrite("ghost_img.png",(ghost.clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("ot_img.png",(img_ot[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))

        ret = self.maskrcnn(images,targets)
        # if self.training:
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        
        return ret
196/21:
model = jsrcnn_v5()
model.to(device)
model.load_state_dict(torch.load('jsrprabhu_1100.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.tobi.parameters(), 'lr' : 1e-3}], lr=1e-3)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.maskrcnn.parameters():
    param.requires_grad = False

scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
196/22:
test = 800
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    #loss_dict.pop("color_loss")
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())
    # losses = loss_dict["color_loss"]

    losses.backward()
    optimizer.step()
    model.vis = False
    print(i+test,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    if i%10==0:
        scheduler.step()
    if i%10==0:
        model.eval()
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsrprabhu_"+str(+test+i)+".torch")
196/23:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.vis = False
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(1,1),stride=1,padding='same'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(3,3),stride=1,padding='same'),
            torch.nn.Conv2d(12,12,(1,1),stride=1,padding='same'),       
        ])
        #self.BC = torch.nn.BCELoss()
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3c = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm1c = nn.BatchNorm2d(1,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(27,1,(12,12),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,trainable_backbone_layers=5,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.ReLU(self.norm3c(self.tobi(img_comb)))
            #print(img_ot[0].shape)
            images[ind] = img_ot[0]
            #ghost = self.ReLU(self.norm1c(self.obito(img_comb)))[0]
            #ghost = ghost>0.5
            #print(ghost[0].shape)
            #print(targets[ind]["sem_mask"][0].shape)
            # if self.training:
            #     seg_loss = dice_loss(ghost[0],targets[ind]["sem_mask"][0])
            #     color_loss[0] += seg_loss
            if self.vis == True:
                #cv2.imwrite("ghost_img.png",(ghost.clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("ot_img.png",(img_ot[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))

        ret = self.maskrcnn(images,targets)
        # if self.training:
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        
        return ret
196/24:
model = jsrcnn_v5()
model.to(device)
model.load_state_dict(torch.load('jsrprabhu_1100.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.tobi.parameters(), 'lr' : 1e-3}], lr=1e-3)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.maskrcnn.parameters():
    param.requires_grad = False

scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
196/25:
test = 800
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    #loss_dict.pop("color_loss")
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())
    # losses = loss_dict["color_loss"]

    losses.backward()
    optimizer.step()
    model.vis = False
    print(i+test,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    if i%10==0:
        scheduler.step()
    if i%10==0:
        model.eval()
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsrprabhu_"+str(+test+i)+".torch")
196/26:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.vis = False
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(1,1),stride=1,padding='same'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(3,3),stride=1,padding='same'),
            torch.nn.Conv2d(12,12,(1,1),stride=1,padding='same'),       
        ])
        #self.BC = torch.nn.BCELoss()
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3c = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm1c = nn.BatchNorm2d(1,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.obito = torch.nn.Conv2d(27,1,(12,12),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,trainable_backbone_layers=5,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.ReLU(self.norm3c(self.tobi(img_comb)))
            #print(img_ot[0].shape)
            images[ind] = img_ot[0]
            #ghost = self.ReLU(self.norm1c(self.obito(img_comb)))[0]
            #ghost = ghost>0.5
            #print(ghost[0].shape)
            #print(targets[ind]["sem_mask"][0].shape)
            # if self.training:
            #     seg_loss = dice_loss(ghost[0],targets[ind]["sem_mask"][0])
            #     color_loss[0] += seg_loss
            if self.vis == True:
                #cv2.imwrite("ghost_img.png",(ghost.clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("ot_img.png",(img_ot[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))

        ret = self.maskrcnn(images,targets)
        # if self.training:
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        
        return ret
196/27:
model = jsrcnn_v5()
model.to(device)
model.load_state_dict(torch.load('jsrprabhu_1100.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-4},{ 'params' : model.tobi.parameters(), 'lr' : 1e-3}], lr=1e-3)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.maskrcnn.parameters():
    param.requires_grad = False

scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
196/28:
test = 1100
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    #loss_dict.pop("color_loss")
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())
    # losses = loss_dict["color_loss"]

    losses.backward()
    optimizer.step()
    model.vis = False
    print(i+test,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    if i%10==0:
        scheduler.step()
    if i%10==0:
        model.eval()
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsrprabhu_"+str(+test+i)+".torch")
196/29:
model = jsrcnn_v5()
model.to(device)
model.load_state_dict(torch.load('jsrprabhu_1100.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-40},{ 'params' : model.tobi.parameters(), 'lr' : 1e-4}], lr=1e-4)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.maskrcnn.parameters():
    param.requires_grad = False

scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
196/30:
test = 1100
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    #loss_dict.pop("color_loss")
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())
    # losses = loss_dict["color_loss"]

    losses.backward()
    optimizer.step()
    model.vis = False
    print(i+test,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    if i%100==0:
        scheduler.step()
    if i%10==0:
        model.eval()
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsrprabhu_"+str(+test+i)+".torch")
196/31:
model = jsrcnn_v5()
model.to(device)
model.load_state_dict(torch.load('jsrprabhu_1100.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-40},{ 'params' : model.tobi.parameters(), 'lr' : 1e-2}], lr=1e-2)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.maskrcnn.parameters():
    param.requires_grad = False

scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
196/32:
test = 1100
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    #loss_dict.pop("color_loss")
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())
    # losses = loss_dict["color_loss"]

    losses.backward()
    optimizer.step()
    model.vis = False
    print(i+test,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    if i%100==0:
        scheduler.step()
    if i%10==0:
        model.eval()
        evaluate(model, test_dataloader, device=device)
        model.train()
        torch.save(model.state_dict(), "jsrprabhu_"+str(+test+i)+".torch")
196/33:
model = jsrcnn_v5()
model.to(device)
model.load_state_dict(torch.load('jsrprabhu_2200.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-40},{ 'params' : model.tobi.parameters(), 'lr' : 1e-5}], lr=1e-5)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.maskrcnn.parameters():
    param.requires_grad = False

scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
196/34:
model = jsrcnn_v5()
model.to(device)
model.load_state_dict(torch.load('jsrprabhu_2150.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-40},{ 'params' : model.tobi.parameters(), 'lr' : 1e-5}], lr=1e-5)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.maskrcnn.parameters():
    param.requires_grad = False

scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
196/35:
test = 2200
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    #loss_dict.pop("color_loss")
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())
    # losses = loss_dict["color_loss"]

    losses.backward()
    optimizer.step()
    model.vis = False
    print(i+test,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    if i%30==0:
        scheduler.step()
    if i%10==0:
        model.eval()
        evaluate(model, test_dataloader, device=device)
        model.train()
        
    if i%50==0 and i!=0:
        torch.save(model.state_dict(), "jsrprabhu_"+str(+test+i)+".torch")
196/36:
model = jsrcnn_v5()
model.to(device)
model.load_state_dict(torch.load('jsrprabhu_2150.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-40},{ 'params' : model.tobi.parameters(), 'lr' : 1e-4}], lr=1e-4)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.maskrcnn.parameters():
    param.requires_grad = False

scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
196/37:
test = 2200
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    #loss_dict.pop("color_loss")
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())
    # losses = loss_dict["color_loss"]

    losses.backward()
    optimizer.step()
    model.vis = False
    print(i+test,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    if i%30==0:
        scheduler.step()
    if i%10==0:
        model.eval()
        evaluate(model, test_dataloader, device=device)
        model.train()
        
    if i%50==0 and i!=0:
        torch.save(model.state_dict(), "jsrprabhu_"+str(+test+i)+".torch")
196/38:
model = jsrcnn_v5()
model.to(device)
model.load_state_dict(torch.load('jsrprabhu_2150.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-40},{ 'params' : model.tobi.parameters(), 'lr' : 1e-3}], lr=1e-3)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
for param in model.maskrcnn.parameters():
    param.requires_grad = False

scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
196/39:
test = 2150
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    #loss_dict.pop("color_loss")
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())
    # losses = loss_dict["color_loss"]

    losses.backward()
    optimizer.step()
    model.vis = False
    print(i+test,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    if i%30==0:
        scheduler.step()
    if i%10==0:
        model.eval()
        evaluate(model, test_dataloader, device=device)
        model.train()
        
    if i%50==0 and i!=0:
        torch.save(model.state_dict(), "jsrprabhu_"+str(+test+i)+".torch")
198/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
198/2:
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
device = 'cuda'
198/3:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
198/4:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

# for im_name in os.listdir("../dataset/test"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)
198/5:
batchSize = 1
img_siz = [512,512]
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.8))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(1000,patch_stack.shape[2])
        k_h = random.randint(1000,patch_stack.shape[1])
        #k_w = patch_stack.shape[2]
        #k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.float32)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.float32)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
198/6:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.float32)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.float32)
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
198/7:
def dice_loss(input, target):
    smooth = 1.
    iflat = input.view(-1)
    tflat = target.view(-1)
    intersection = (iflat * tflat).sum()
    
    return 1 - ((2. * intersection + smooth) /
              (iflat.sum() + tflat.sum() + smooth))
198/8:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.vis = False
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(1,1),stride=1,padding='same'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(3,3),stride=1,padding='same'),
            torch.nn.Conv2d(12,12,(1,1),stride=1,padding='same'),       
        ])
        #self.BC = torch.nn.BCELoss()
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3c = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm1c = nn.BatchNorm2d(1,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.obito = torch.nn.Conv2d(27,1,(12,12),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(trainable_backbone_layers=5,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.ReLU(self.norm3c(self.tobi(img_comb)))
            #print(img_ot[0].shape)
            images[ind] = img_ot[0]
            #ghost = self.ReLU(self.norm1c(self.obito(img_comb)))[0]
            #ghost = ghost>0.5
            #print(ghost[0].shape)
            #print(targets[ind]["sem_mask"][0].shape)
            # if self.training:
            #     seg_loss = dice_loss(ghost[0],targets[ind]["sem_mask"][0])
            #     color_loss[0] += seg_loss
            #if self.vis == True:
                #cv2.imwrite("ghost_img.png",(ghost.clone().detach().cpu().numpy()*255).transpose(1,2,0))
                #cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                #cv2.imwrite("ot_img.png",(img_ot[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))

        ret = self.maskrcnn(images,targets)
        # if self.training:
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        
        return ret
198/9:
model = jsrcnn_v5()
model.to(device)
#model.load_state_dict(torch.load('jsr_v5_2150.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 2e-4},{ 'params' : model.tobi.parameters(), 'lr' : 1e-3}], lr=1e-4)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False

scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
198/10:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    #loss_dict.pop("color_loss")
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())
    # losses = loss_dict["color_loss"]

    losses.backward()
    optimizer.step()
    model.vis = False
    print(i+test,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    if i%80==0:
        scheduler.step()
    if i%50==0:
        model.eval()
        evaluate(model, test_dataloader, device=device)
        model.train()
        
    if i%200==0 and i!=0:
        torch.save(model.state_dict(), "jsr_v5"+str(+test+i)+".torch")
198/11:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.vis = False
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(1,1),stride=1,padding='same'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(3,3),stride=1,padding='same'),
            torch.nn.Conv2d(12,12,(1,1),stride=1,padding='same'),       
        ])
        #self.BC = torch.nn.BCELoss()
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3c = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm1c = nn.BatchNorm2d(1,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.obito = torch.nn.Conv2d(27,1,(12,12),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(trainable_backbone_layers=5,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.ReLU(self.norm3c(self.tobi(img_comb)))
            #print(img_ot[0].shape)
            images[ind] = img_ot[0]
            #ghost = self.ReLU(self.norm1c(self.obito(img_comb)))[0]
            #ghost = ghost>0.5
            #print(ghost[0].shape)
            #print(targets[ind]["sem_mask"][0].shape)
            # if self.training:
            #     seg_loss = dice_loss(ghost[0],targets[ind]["sem_mask"][0])
            #     color_loss[0] += seg_loss
            if self.vis == True:
                #cv2.imwrite("ghost_img.png",(ghost.clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("ot_img.png",(img_ot[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))

        ret = self.maskrcnn(images,targets)
        # if self.training:
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        
        return ret
198/12:
model = jsrcnn_v5()
model.to(device)
#model.load_state_dict(torch.load('jsr_v5_2150.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 2e-4},{ 'params' : model.tobi.parameters(), 'lr' : 1e-3}], lr=1e-4)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False

scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
198/13:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    #loss_dict.pop("color_loss")
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())
    # losses = loss_dict["color_loss"]

    losses.backward()
    optimizer.step()
    model.vis = False
    print(i+test,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    if i%80==0:
        scheduler.step()
    if i%50==0:
        model.eval()
        evaluate(model, test_dataloader, device=device)
        model.train()
        
    if i%200==0 and i!=0:
        torch.save(model.state_dict(), "jsr_v5"+str(+test+i)+".torch")
198/14:
#model = jsrcnn_v5()
model.to(device)
#model.load_state_dict(torch.load('jsr_v5_2150.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 2e-4},{ 'params' : model.tobi.parameters(), 'lr' : 1e-3}], lr=1e-4)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False

scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
198/15:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    #loss_dict.pop("color_loss")
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())
    # losses = loss_dict["color_loss"]

    losses.backward()
    optimizer.step()
    model.vis = False
    print(i+test,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    if i%80==0:
        scheduler.step()
    if i%50==0:
        model.eval()
        evaluate(model, test_dataloader, device=device)
        model.train()
        
    if i%200==0 and i!=0:
        torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
198/16:
#model = jsrcnn_v5()
model.to(device)
#model.load_state_dict(torch.load('jsr_v5_2150.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 2e-4},{ 'params' : model.tobi.parameters(), 'lr' : 1e-30}], lr=1e-40)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False

scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
198/17:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    #loss_dict.pop("color_loss")
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())
    # losses = loss_dict["color_loss"]

    losses.backward()
    optimizer.step()
    model.vis = False
    print(i+test,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    if i%80==0:
        scheduler.step()
    if i%50==0:
        model.eval()
        evaluate(model, test_dataloader, device=device)
        model.train()
        
    if i%200==0 and i!=0:
        torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
198/18:
#model = jsrcnn_v5()
model.to(device)
#model.load_state_dict(torch.load('jsr_v5_2150.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 2e-4},{ 'params' : model.tobi.parameters(), 'lr' : 1e-6}], lr=1e-7)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False

scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
198/19:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    #loss_dict.pop("color_loss")
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())
    # losses = loss_dict["color_loss"]

    losses.backward()
    optimizer.step()
    model.vis = False
    print(i+test,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    if i%80==0:
        scheduler.step()
    if i%50==0:
        model.eval()
        evaluate(model, test_dataloader, device=device)
        model.train()
        
    if i%200==0 and i!=0:
        torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
198/20:
#model = jsrcnn_v5()
model.to(device)
#model.load_state_dict(torch.load('jsr_v5_2150.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 2e-5},{ 'params' : model.tobi.parameters(), 'lr' : 1e-12}], lr=1e-15)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False

scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
198/21:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    #loss_dict.pop("color_loss")
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())
    # losses = loss_dict["color_loss"]

    losses.backward()
    optimizer.step()
    model.vis = False
    print(i+test,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    if i%80==0:
        scheduler.step()
    if i%50==0:
        model.eval()
        evaluate(model, test_dataloader, device=device)
        model.train()
        
    if i%200==0 and i!=0:
        torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
198/22:
#model = jsrcnn_v5()
model.to(device)
#model.load_state_dict(torch.load('jsr_v5_2150.torch'))
optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 2e-5},{ 'params' : model.tobi.parameters(), 'lr' : 1e-18}], lr=1e-18)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False

scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
198/23:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    #loss_dict.pop("color_loss")
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())
    # losses = loss_dict["color_loss"]

    losses.backward()
    optimizer.step()
    model.vis = False
    print(i+test,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    if i%80==0:
        scheduler.step()
    if i%50==0:
        model.eval()
        evaluate(model, test_dataloader, device=device)
        model.train()
        
    if i%200==0 and i!=0:
        torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
198/24:
#model = jsrcnn_v5()
model.to(device)
#model.load_state_dict(torch.load('jsr_v5_2150.torch'))
optimizer = torch.optim.SGD([{ 'params': model.maskrcnn.parameters(), 'lr' : 2e-3},{ 'params' : model.tobi.parameters(), 'lr' : 1e-10}], lr=1e-10)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False

scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
198/25:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    #loss_dict.pop("color_loss")
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())
    # losses = loss_dict["color_loss"]

    losses.backward()
    optimizer.step()
    model.vis = False
    print(i+test,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    if i%80==0:
        scheduler.step()
    if i%50==0:
        model.eval()
        evaluate(model, test_dataloader, device=device)
        model.train()
        
    if i%200==0 and i!=0:
        torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
198/26:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
198/27:
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
device = 'cuda'
198/28:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
198/29:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

# for im_name in os.listdir("../dataset/test"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)
198/30:
batchSize = 2
img_siz = [512,512]
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.8))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(1000,patch_stack.shape[2])
        k_h = random.randint(1000,patch_stack.shape[1])
        #k_w = patch_stack.shape[2]
        #k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.float32)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.float32)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
198/31:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.float32)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.float32)
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
198/32:
def dice_loss(input, target):
    smooth = 1.
    iflat = input.view(-1)
    tflat = target.view(-1)
    intersection = (iflat * tflat).sum()
    
    return 1 - ((2. * intersection + smooth) /
              (iflat.sum() + tflat.sum() + smooth))
198/33:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.vis = False
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(1,1),stride=1,padding='same'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(3,3),stride=1,padding='same'),
            torch.nn.Conv2d(12,12,(1,1),stride=1,padding='same'),       
        ])
        #self.BC = torch.nn.BCELoss()
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3c = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.bc1 = nn.BatchNorm2d(12,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.bc2 = nn.BatchNorm2d(12,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.bc3 = nn.BatchNorm2d(12,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm1c = nn.BatchNorm2d(1,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.obito = torch.nn.Conv2d(27,1,(12,12),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(trainable_backbone_layers=5,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.bc1(img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.bc2(img3)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.bc3(img4)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.ReLU(self.norm3c(self.tobi(img_comb)))
            #print(img_ot[0].shape)
            images[ind] = img_ot[0]
            #ghost = self.ReLU(self.norm1c(self.obito(img_comb)))[0]
            #ghost = ghost>0.5
            #print(ghost[0].shape)
            #print(targets[ind]["sem_mask"][0].shape)
            # if self.training:
            #     seg_loss = dice_loss(ghost[0],targets[ind]["sem_mask"][0])
            #     color_loss[0] += seg_loss
            if self.vis == True:
                #cv2.imwrite("ghost_img.png",(ghost.clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("ot_img.png",(img_ot[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))

        ret = self.maskrcnn(images,targets)
        # if self.training:
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        
        return ret
198/34:
model = jsrcnn_v5()
model.to(device)
#model.load_state_dict(torch.load('jsr_v5_2150.torch'))
optimizer = torch.optim.SGD([{ 'params': model.maskrcnn.parameters(), 'lr' : 2e-2},{ 'params' : model.tobi.parameters(), 'lr' : 1e-8}], lr=1e-8)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False

scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
198/35:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    #loss_dict.pop("color_loss")
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())
    # losses = loss_dict["color_loss"]

    losses.backward()
    optimizer.step()
    model.vis = False
    print(i+test,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    if i%80==0:
        scheduler.step()
    if i%50==0:
        model.eval()
        evaluate(model, test_dataloader, device=device)
        model.train()
        
    if i%500==0 and i!=0:
        torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
198/36:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/Experimental')
sys.path.insert(1, '/home/prakharug/Experimental/pycoco')
from pycoco.engine import train_one_epoch, evaluate
198/37:
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') 
device = 'cuda'
198/38:
def combine_dims(a, start=0, count=2):
    """ Reshapes numpy array a by combining count dimensions, 
        starting at dimension index start """
    s = a.shape
    return np.reshape(a, s[:start] + (-1,) + s[start+count:])
198/39:
instance_toimage = []

for im_name in os.listdir("../dataset/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

# for im_name in os.listdir("../dataset/test"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset/test/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)
198/40:
batchSize = 2
img_siz = [512,512]
print(device)

from readline import append_history_file
from unittest.mock import patch


def loadBatch():
    rtrnd=random.randint(0,4)
    seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.8))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])
    batch_Imgs=[]
    batch_Data=[]
    for i in range(batchSize):
        idx=random.randint(0,len(instance_toimage)-1)
        patch_stack = seq(images=instance_toimage[idx])
        patch_stack = np.array(patch_stack)
        k_w = random.randint(1000,patch_stack.shape[2])
        k_h = random.randint(1000,patch_stack.shape[1])
        #k_w = patch_stack.shape[2]
        #k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            if np.all(dispim == 0):
                continue
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.float32)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.float32)
        if t==0:
            return loadBatch()
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        batch_Imgs.append(img)
        batch_Data.append(data)
    return batch_Imgs, batch_Data
198/41:
class MMCellDataset(Dataset):
    def __init__(self,root_dir, transform=None):
        """
        Args:
            csv_file (string): Path to the csv file with annotations.
            root_dir (string): Directory with all the images.
            transform (callable, optional): Optional transform to be applied
                on a sample.
        """
        self.root_dir = root_dir
        self.transform = transform
        self.img = []
        for im_name in os.listdir(self.root_dir):
            tmplist = []
            lent = len(im_name[:-4])
            print(im_name,im_name[:lent])
            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)
            tmplist.append(imgtmp[0])
            tmplist.append(imgtmp[1])
            tmplist.append(imgtmp[2])
            for gt_name in os.listdir("../dataset/ground_truths"):
                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
                    mask = np.array(cv2.imread("../dataset/ground_truths/"+gt_name,0))
                    tmplist.append(mask)
            tmplist = np.array(tmplist)
            print(tmplist.shape)
            self.img.append(tmplist)   
    
    def __len__(self):
        return len([name for name in os.listdir(self.root_dir)])

    def __getitem__(self, idx):
        if torch.is_tensor(idx):
            idx = idx.tolist()
        
        patch_stack = self.img[idx]
        patch_stack = np.array(patch_stack)
        k_w = patch_stack.shape[2]
        k_h = patch_stack.shape[1]
        o_w = random.randint(0,patch_stack.shape[2]-k_w)
        o_h = random.randint(0,patch_stack.shape[1]-k_h)
        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]
        instances = instances.transpose(1,2,0)
        data = {}
        masks = []
        boxes = []
        area = []
        t=0
        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))
        for a in range(instances.shape[2]):
            dispim = instances[:,:,a]
            x,y,w,h = cv2.boundingRect(dispim)
            boxes.append([x, y, x+w, y+h])
            area.append(torch.tensor(h*w))
            masks.append(dispim/255)
            sem_mask += dispim
            t=1
        masks = np.array(masks)
        sem_mask = sem_mask > 0
        sem_mask_c = torch.as_tensor(sem_mask.sum())
        neg_mask = (sem_mask == 0)
        neg_mask_c = torch.as_tensor(neg_mask.sum())
        sem_mask = np.array([sem_mask,sem_mask,sem_mask])
        sem_mask = torch.as_tensor(sem_mask,dtype=torch.float32)
        neg_mask = np.array([neg_mask,neg_mask,neg_mask])
        neg_mask = torch.as_tensor(neg_mask,dtype=torch.float32)
        masks = np.array(masks)
        masks = torch.as_tensor(masks, dtype=torch.uint8)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        area = torch.as_tensor(area, dtype=torch.float32)
        img = torch.as_tensor(patch_img, dtype=torch.float32)
        img = img/255
        data["boxes"] =  boxes
        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)
        data["iscrowd"] = iscrowd
        data["labels"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class
        data["masks"] = masks
        data["area"] = area
        data["image_id"] = torch.tensor(idx)
        data["sem_mask"] = sem_mask
        data["neg_mask"] = neg_mask
        data["sem_mask_c"] = sem_mask_c
        data["neg_mask_c"] = neg_mask_c
        return img,data
test = MMCellDataset("../dataset/test/")
def collate_fn(batch):
    return tuple(zip(*batch))
print(len(test))
test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)
198/42:
def dice_loss(input, target):
    smooth = 1.
    iflat = input.view(-1)
    tflat = target.view(-1)
    intersection = (iflat * tflat).sum()
    
    return 1 - ((2. * intersection + smooth) /
              (iflat.sum() + tflat.sum() + smooth))
198/43:
class jsrcnn_v5(nn.Module):
    def __init__(self,**kwargs) -> None:
        super(jsrcnn_v5, self).__init__()
        #self.param = param
        self.vis = False
        self.channel_trans = nn.ModuleList([
            torch.nn.Conv2d(3,12,(1,1),stride=1,padding='same'),
            #torch.nn.Conv2d(12,24,(3,3),stride=1,padding='same',padding_mode='replicate'),
            #torch.nn.Conv2d(24,24,(1,1),stride=1,padding='same',padding_mode='replicate'),
            torch.nn.Conv2d(12,12,(1,1),stride=1,padding='same'),
            torch.nn.Conv2d(12,12,(1,1),stride=1,padding='same'),       
        ])
        #self.BC = torch.nn.BCELoss()
        # self.maskbone = nn.Sequential(
        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),
        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),
        # )
        self.sigma = nn.Sigmoid()
        self.norm3c = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.bc1 = nn.BatchNorm2d(12,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.bc2 = nn.BatchNorm2d(12,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.bc3 = nn.BatchNorm2d(12,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.norm1c = nn.BatchNorm2d(1,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        self.ReLU = nn.ReLU()
        self.tobi = torch.nn.Conv2d(27,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        #self.obito = torch.nn.Conv2d(27,1,(12,12),stride=1,padding='same',padding_mode='replicate')
        #self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')
        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(trainable_backbone_layers=5,**kwargs)
        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier
        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one

    def forward(self, images, targets=None):
        trniml = []
        color_loss = torch.Tensor([0]).to(device)
        for ind in range(len(images)):
            img = torch.stack([images[ind]])
            imgog = img
            img = self.channel_trans[0](img)
            img = self.bc1(img)
            img = self.ReLU(img)
            # img = self.channel_trans[1](img)
            # img = self.ReLU(img)
            img3 = self.channel_trans[1](img)
            img3 = self.bc2(img3)
            img3 = self.ReLU(img3)
            img4 = self.channel_trans[2](img3)
            img4 = self.bc3(img4)
            img4 = self.ReLU(img4)
            img_comb = torch.concat((imgog,img3,img4),1)
            img_ot = self.ReLU(self.norm3c(self.tobi(img_comb)))
            #print(img_ot[0].shape)
            images[ind] = img_ot[0]
            #ghost = self.ReLU(self.norm1c(self.obito(img_comb)))[0]
            #ghost = ghost>0.5
            #print(ghost[0].shape)
            #print(targets[ind]["sem_mask"][0].shape)
            # if self.training:
            #     seg_loss = dice_loss(ghost[0],targets[ind]["sem_mask"][0])
            #     color_loss[0] += seg_loss
            if self.vis == True:
                #cv2.imwrite("ghost_img.png",(ghost.clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("og_img.png",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
                cv2.imwrite("ot_img.png",(img_ot[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))
            # img5 = self.channel_trans[3](img4)
            # img5 = self.ReLU(img5)
            # imgmask = torch.concat((img5,img4,imgog),1)
            # mk = self.maskbone(imgmask)
            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))
            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)
            # H = images[ind].shape[1]
            # W = images[ind].shape[2]
            # kek = images[ind].to(device)
            # images[ind] = images[ind].permute((1,2,0))
            # images[ind] = images[ind].reshape([H*W,12])
            # images[ind] = self.channel_trans(images[ind])
            # images[ind] = images[ind].reshape([H,W,3])
            # images[ind] = images[ind].permute((2,0,1))

        ret = self.maskrcnn(images,targets)
        # if self.training:
        #     color_loss = color_loss[0]
        #     ret["color_loss"] = color_loss
        
        return ret
198/44:
model = jsrcnn_v5()
model.to(device)
#model.load_state_dict(torch.load('jsr_v5_2150.torch'))
optimizer = torch.optim.SGD([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-2},{ 'params' : model.tobi.parameters(), 'lr' : 1e-6}], lr=1e-6)
#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)
model.train()
lmbda = lambda epoch: 0.8
# for param in model.maskrcnn.parameters():
#     param.requires_grad = False

scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)

minloss = 10
198/45:
test = 0
for i in range(10000):
    if i%20==0:
        model.vis = True
    images, targets = loadBatch()
    images = list(image.to(device) for image in images)
    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

    optimizer.zero_grad()
    loss_dict = model(images, targets)
    #loss_dict.pop("color_loss")
    # for losst in loss_dict.keys():
    #     print(losst,loss_dict[losst])
    losses = sum(loss for loss in loss_dict.values())
    # losses = loss_dict["color_loss"]

    losses.backward()
    optimizer.step()
    model.vis = False
    print(i+test,'loss:', losses.item())#," color_loss:",loss_dict["color_loss"].item())
    if i%80==0:
        scheduler.step()
    if i%50==0:
        model.eval()
        evaluate(model, test_dataloader, device=device)
        model.train()
        
    if i%500==0 and i!=0:
        torch.save(model.state_dict(), "jsr_v5_"+str(+test+i)+".torch")
205/1:
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/AFO')
sys.path.insert(1, '/home/prakharug/AFO/pycoco')
from pycoco.engine import train_one_epoch, evaluate
205/2:
instance_toimage = []

for im_name in os.listdir("../dataset1/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset1/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset1/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset1/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("../dataset1/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset1/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset1/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset1/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
205/3:
instance_toimage = []

for im_name in os.listdir("../dataset1/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset1/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset1/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset1/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("../dataset1/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset1/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset1/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset1/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
205/4:
instance_toimage = []

# for im_name in os.listdir("../dataset1/images"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset1/images/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset1/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset1/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)

for im_name in os.listdir("../dataset1/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset1/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset1/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset1/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
205/5:
instance_toimage = []

# for im_name in os.listdir("../dataset1/images"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset1/images/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset1/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset1/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset1/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset1/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset1/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset1/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
205/6:
cell_img = []

for mksl in cell_img:
    for mksin in range(3,len(mksl)):
        x,y,w,h = cv2.boundingRect(mksl[mksin])
        stck1 = mksl[0]*(mksl[mksin]/255)
        stck2 = mksl[0]*(mksl[mksin]/255)
        stck3 = mksl[0]*(mksl[mksin]/255)
        new_img = np.array([stck1,stck2,stck3])
        cv2.imshow('jsr',new_img)
        cv2.waitkey(0)
        cv2.destroyAllindows
205/7:
cell_img = []

for mksl in cell_img:
    for mksin in range(3,len(mksl)):
        x,y,w,h = cv2.boundingRect(mksl[mksin])
        stck1 = mksl[0]*(mksl[mksin]/255)
        stck2 = mksl[0]*(mksl[mksin]/255)
        stck3 = mksl[0]*(mksl[mksin]/255)
        new_img = np.array([stck1,stck2,stck3])
        cv2.imwrite('jsr',new_img)
        #cv2.waitkey(0)
        #cv2.destroyAllindows
205/8:
cell_img = []

for mksl in cell_img:
    for mksin in range(3,len(mksl)):
        x,y,w,h = cv2.boundingRect(mksl[mksin])
        stck1 = mksl[0]*(mksl[mksin]/255)
        stck2 = mksl[0]*(mksl[mksin]/255)
        stck3 = mksl[0]*(mksl[mksin]/255)
        new_img = np.array([stck1,stck2,stck3])
        cv2.imwrite('jsr.pmg',new_img)
        #cv2.waitkey(0)
        #cv2.destroyAllindows
205/9:
cell_img = []

for mksl in instance_toimage:
    for mksin in range(3,len(mksl)):
        x,y,w,h = cv2.boundingRect(mksl[mksin])
        stck1 = mksl[0]*(mksl[mksin]/255)
        stck2 = mksl[0]*(mksl[mksin]/255)
        stck3 = mksl[0]*(mksl[mksin]/255)
        new_img = np.array([stck1,stck2,stck3])
        cv2.imwrite('jsr.pmg',new_img)
        #cv2.waitkey(0)
        #cv2.destroyAllindows
205/10:
cell_img = []

for mksl in instance_toimage:
    for mksin in range(3,len(mksl)):
        x,y,w,h = cv2.boundingRect(mksl[mksin])
        stck1 = mksl[0]*(mksl[mksin]/255)
        stck2 = mksl[0]*(mksl[mksin]/255)
        stck3 = mksl[0]*(mksl[mksin]/255)
        new_img = np.array([stck1,stck2,stck3])
        cv2.imwrite('jsr.png',new_img)
        #cv2.waitkey(0)
        #cv2.destroyAllindows
205/11:
cell_img = []

for mksl in instance_toimage:
    for mksin in range(3,len(mksl)):
        x,y,w,h = cv2.boundingRect(mksl[mksin])
        stck1 = mksl[0]*(mksl[mksin]/255)
        stck2 = mksl[0]*(mksl[mksin]/255)
        stck3 = mksl[0]*(mksl[mksin]/255)
        new_img = np.array([stck1,stck2,stck3])
        cv2.imwrite('jsr.bmp',new_img)
        #cv2.waitkey(0)
        #cv2.destroyAllindows
205/12:
cell_img = []

for mksl in instance_toimage:
    for mksin in range(3,len(mksl)):
        x,y,w,h = cv2.boundingRect(mksl[mksin])
        stck1 = mksl[0]*(mksl[mksin]/255)
        stck2 = mksl[0]*(mksl[mksin]/255)
        stck3 = mksl[0]*(mksl[mksin]/255)
        new_img = np.array([stck1,stck2,stck3])
        print(new_img.shape)
        cv2.imwrite('jsr.pmg',new_img)
        #cv2.waitkey(0)
        #cv2.destroyAllindows
205/13:
cell_img = []

for mksl in instance_toimage:
    for mksin in range(3,len(mksl)):
        x,y,w,h = cv2.boundingRect(mksl[mksin])
        stck1 = mksl[0]*(mksl[mksin]/255)
        stck2 = mksl[0]*(mksl[mksin]/255)
        stck3 = mksl[0]*(mksl[mksin]/255)
        new_img = np.array([stck1,stck2,stck3])
        print(new_img.shape)
        cv2.imwrite('jsr.png',new_img)
        #cv2.waitkey(0)
        #cv2.destroyAllindows
205/14:
cell_img = []

for mksl in instance_toimage:
    for mksin in range(3,len(mksl)):
        x,y,w,h = cv2.boundingRect(mksl[mksin])
        stck1 = mksl[0]*(mksl[mksin]/255)
        stck2 = mksl[1]*(mksl[mksin]/255)
        stck3 = mksl[2]*(mksl[mksin]/255)
        new_img = np.array([stck1,stck2,stck3])
        print(new_img.shape)
        cv2.imwrite('jsr.png',new_img)
        #cv2.waitkey(0)
        #cv2.destroyAllindows
205/15:
cell_img = []

for mksl in instance_toimage:
    for mksin in range(3,len(mksl)):
        x,y,w,h = cv2.boundingRect(mksl[mksin])
        stck1 = mksl[0]*(mksl[mksin]/255)
        stck2 = mksl[1]*(mksl[mksin]/255)
        stck3 = mksl[2]*(mksl[mksin]/255)
        new_img = np.array([stck1,stck2,stck3])
        print(new_img.shape)
        cv2.imwrite('jsr.png',new_img)
        break
        #cv2.waitkey(0)
        #cv2.destroyAllindows
    break
205/16:
cell_img = []

for mksl in instance_toimage:
    for mksin in range(3,len(mksl)):
        x,y,w,h = cv2.boundingRect(mksl[mksin])
        stck1 = mksl[0]*(mksl[mksin]/255)
        stck2 = mksl[1]*(mksl[mksin]/255)
        stck3 = mksl[2]*(mksl[mksin]/255)
        new_img = np.array([stck1,stck2,stck3]).transpose(1,2,0)
        print(new_img.shape)
        cv2.imwrite('jsr.png',new_img)
        break
        #cv2.waitkey(0)
        #cv2.destroyAllindows
    break
205/17:
cell_img = []

for mksl in instance_toimage:
    for mksin in range(3,len(mksl)):
        x,y,w,h = cv2.boundingRect(mksl[mksin])
        stck1 = mksl[0]*(mksl[mksin]/255)
        stck2 = mksl[1]*(mksl[mksin]/255)
        stck3 = mksl[2]*(mksl[mksin]/255)
        new_img = np.array([stck1,stck2,stck3]).transpose(1,2,0)[x:x+w,y:y+h]
        print(new_img.shape)
        cv2.imwrite('jsr.png',new_img)
        break
        #cv2.waitkey(0)
        #cv2.destroyAllindows
    break
205/18:
cell_img = []

for mksl in instance_toimage:
    for mksin in range(3,len(mksl)):
        x,y,w,h = cv2.boundingRect(mksl[mksin])
        stck1 = mksl[0]*(mksl[mksin]/255)
        stck2 = mksl[1]*(mksl[mksin]/255)
        stck3 = mksl[2]*(mksl[mksin]/255)
        new_img = np.array([stck1,stck2,stck3]).transpose(1,2,0)[x:x+w,y:y+h,:]
        print(new_img.shape)
        cv2.imwrite('jsr.png',new_img)
        break
        #cv2.waitkey(0)
        #cv2.destroyAllindows
    break
205/19:
cell_img = []

for mksl in instance_toimage:
    for mksin in range(3,len(mksl)):
        x,y,w,h = cv2.boundingRect(mksl[mksin])
        stck1 = mksl[0]*(mksl[mksin]/255)
        stck2 = mksl[1]*(mksl[mksin]/255)
        stck3 = mksl[2]*(mksl[mksin]/255)
        new_img = np.array([stck1,stck2,stck3]).transpose(1,2,0)
        print(new_img.shape)
        cv2.imwrite('jsr.png',new_img)
        break
        #cv2.waitkey(0)
        #cv2.destroyAllindows
    break
205/20:
cell_img = []

for mksl in instance_toimage:
    for mksin in range(3,len(mksl)):
        x,y,w,h = cv2.boundingRect(mksl[mksin])
        stck1 = mksl[0]*(mksl[mksin]/255)
        stck2 = mksl[1]*(mksl[mksin]/255)
        stck3 = mksl[2]*(mksl[mksin]/255)
        new_img = np.array([stck1,stck2,stck3]).transpose(1,2,0)[x:x+h,y:y+w,:]
        print(new_img.shape)
        cv2.imwrite('jsr.png',new_img)
        break
        #cv2.waitkey(0)
        #cv2.destroyAllindows
    break
205/21:
cell_img = []

for mksl in instance_toimage:
    for mksin in range(3,len(mksl)):
        x,y,w,h = cv2.boundingRect(mksl[mksin])
        stck1 = mksl[0]*(mksl[mksin]/255)
        stck2 = mksl[1]*(mksl[mksin]/255)
        stck3 = mksl[2]*(mksl[mksin]/255)
        new_img = np.array([stck1,stck2,stck3]).transpose(1,2,0)[y:y+h,x:x+w,:]
        print(new_img.shape)
        cv2.imwrite('jsr.png',new_img)
        break
        #cv2.waitkey(0)
        #cv2.destroyAllindows
    break
205/22:
cell_img = []

for mksl in instance_toimage:
    for mksin in range(3,len(mksl)):
        x,y,w,h = cv2.boundingRect(mksl[mksin])
        stck1 = mksl[0]*(mksl[mksin]/255)
        stck2 = mksl[1]*(mksl[mksin]/255)
        stck3 = mksl[2]*(mksl[mksin]/255)
        new_img = np.array([stck1,stck2,stck3]).transpose(1,2,0)[y:y+h,x:x+w,:]
        print(new_img.shape)
        cv2.imwrite('jsr.png',new_img)
        break
        #cv2.waitkey(0)
        #cv2.destroyAllindows
    break
205/23:
cell_img = []

for mksl in instance_toimage:
    for mksin in range(3,len(mksl)):
        x,y,w,h = cv2.boundingRect(mksl[mksin])
        stck1 = mksl[0]*(mksl[mksin]/255)
        stck2 = mksl[1]*(mksl[mksin]/255)
        stck3 = mksl[2]*(mksl[mksin]/255)
        new_img = np.array([stck1,stck2,stck3]).transpose(1,2,0)[y:y+h,x:x+w,:]
        print(new_img.shape)
        cv2.imwrite('jsr.png',new_img)
        break
        #cv2.waitkey(0)
        #cv2.destroyAllindows
    break
205/24:
cell_img = []

for mksl in instance_toimage:
    for mksin in range(3,len(mksl)):
        x,y,w,h = cv2.boundingRect(mksl[mksin])
        stck1 = mksl[0]*(mksl[mksin]/255)
        stck2 = mksl[1]*(mksl[mksin]/255)
        stck3 = mksl[2]*(mksl[mksin]/255)
        new_img = np.array([stck1,stck2,stck3]).transpose(1,2,0)
        print(new_img.shape)
        cv2.imwrite('jsr.png',new_img)
        break
        #cv2.waitkey(0)
        #cv2.destroyAllindows
    break
205/25:
cell_img = []

for mksl in instance_toimage:
    for mksin in range(3,len(mksl)):
        x,y,w,h = cv2.boundingRect(mksl[mksin])
        stck1 = mksl[0]*(mksl[mksin]/255)
        stck2 = mksl[1]*(mksl[mksin]/255)
        stck3 = mksl[2]*(mksl[mksin]/255)
        new_img = np.array([stck1,stck2,stck3]).transpose(1,2,0)[y:y+h,x:x+w,:]
        print(new_img.shape)
        cv2.imwrite('jsr.png',new_img)
        break
        #cv2.waitkey(0)
        #cv2.destroyAllindows
    break
205/26:
cell_img = []

for mksl in instance_toimage:
    for mksin in range(3,len(mksl)):
        x,y,w,h = cv2.boundingRect(mksl[mksin])
        stck1 = mksl[0]*(mksl[mksin]/255)
        stck2 = mksl[1]*(mksl[mksin]/255)
        stck3 = mksl[2]*(mksl[mksin]/255)
        new_img = np.array([stck1,stck2,stck3]).transpose(1,2,0)[y:y+h,x:x+w,:]
        print(new_img.shape)
        cv2.imwrite('jsr.png',new_img)
        #break
        #cv2.waitkey(0)
        #cv2.destroyAllindows
    break
205/27:
cell_img = []

for mksl in instance_toimage:
    for mksin in range(3,len(mksl)):
        x,y,w,h = cv2.boundingRect(mksl[mksin])
        stck1 = mksl[0]*(mksl[mksin]/255)
        stck2 = mksl[1]*(mksl[mksin]/255)
        stck3 = mksl[2]*(mksl[mksin]/255)
        new_img = np.array([stck1,stck2,stck3]).transpose(1,2,0)[y:y+h,x:x+w,:]
        #print(new_img.shape)
        cv2.imwrite('jsr.png',new_img)
        #break
        #cv2.waitkey(0)
        #cv2.destroyAllindows
205/28: print(len(cell_img))
205/29:
cell_img = []

for mksl in instance_toimage:
    for mksin in range(3,len(mksl)):
        x,y,w,h = cv2.boundingRect(mksl[mksin])
        stck1 = mksl[0]*(mksl[mksin]/255)
        stck2 = mksl[1]*(mksl[mksin]/255)
        stck3 = mksl[2]*(mksl[mksin]/255)
        new_img = np.array([stck1,stck2,stck3]).transpose(1,2,0)[y:y+h,x:x+w,:]
        #print(new_img.shape)
        cell_img.append(new_img)
        cv2.imwrite('jsr.png',new_img)
        #break
        #cv2.waitkey(0)
        #cv2.destroyAllindows
205/30: print(len(cell_img))
205/31:
for iml in instance_toimage:
    i=0
    while i<3:
        tmpl = iml
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img))
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])
            cell_paste = seq(images=cell_img[idx])
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster:
                back = np.zeros(tmpl.shape,np.uint8)
                back[0:3,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                newimg = blend_transparent(tmpl[0:3],back)
                cv2.imwrite('jsr.png',newimg)
205/32:
cell_img = []

for mksl in instance_toimage:
    for mksin in range(3,len(mksl)):
        x,y,w,h = cv2.boundingRect(mksl[mksin])
        stck1 = mksl[0]*(mksl[mksin]/255)
        stck2 = mksl[1]*(mksl[mksin]/255)
        stck3 = mksl[2]*(mksl[mksin]/255)
        new_img = np.array([stck1,stck2,stck3],np.uint8).transpose(1,2,0)[y:y+h,x:x+w,:]
        #print(new_img.shape)
        cell_img.append(new_img)
        cv2.imwrite('jsr.png',new_img)
        #break
        #cv2.waitkey(0)
        #cv2.destroyAllindows
205/33:
def blend_transparent(face_img, overlay_t_img):
    # Split out the transparency mask from the colour info
    overlay_img = overlay_t_img[:,:,:3] # Grab the BRG planes
    overlay_mask = overlay_t_img[:,:,3:]  # And the alpha plane

    # Again calculate the inverse mask
    background_mask = 255 - overlay_mask

    # Turn the masks into three channel, so we can use them as weights
    overlay_mask = cv2.cvtColor(overlay_mask, cv2.COLOR_GRAY2BGR)
    background_mask = cv2.cvtColor(background_mask, cv2.COLOR_GRAY2BGR)

    # Create a masked out face image, and masked out overlay
    # We convert the images to floating point in range 0.0 - 1.0
    face_part = (face_img * (1 / 255.0)) * (background_mask * (1 / 255.0))
    overlay_part = (overlay_img * (1 / 255.0)) * (overlay_mask * (1 / 255.0))

    # And finally just add them together, and rescale it back to an 8bit integer image    
    return np.uint8(cv2.addWeighted(face_part, 255.0, overlay_part, 255.0, 0.0))
205/34:
for iml in instance_toimage:
    i=0
    while i<3:
        tmpl = iml
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img))
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])
            cell_paste = seq(images=cell_img[idx])
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster:
                back = np.zeros(tmpl.shape,np.uint8)
                back[0:3,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                newimg = blend_transparent(tmpl[0:3],back)
                cv2.imwrite('jsr.png',newimg)
205/35:
cell_img = []

for mksl in instance_toimage:
    for mksin in range(3,len(mksl)):
        x,y,w,h = cv2.boundingRect(mksl[mksin])
        stck1 = mksl[0]*(mksl[mksin]/255)
        stck2 = mksl[1]*(mksl[mksin]/255)
        stck3 = mksl[2]*(mksl[mksin]/255)
        new_img = np.array([stck1,stck2,stck3],np.uint8)[:,y:y+h,x:x+w]
        #print(new_img.shape)
        cell_img.append(new_img)
        cv2.imwrite('jsr.png',new_img)
        #break
        #cv2.waitkey(0)
        #cv2.destroyAllindows
205/36:
cell_img = []

for mksl in instance_toimage:
    for mksin in range(3,len(mksl)):
        x,y,w,h = cv2.boundingRect(mksl[mksin])
        stck1 = mksl[0]*(mksl[mksin]/255)
        stck2 = mksl[1]*(mksl[mksin]/255)
        stck3 = mksl[2]*(mksl[mksin]/255)
        new_img = np.array([stck1,stck2,stck3],np.uint8)[:,y:y+h,x:x+w]
        #print(new_img.shape)
        cell_img.append(new_img)
        #cv2.imwrite('jsr.png',new_img)
        #break
        #cv2.waitkey(0)
        #cv2.destroyAllindows
205/37:
def blend_transparent(face_img, overlay_t_img):
    # Split out the transparency mask from the colour info
    overlay_img = overlay_t_img[:,:,:3] # Grab the BRG planes
    overlay_mask = overlay_t_img[:,:,3:]  # And the alpha plane

    # Again calculate the inverse mask
    background_mask = 255 - overlay_mask

    # Turn the masks into three channel, so we can use them as weights
    overlay_mask = cv2.cvtColor(overlay_mask, cv2.COLOR_GRAY2BGR)
    background_mask = cv2.cvtColor(background_mask, cv2.COLOR_GRAY2BGR)

    # Create a masked out face image, and masked out overlay
    # We convert the images to floating point in range 0.0 - 1.0
    face_part = (face_img * (1 / 255.0)) * (background_mask * (1 / 255.0))
    overlay_part = (overlay_img * (1 / 255.0)) * (overlay_mask * (1 / 255.0))

    # And finally just add them together, and rescale it back to an 8bit integer image    
    return np.uint8(cv2.addWeighted(face_part, 255.0, overlay_part, 255.0, 0.0))
205/38:
for iml in instance_toimage:
    i=0
    while i<3:
        tmpl = iml
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img))
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])
            cell_paste = seq(images=cell_img[idx])
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster:
                back = np.zeros(tmpl.shape,np.uint8)
                back[0:3,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                newimg = blend_transparent(tmpl[0:3],back)
                cv2.imwrite('jsr.png',newimg)
205/39:
cell_img = []

for mksl in instance_toimage:
    for mksin in range(3,len(mksl)):
        x,y,w,h = cv2.boundingRect(mksl[mksin])
        stck1 = mksl[0]*(mksl[mksin]/255)
        stck2 = mksl[1]*(mksl[mksin]/255)
        stck3 = mksl[2]*(mksl[mksin]/255)
        new_img = np.array([stck1,stck2,stck3],np.uint8)[:,y:y+h,x:x+w]
        print(new_img.shape)
        cell_img.append(new_img)
        #cv2.imwrite('jsr.png',new_img)
        #break
        #cv2.waitkey(0)
        #cv2.destroyAllindows
205/40:
for iml in instance_toimage:
    i=0
    while i<3:
        tmpl = iml
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img))
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])
            print(cell_img[idx].shape)
            cell_paste = seq(images=cell_img[idx])
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster:
                back = np.zeros(tmpl.shape,np.uint8)
                back[0:3,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                newimg = blend_transparent(tmpl[0:3],back)
                cv2.imwrite('jsr.png',newimg)
205/41:
for iml in instance_toimage:
    i=0
    while i<3:
        tmpl = iml
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img))
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            print(cell_img[idx].shape)
            test = cell_img[idx]
            cell_paste = seq(images=test)
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster:
                back = np.zeros(tmpl.shape,np.uint8)
                back[0:3,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                newimg = blend_transparent(tmpl[0:3],back)
                cv2.imwrite('jsr.png',newimg)
205/42:
for iml in instance_toimage:
    i=0
    while i<3:
        tmpl = iml
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img))
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
    iaa.Sometimes(
        0.5,
        iaa.GaussianBlur(sigma=(0, 0.5))
    ),
    iaa.Rot90((rtrnd), keep_size=False)
    ])

            print(cell_img[idx].shape)
            test = cell_img[idx]
            cell_paste = seq(images=test)
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster:
                back = np.zeros(tmpl.shape,np.uint8)
                back[0:3,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                newimg = blend_transparent(tmpl[0:3],back)
                cv2.imwrite('jsr.png',newimg)
205/43:
for iml in instance_toimage:
    i=0
    while i<3:
        tmpl = iml
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img))
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster:
                back = np.zeros(tmpl.shape,np.uint8)
                back[0:3,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                newimg = blend_transparent(tmpl[0:3],back)
                cv2.imwrite('jsr.png',newimg)
205/44:
for iml in instance_toimage:
    i=0
    while i<3:
        tmpl = iml
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img))
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[0:3,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                newimg = blend_transparent(tmpl[0:3],back)
                cv2.imwrite('jsr.png',newimg)
205/45:
for iml in instance_toimage:
    i=0
    while i<3:
        tmpl = iml
        print(iml.shape)
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img))
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[0:3,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                newimg = blend_transparent(tmpl[0:3],back)
                cv2.imwrite('jsr.png',newimg)
205/46:
for iml in instance_toimage:
    i=0
    while i<3:
        tmpl = iml
        print(iml.shape)
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img))
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                print(back.shape)
                back[0:3,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                newimg = blend_transparent(tmpl[0:3],back)
                cv2.imwrite('jsr.png',newimg)
205/47:
for iml in instance_toimage:
    i=0
    while i<3:
        tmpl = iml
        print(iml.shape)
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img))
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[0:3,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                print(back.shape)
                newimg = blend_transparent(tmpl[0:3],back)
                cv2.imwrite('jsr.png',newimg)
205/48:
for iml in instance_toimage:
    i=0
    while i<3:
        tmpl = iml
        print(iml.shape)
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img))
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[0:3,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                print(tmpl[0:3].shape)
                newimg = blend_transparent(tmpl[0:3],back)
                cv2.imwrite('jsr.png',newimg)
205/49:
for iml in instance_toimage:
    i=0
    while i<3:
        tmpl = iml
        print(iml.shape)
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img))
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[0:3,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                print(tmpl[0:3].shape)
                newimg = blend_transparent(tmpl[0:3],back)
                cv2.imwrite('jsr.png',newimg)
205/50:
for iml in instance_toimage:
    i=0
    while i<3:
        tmpl = iml
        print(iml.shape)
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img))
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[0:3,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                print(tmpl[0:3].shape)
                newimg = blend_transparent(tmpl[0:3],back)
                cv2.imwrite('jsr.png',newimg)
205/51:
for iml in instance_toimage:
    i=0
    while i<3:
        tmpl = iml
        #print(iml.shape)
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img))
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[0:3,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                print(tmpl[0:3].shape)
                newimg = blend_transparent(tmpl[0:3],back)
                cv2.imwrite('jsr.png',newimg)
205/52:
for iml in instance_toimage:
    i=0
    while i<3:
        tmpl = iml
        #print(iml.shape)
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img))
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[0:3,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                print(tmpl[0:3].shape,back.shape)
                newimg = blend_transparent(tmpl[0:3],back)
                cv2.imwrite('jsr.png',newimg)
205/53:
def blend_non_transparent(face_img, overlay_img):
    # Let's find a mask covering all the non-black (foreground) pixels
    # NB: We need to do this on grayscale version of the image
    gray_overlay = cv2.cvtColor(overlay_img, cv2.COLOR_BGR2GRAY)
    overlay_mask = cv2.threshold(gray_overlay, 1, 255, cv2.THRESH_BINARY)[1]

    # Let's shrink and blur it a little to make the transitions smoother...
    overlay_mask = cv2.erode(overlay_mask, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3)))
    overlay_mask = cv2.blur(overlay_mask, (3, 3))

    # And the inverse mask, that covers all the black (background) pixels
    background_mask = 255 - overlay_mask

    # Turn the masks into three channel, so we can use them as weights
    overlay_mask = cv2.cvtColor(overlay_mask, cv2.COLOR_GRAY2BGR)
    background_mask = cv2.cvtColor(background_mask, cv2.COLOR_GRAY2BGR)

    # Create a masked out face image, and masked out overlay
    # We convert the images to floating point in range 0.0 - 1.0
    face_part = (face_img * (1 / 255.0)) * (background_mask * (1 / 255.0))
    overlay_part = (overlay_img * (1 / 255.0)) * (overlay_mask * (1 / 255.0))

    # And finally just add them together, and rescale it back to an 8bit integer image
    return np.uint8(cv2.addWeighted(face_part, 255.0, overlay_part, 255.0, 0.0))
205/54:
def blend_non_transparent(face_img, overlay_img):
    # Let's find a mask covering all the non-black (foreground) pixels
    # NB: We need to do this on grayscale version of the image
    gray_overlay = cv2.cvtColor(overlay_img, cv2.COLOR_BGR2GRAY)
    overlay_mask = cv2.threshold(gray_overlay, 1, 255, cv2.THRESH_BINARY)[1]

    # Let's shrink and blur it a little to make the transitions smoother...
    #overlay_mask = cv2.erode(overlay_mask, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3)))
    #overlay_mask = cv2.blur(overlay_mask, (3, 3))

    # And the inverse mask, that covers all the black (background) pixels
    background_mask = 255 - overlay_mask

    # Turn the masks into three channel, so we can use them as weights
    overlay_mask = cv2.cvtColor(overlay_mask, cv2.COLOR_GRAY2BGR)
    background_mask = cv2.cvtColor(background_mask, cv2.COLOR_GRAY2BGR)

    # Create a masked out face image, and masked out overlay
    # We convert the images to floating point in range 0.0 - 1.0
    face_part = (face_img * (1 / 255.0)) * (background_mask * (1 / 255.0))
    overlay_part = (overlay_img * (1 / 255.0)) * (overlay_mask * (1 / 255.0))

    # And finally just add them together, and rescale it back to an 8bit integer image
    return np.uint8(cv2.addWeighted(face_part, 255.0, overlay_part, 255.0, 0.0))
205/55:
for iml in instance_toimage:
    i=0
    while i<3:
        tmpl = iml
        #print(iml.shape)
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img))
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[0:3,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                print(tmpl[0:3].shape,back.shape)
                newimg = blend_transparent(tmpl[0:3],back)
                cv2.imwrite('jsr.png',newimg)
205/56:
for iml in instance_toimage:
    i=0
    while i<3:
        tmpl = iml
        #print(iml.shape)
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img))
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[0:3,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                print(tmpl[0:3].shape,back.shape)
                newimg = blend_transparent(tmpl[0:3].transpose(1,2,0),back.transpose(1,2,0))
                cv2.imwrite('jsr.png',newimg)
205/57:
for iml in instance_toimage:
    i=0
    while i<3:
        tmpl = iml
        #print(iml.shape)
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img))
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[0:3,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                print(tmpl[0:3].shape,back.shape)
                newimg = blend_transparent(tmpl[0:3],back)
                cv2.imwrite('jsr.png',newimg)
205/58:
def blend_non_transparent(face_img, overlay_img):
    # Let's find a mask covering all the non-black (foreground) pixels
    # NB: We need to do this on grayscale version of the image
    gray_overlay = cv2.cvtColor(overlay_img, cv2.COLOR_BGR2GRAY)
    overlay_mask = cv2.threshold(gray_overlay, 1, 255, cv2.THRESH_BINARY)[1]

    # Let's shrink and blur it a little to make the transitions smoother...
    #overlay_mask = cv2.erode(overlay_mask, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3)))
    #overlay_mask = cv2.blur(overlay_mask, (3, 3))

    # And the inverse mask, that covers all the black (background) pixels
    background_mask = (255*np.ones(overlay_mask.shape)) - overlay_mask

    # Turn the masks into three channel, so we can use them as weights
    overlay_mask = cv2.cvtColor(overlay_mask, cv2.COLOR_GRAY2BGR)
    background_mask = cv2.cvtColor(background_mask, cv2.COLOR_GRAY2BGR)

    # Create a masked out face image, and masked out overlay
    # We convert the images to floating point in range 0.0 - 1.0
    face_part = (face_img * (1 / 255.0)) * (background_mask * (1 / 255.0))
    overlay_part = (overlay_img * (1 / 255.0)) * (overlay_mask * (1 / 255.0))

    # And finally just add them together, and rescale it back to an 8bit integer image
    return np.uint8(cv2.addWeighted(face_part, 255.0, overlay_part, 255.0, 0.0))
205/59:
for iml in instance_toimage:
    i=0
    while i<3:
        tmpl = iml
        #print(iml.shape)
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img))
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[0:3,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                print(tmpl[0:3].shape,back.shape)
                newimg = blend_transparent(tmpl[0:3],back)
                cv2.imwrite('jsr.png',newimg)
205/60:
for iml in instance_toimage:
    i=0
    while i<3:
        tmpl = iml
        #print(iml.shape)
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img))
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[0:3,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                print(tmpl[0:3].shape,back.shape)
                newimg = blend_transparent(tmpl[0:3].transpose(1,2,0),back.transpose(1,2,0))
                cv2.imwrite('jsr.png',newimg)
205/61:
def blend_non_transparent(face_img, overlay_img):
    # Let's find a mask covering all the non-black (foreground) pixels
    # NB: We need to do this on grayscale version of the image
    gray_overlay = cv2.cvtColor(overlay_img, cv2.COLOR_BGR2GRAY)
    overlay_mask = cv2.threshold(gray_overlay, 1, 255, cv2.THRESH_BINARY)[1]

    # Let's shrink and blur it a little to make the transitions smoother...
    #overlay_mask = cv2.erode(overlay_mask, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3)))
    #overlay_mask = cv2.blur(overlay_mask, (3, 3))

    # And the inverse mask, that covers all the black (background) pixels
    background_mask = (255*np.ones(overlay_mask.shape)) - overlay_mask

    # Turn the masks into three channel, so we can use them as weights
    overlay_mask = cv2.cvtColor(overlay_mask, cv2.COLOR_GRAY2BGR)
    background_mask = cv2.cvtColor(background_mask, cv2.COLOR_GRAY2BGR)

    # Create a masked out face image, and masked out overlay
    # We convert the images to floating point in range 0.0 - 1.0
    face_part = (face_img * (1 / 255.0)) * (background_mask * (1 / 255.0))
    overlay_part = (overlay_img * (1 / 255.0)) * (overlay_mask * (1 / 255.0))

    # And finally just add them together, and rescale it back to an 8bit integer image
    return np.uint8(cv2.addWeighted(face_part, 255.0, overlay_part, 255.0, 0.0))
205/62:
for iml in instance_toimage:
    i=0
    while i<3:
        tmpl = iml
        #print(iml.shape)
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img))
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[0:3,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                print(tmpl[0:3].shape,back.shape)
                newimg = blend_non_transparent(tmpl[0:3].transpose(1,2,0),back.transpose(1,2,0))
                cv2.imwrite('jsr.png',newimg)
205/63:
for iml in instance_toimage:
    i=0
    while i<3:
        tmpl = iml
        #print(iml.shape)
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img))
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[0:3,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                print(tmpl[0:3].shape,back.shape)
                #vis0 = cv2.fromarray(back.transpose(1,2,0))
                newimg = blend_non_transparent(cv2.fromarray(tmpl[0:3].transpose(1,2,0)),cv2.fromarray(back.transpose(1,2,0)))
                cv2.imwrite('jsr.png',newimg)
205/64:
for iml in instance_toimage:
    i=0
    while i<3:
        tmpl = iml
        #print(iml.shape)
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img))
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[0:3,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                print(tmpl[0:3].shape,type(back.shape))
                newimg = blend_non_transparent(tmpl[0:3].transpose(1,2,0),back.transpose(1,2,0))
                cv2.imwrite('jsr.png',newimg)
205/65:
for iml in instance_toimage:
    i=0
    while i<3:
        tmpl = iml
        #print(iml.shape)
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img))
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[0:3,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                print(tmpl[0:3].shape,type(back))
                newimg = blend_non_transparent(tmpl[0:3].transpose(1,2,0),back.transpose(1,2,0))
                cv2.imwrite('jsr.png',newimg)
205/66:
for iml in instance_toimage:
    i=0
    while i<3:
        tmpl = iml
        #print(iml.shape)
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img))
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[0:3,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                print(type(tmpl[0:3]),type(back))
                newimg = blend_non_transparent(tmpl[0:3].transpose(1,2,0),back.transpose(1,2,0))
                cv2.imwrite('jsr.png',newimg)
205/67:
for iml in instance_toimage:
    i=0
    while i<3:
        tmpl = iml
        #print(iml.shape)
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img))
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[:,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                cv2.imwrite('jsr.png',back)
                print(type(tmpl[0:3]),type(back))
                newimg = blend_non_transparent(tmpl[0:3].transpose(1,2,0),back.transpose(1,2,0))
                cv2.imwrite('jsr.png',newimg)
205/68:
for iml in instance_toimage:
    i=0
    while i<3:
        tmpl = iml
        #print(iml.shape)
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img))
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[:,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                cv2.imwrite('jsr.png',cell_paste)
                print(type(tmpl[0:3]),type(back))
                newimg = blend_non_transparent(tmpl[0:3].transpose(1,2,0),back.transpose(1,2,0))
                cv2.imwrite('jsr.png',newimg)
205/69:
for iml in instance_toimage:
    i=0
    while i<3:
        tmpl = iml
        #print(iml.shape)
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img))
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[:,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                print(cell_paste.shape)
                cv2.imwrite('jsr.png',cell_paste)
                print(type(tmpl[0:3]),type(back))
                newimg = blend_non_transparent(tmpl[0:3].transpose(1,2,0),back.transpose(1,2,0))
                cv2.imwrite('jsr.png',newimg)
205/70:
for iml in instance_toimage:
    i=0
    while i<3:
        tmpl = iml
        #print(iml.shape)
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img))
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[:,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                print(cell_paste.shape,k_h)
                cv2.imwrite('jsr.png',cell_paste)
                print(type(tmpl[0:3]),type(back))
                newimg = blend_non_transparent(tmpl[0:3].transpose(1,2,0),back.transpose(1,2,0))
                cv2.imwrite('jsr.png',newimg)
205/71:
for iml in instance_toimage:
    i=0
    while i<3:
        tmpl = iml
        #print(iml.shape)
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img))
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[:,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                print(cell_paste.shape,k_h)
                cv2.imwrite('jsr.png',back.transpose(1,2,0))
                print(type(tmpl[0:3]),type(back))
                newimg = blend_non_transparent(tmpl[0:3].transpose(1,2,0),back.transpose(1,2,0))
                cv2.imwrite('jsr.png',newimg)
205/72:
for iml in instance_toimage:
    i=0
    while i<3:
        tmpl = iml
        #print(iml.shape)
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img))
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[:,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                print(cell_paste.shape,k_h)
                cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                print(type(tmpl[0:3]),type(back))
                newimg = blend_non_transparent(tmpl[0:3].transpose(1,2,0),back.transpose(1,2,0))
                cv2.imwrite('jsr.png',newimg)
205/73:
def blend_non_transparent(face_img, overlay_img):
    # Let's find a mask covering all the non-black (foreground) pixels
    # NB: We need to do this on grayscale version of the image
    print("overlay img ",overlay_img.shape)
    gray_overlay = cv2.cvtColor(overlay_img, cv2.COLOR_BGR2GRAY)
    overlay_mask = cv2.threshold(gray_overlay, 1, 255, cv2.THRESH_BINARY)[1]

    # Let's shrink and blur it a little to make the transitions smoother...
    #overlay_mask = cv2.erode(overlay_mask, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3)))
    #overlay_mask = cv2.blur(overlay_mask, (3, 3))

    # And the inverse mask, that covers all the black (background) pixels
    background_mask = (255*np.ones(overlay_mask.shape)) - overlay_mask

    # Turn the masks into three channel, so we can use them as weights
    overlay_mask = cv2.cvtColor(overlay_mask, cv2.COLOR_GRAY2BGR)
    background_mask = cv2.cvtColor(background_mask, cv2.COLOR_GRAY2BGR)

    # Create a masked out face image, and masked out overlay
    # We convert the images to floating point in range 0.0 - 1.0
    face_part = (face_img * (1 / 255.0)) * (background_mask * (1 / 255.0))
    overlay_part = (overlay_img * (1 / 255.0)) * (overlay_mask * (1 / 255.0))

    # And finally just add them together, and rescale it back to an 8bit integer image
    return np.uint8(cv2.addWeighted(face_part, 255.0, overlay_part, 255.0, 0.0))
205/74:
for iml in instance_toimage:
    i=0
    while i<3:
        tmpl = iml
        #print(iml.shape)
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img))
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[:,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                print(cell_paste.shape,k_h)
                #cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                print(type(tmpl[0:3]),type(back))
                newimg = blend_non_transparent(tmpl[0:3].transpose(1,2,0),back.transpose(1,2,0))
                cv2.imwrite('jsr.png',newimg)
205/75:
def blend_non_transparent(face_img, overlay_img):
    # Let's find a mask covering all the non-black (foreground) pixels
    # NB: We need to do this on grayscale version of the image
    print("overlay img ",overlay_img.shape)
    gray_overlay = cv2.cvtColor(overlay_img, cv2.COLOR_BGR2GRAY)
    overlay_mask = cv2.threshold(gray_overlay, 1, 255, cv2.THRESH_BINARY)[1]

    # Let's shrink and blur it a little to make the transitions smoother...
    #overlay_mask = cv2.erode(overlay_mask, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3)))
    #overlay_mask = cv2.blur(overlay_mask, (3, 3))

    # And the inverse mask, that covers all the black (background) pixels
    background_mask = (255) - overlay_mask

    # Turn the masks into three channel, so we can use them as weights
    overlay_mask = cv2.cvtColor(overlay_mask, cv2.COLOR_GRAY2BGR)
    background_mask = cv2.cvtColor(background_mask, cv2.COLOR_GRAY2BGR)

    # Create a masked out face image, and masked out overlay
    # We convert the images to floating point in range 0.0 - 1.0
    face_part = (face_img * (1 / 255.0)) * (background_mask * (1 / 255.0))
    overlay_part = (overlay_img * (1 / 255.0)) * (overlay_mask * (1 / 255.0))

    # And finally just add them together, and rescale it back to an 8bit integer image
    return np.uint8(cv2.addWeighted(face_part, 255.0, overlay_part, 255.0, 0.0))
205/76:
for iml in instance_toimage:
    i=0
    while i<3:
        tmpl = iml
        #print(iml.shape)
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img))
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[:,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                print(cell_paste.shape,k_h)
                #cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                print(type(tmpl[0:3]),type(back))
                newimg = blend_non_transparent(tmpl[0:3].transpose(1,2,0),back.transpose(1,2,0))
                cv2.imwrite('jsr.png',newimg)
205/77:
for iml in instance_toimage:
    i=0
    while i<3:
        tmpl = iml
        #print(iml.shape)
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img)-1)
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[:,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                #print(cell_paste.shape,k_h)
                #cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                #print(type(tmpl[0:3]),type(back))
                newimg = blend_non_transparent(tmpl[0:3].transpose(1,2,0),back.transpose(1,2,0))
                cv2.imwrite('jsr.png',newimg)
205/78:
for iml in instance_toimage:
    i=0
    while i<3:
        tmpl = iml
        #print(iml.shape)
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img)-1)
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[:,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                #print(cell_paste.shape,k_h)
                #cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                #print(type(tmpl[0:3]),type(back))
                tmpl[0:3] = blend_non_transparent(tmpl[0:3].transpose(1,2,0),back.transpose(1,2,0))
                cv2.imwrite('jsr.png',tmpl[0:3])
205/79:
for iml in instance_toimage:
    i=0
    while i<3:
        tmpl = iml
        #print(iml.shape)
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img)-1)
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[:,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                #print(cell_paste.shape,k_h)
                #cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                #print(type(tmpl[0:3]),type(back))
                tmpl[0:3] = blend_non_transparent(tmpl[0:3].transpose(1,2,0),back.transpose(1,2,0)).transpose(2,0,1)
                cv2.imwrite('jsr.png',tmpl[0:3])
205/80:
for iml in instance_toimage:
    i=0
    while i<3:
        tmpl = iml
        #print(iml.shape)
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img)-1)
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[:,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                #print(cell_paste.shape,k_h)
                #cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                #print(type(tmpl[0:3]),type(back))
                tmpl[0:3] = blend_non_transparent(tmpl[0:3].transpose(1,2,0),back.transpose(1,2,0)).transpose(2,0,1)
                cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
205/81:
for iml in instance_toimage:
    i=0
    while i<=3:
        i+=1
        tmpl = iml
        #print(iml.shape)
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img)-1)
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[:,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                grao = cv2.cvtColor(back, cv2.COLOR_BGR2GRAY)
                grao = (grao>1)*255
                #print(cell_paste.shape,k_h)
                #cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                #print(type(tmpl[0:3]),type(back))
                tmpl[0:3] = blend_non_transparent(tmpl[0:3].transpose(1,2,0),back.transpose(1,2,0)).transpose(2,0,1)
                cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                tmpl.append(grao)
205/82:
for iml in instance_toimage:
    i=0
    while i<=3:
        i+=1
        tmpl = iml
        #print(iml.shape)
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img)-1)
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[:,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                grao = cv2.cvtColor(back.transpose(1,2,0), cv2.COLOR_BGR2GRAY)
                grao = (grao>1)*255
                #print(cell_paste.shape,k_h)
                #cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                #print(type(tmpl[0:3]),type(back))
                tmpl[0:3] = blend_non_transparent(tmpl[0:3].transpose(1,2,0),back.transpose(1,2,0)).transpose(2,0,1)
                cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                tmpl.append(grao)
205/83:
for iml in instance_toimage:
    i=0
    while i<=3:
        i+=1
        tmpl = iml
        #print(iml.shape)
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img)-1)
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[:,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                grao = cv2.cvtColor(back.transpose(1,2,0), cv2.COLOR_BGR2GRAY)
                grao = (grao>1)*255
                #print(cell_paste.shape,k_h)
                #cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                #print(type(tmpl[0:3]),type(back))
                tmpl[0:3] = blend_non_transparent(tmpl[0:3].transpose(1,2,0),back.transpose(1,2,0)).transpose(2,0,1)
                cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                tmpl = np.append(tmpl,grao)
205/84:
for iml in instance_toimage:
    i=0
    while i<=3:
        i+=1
        tmpl = iml
        #print(iml.shape)
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img)-1)
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[:,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                grao = cv2.cvtColor(back.transpose(1,2,0), cv2.COLOR_BGR2GRAY)
                grao = (grao>1)*255
                #print(cell_paste.shape,k_h)
                #cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                #print(type(tmpl[0:3]),type(back))
                tmpl[0:3] = blend_non_transparent(tmpl[0:3].transpose(1,2,0),back.transpose(1,2,0)).transpose(2,0,1)
                cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                tmpl = np.append(tmpl,grao,0)
205/85:
for iml in instance_toimage:
    i=0
    while i<=3:
        i+=1
        tmpl = iml
        #print(iml.shape)
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img)-1)
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[:,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                grao = cv2.cvtColor(back.transpose(1,2,0), cv2.COLOR_BGR2GRAY)
                grao = (grao>1)*255
                #print(cell_paste.shape,k_h)
                #cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                #print(type(tmpl[0:3]),type(back))
                tmpl[0:3] = blend_non_transparent(tmpl[0:3].transpose(1,2,0),back.transpose(1,2,0)).transpose(2,0,1)
                cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                print(tmpl.shape,grao.shape)
                tmpl = np.append(tmpl,grao,0)
205/86:
for iml in instance_toimage:
    i=0
    while i<=3:
        i+=1
        tmpl = iml
        #print(iml.shape)
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img)-1)
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[:,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                grao = cv2.cvtColor(back.transpose(1,2,0), cv2.COLOR_BGR2GRAY)
                grao = (grao>1)*255
                grao = np.array([grao])
                #print(cell_paste.shape,k_h)
                #cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                #print(type(tmpl[0:3]),type(back))
                tmpl[0:3] = blend_non_transparent(tmpl[0:3].transpose(1,2,0),back.transpose(1,2,0)).transpose(2,0,1)
                cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                print(tmpl.shape,grao.shape)
                tmpl = np.append(tmpl,grao,0)
205/87:
for iml in instance_toimage:
    i=0
    while i<=3:
        i+=1
        tmpl = iml
        #print(iml.shape)
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img)-1)
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[:,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                grao = cv2.cvtColor(back.transpose(1,2,0), cv2.COLOR_BGR2GRAY)
                grao = (grao>1)*255
                grao = np.array([grao])
                #print(cell_paste.shape,k_h)
                #cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                #print(type(tmpl[0:3]),type(back))
                tmpl[0:3] = blend_non_transparent(tmpl[0:3].transpose(1,2,0),back.transpose(1,2,0)).transpose(2,0,1)
                cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                print(tmpl.shape,grao.shape)
                tmpl = np.append(tmpl,grao,0)
205/88:
for iml in instance_toimage:
    i=0
    while i<=3:
        i+=1
        tmpl = iml
        print('hello')
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img)-1)
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[:,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                grao = cv2.cvtColor(back.transpose(1,2,0), cv2.COLOR_BGR2GRAY)
                grao = (grao>1)*255
                grao = np.array([grao])
                #print(cell_paste.shape,k_h)
                #cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                #print(type(tmpl[0:3]),type(back))
                tmpl[0:3] = blend_non_transparent(tmpl[0:3].transpose(1,2,0),back.transpose(1,2,0)).transpose(2,0,1)
                cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                print(tmpl.shape,grao.shape)
                tmpl = np.append(tmpl,grao,0)
205/89:
for iml in instance_toimage:
    i=0
    while i<=3:
        i+=1
        tmpl = iml
        print('hello')
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img)-1)
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[:,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                grao = cv2.cvtColor(back.transpose(1,2,0), cv2.COLOR_BGR2GRAY)
                grao = (grao>1)*255
                grao = np.array([grao])
                #print(cell_paste.shape,k_h)
                #cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                #print(type(tmpl[0:3]),type(back))
                tmpl[0:3] = blend_non_transparent(tmpl[0:3].transpose(1,2,0),back.transpose(1,2,0)).transpose(2,0,1)
                cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                #print(tmpl.shape,grao.shape)
                tmpl = np.append(tmpl,grao,0)
205/90:
def blend_non_transparent(face_img, overlay_img):
    # Let's find a mask covering all the non-black (foreground) pixels
    # NB: We need to do this on grayscale version of the image
    #print("overlay img ",overlay_img.shape)
    gray_overlay = cv2.cvtColor(overlay_img, cv2.COLOR_BGR2GRAY)
    overlay_mask = cv2.threshold(gray_overlay, 1, 255, cv2.THRESH_BINARY)[1]

    # Let's shrink and blur it a little to make the transitions smoother...
    #overlay_mask = cv2.erode(overlay_mask, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3)))
    #overlay_mask = cv2.blur(overlay_mask, (3, 3))

    # And the inverse mask, that covers all the black (background) pixels
    background_mask = (255) - overlay_mask

    # Turn the masks into three channel, so we can use them as weights
    overlay_mask = cv2.cvtColor(overlay_mask, cv2.COLOR_GRAY2BGR)
    background_mask = cv2.cvtColor(background_mask, cv2.COLOR_GRAY2BGR)

    # Create a masked out face image, and masked out overlay
    # We convert the images to floating point in range 0.0 - 1.0
    face_part = (face_img * (1 / 255.0)) * (background_mask * (1 / 255.0))
    overlay_part = (overlay_img * (1 / 255.0)) * (overlay_mask * (1 / 255.0))

    # And finally just add them together, and rescale it back to an 8bit integer image
    return np.uint8(cv2.addWeighted(face_part, 255.0, overlay_part, 255.0, 0.0))
205/91:
def blend_non_transparent(face_img, overlay_img):
    # Let's find a mask covering all the non-black (foreground) pixels
    # NB: We need to do this on grayscale version of the image
    #print("overlay img ",overlay_img.shape)
    gray_overlay = cv2.cvtColor(overlay_img, cv2.COLOR_BGR2GRAY)
    overlay_mask = cv2.threshold(gray_overlay, 1, 255, cv2.THRESH_BINARY)[1]

    # Let's shrink and blur it a little to make the transitions smoother...
    #overlay_mask = cv2.erode(overlay_mask, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3)))
    #overlay_mask = cv2.blur(overlay_mask, (3, 3))

    # And the inverse mask, that covers all the black (background) pixels
    background_mask = (255) - overlay_mask

    # Turn the masks into three channel, so we can use them as weights
    overlay_mask = cv2.cvtColor(overlay_mask, cv2.COLOR_GRAY2BGR)
    background_mask = cv2.cvtColor(background_mask, cv2.COLOR_GRAY2BGR)

    # Create a masked out face image, and masked out overlay
    # We convert the images to floating point in range 0.0 - 1.0
    face_part = (face_img * (1 / 255.0)) * (background_mask * (1 / 255.0))
    overlay_part = (overlay_img * (1 / 255.0)) * (overlay_mask * (1 / 255.0))

    # And finally just add them together, and rescale it back to an 8bit integer image
    return np.uint8(cv2.addWeighted(face_part, 255.0, overlay_part, 255.0, 0.0))
205/92:
for iml in instance_toimage:
    i=0
    while i<=3:
        i+=1
        tmpl = iml
        print('hello')
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img)-1)
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[:,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                grao = cv2.cvtColor(back.transpose(1,2,0), cv2.COLOR_BGR2GRAY)
                grao = (grao>1)*255
                grao = np.array([grao])
                #print(cell_paste.shape,k_h)
                #cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                #print(type(tmpl[0:3]),type(back))
                tmpl[0:3] = blend_non_transparent(tmpl[0:3].transpose(1,2,0),back.transpose(1,2,0)).transpose(2,0,1)
                cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                #print(tmpl.shape,grao.shape)
                tmpl = np.append(tmpl,grao,0)
205/93:
for iml in instance_toimage:
    i=0
    while i<=3:
        i+=1
        tmpl = iml
        print('hello')
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img)-1)
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[:,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                grao = cv2.cvtColor(back.transpose(1,2,0), cv2.COLOR_BGR2GRAY)
                grao = (grao>1)*255
                grao = np.array([grao])
                #print(cell_paste.shape,k_h)
                #cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                #print(type(tmpl[0:3]),type(back))
                tmpl[0:3] = blend_non_transparent(tmpl[0:3].transpose(1,2,0),back.transpose(1,2,0)).transpose(2,0,1)
                cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                print(tmpl.shape,grao.shape)
                tmpl = np.append(tmpl,grao,0)
205/94:
for iml in instance_toimage:
    i=0
    while i<=3:
        i+=1
        tmpl = iml
        print('hello')
        cnt=0
        for j in range(0,10):
            paster = True
            idx = random.randint(0,len(cell_img)-1)
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[:,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                grao = cv2.cvtColor(back.transpose(1,2,0), cv2.COLOR_BGR2GRAY)
                grao = (grao>1)*255
                grao = np.array([grao])
                #print(cell_paste.shape,k_h)
                #cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                #print(type(tmpl[0:3]),type(back))
                tmpl[0:3] = blend_non_transparent(tmpl[0:3].transpose(1,2,0),back.transpose(1,2,0)).transpose(2,0,1)
                cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                print(tmpl.shape,grao.shape)
                tmpl = np.append(tmpl,grao,0)
205/95:
for iml in instance_toimage:
    i=0
    while i<=3:
        i+=1
        tmpl = iml
        print('hello')
        cnt=0
        for j in range(0,10):
            paster = True
            idx = random.randint(0,len(cell_img)-1)
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster==True:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[:,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                grao = cv2.cvtColor(back.transpose(1,2,0), cv2.COLOR_BGR2GRAY)
                grao = (grao>1)*255
                grao = np.array([grao])
                #print(cell_paste.shape,k_h)
                #cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                #print(type(tmpl[0:3]),type(back))
                tmpl[0:3] = blend_non_transparent(tmpl[0:3].transpose(1,2,0),back.transpose(1,2,0)).transpose(2,0,1)
                cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                print(tmpl.shape,grao.shape)
                tmpl = np.append(tmpl,grao,0)
205/96:
for iml in instance_toimage:
    i=0
    while i<=3:
        i+=1
        tmpl = iml
        print('hello')
        cnt=0
        for j in range(0,10):
            paster = True
            idx = random.randint(0,len(cell_img)-1)
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            instances.transpose(1,2,0)
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster==True:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[:,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                grao = cv2.cvtColor(back.transpose(1,2,0), cv2.COLOR_BGR2GRAY)
                grao = (grao>1)*255
                grao = np.array([grao])
                #print(cell_paste.shape,k_h)
                #cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                #print(type(tmpl[0:3]),type(back))
                tmpl[0:3] = blend_non_transparent(tmpl[0:3].transpose(1,2,0),back.transpose(1,2,0)).transpose(2,0,1)
                cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                print(tmpl.shape,grao.shape)
                tmpl = np.append(tmpl,grao,0)
205/97:
for iml in instance_toimage:
    i=0
    while i<=3:
        i+=1
        tmpl = iml
        print('hello')
        cnt=0
        for j in range(0,10):
            paster = True
            idx = random.randint(0,len(cell_img)-1)
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            instances.transpose(1,2,0)
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster==True:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[:,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                grao = cv2.cvtColor(back.transpose(1,2,0), cv2.COLOR_BGR2GRAY)
                grao = (grao>1)*255
                grao = np.array([grao])
                #print(cell_paste.shape,k_h)
                #cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                #print(type(tmpl[0:3]),type(back))
                tmpl[0:3] = blend_non_transparent(tmpl[0:3].transpose(1,2,0),back.transpose(1,2,0)).transpose(2,0,1)
                cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                print(tmpl.shape,grao.shape)
                tmpl = np.append(tmpl,grao,0)
205/98:
for iml in instance_toimage:
    i=0
    while i<=3:
        i+=1
        tmpl = iml.copy()
        print('hello')
        cnt=0
        for j in range(0,10):
            paster = True
            idx = random.randint(0,len(cell_img)-1)
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            instances.transpose(1,2,0)
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster==True:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[:,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                grao = cv2.cvtColor(back.transpose(1,2,0), cv2.COLOR_BGR2GRAY)
                grao = (grao>1)*255
                grao = np.array([grao])
                #print(cell_paste.shape,k_h)
                #cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                #print(type(tmpl[0:3]),type(back))
                tmpl[0:3] = blend_non_transparent(tmpl[0:3].transpose(1,2,0),back.transpose(1,2,0)).transpose(2,0,1)
                cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                print(tmpl.shape,grao.shape)
                tmpl = np.append(tmpl,grao,0)
205/99:
for iml in instance_toimage:
    i=0
    while i<=3:
        i+=1
        tmpl = iml.copy()
        print('hello')
        cnt=0
        for j in range(0,10):
            paster = True
            idx = random.randint(0,len(cell_img)-1)
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            instances.transpose(1,2,0)
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster==True:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[:,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                grao = cv2.cvtColor(back.transpose(1,2,0), cv2.COLOR_BGR2GRAY)
                grao = (grao>0)*255
                grao = np.array([grao])
                #print(cell_paste.shape,k_h)
                #cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                #print(type(tmpl[0:3]),type(back))
                tmpl[0:3] = blend_non_transparent(tmpl[0:3].transpose(1,2,0),back.transpose(1,2,0)).transpose(2,0,1)
                cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                print(tmpl.shape,grao.shape)
                tmpl = np.append(tmpl,grao,0)
205/100:
for iml in instance_toimage:
    i=0
    while i<=3:
        i+=1
        tmpl = iml.copy()
        print('hello')
        cnt=0
        for j in range(0,10):
            paster = True
            idx = random.randint(0,len(cell_img)-1)
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            instances.transpose(1,2,0)
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster==True:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[:,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                grao = cv2.cvtColor(back.transpose(1,2,0), cv2.COLOR_BGR2GRAY)
                grao = (grao>0)*255
                grao = np.array([grao])
                #print(cell_paste.shape,k_h)
                #cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                #print(type(tmpl[0:3]),type(back))
                tmpl[0:3] = blend_non_transparent(tmpl[0:3].transpose(1,2,0),back.transpose(1,2,0)).transpose(2,0,1)
                cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                print(tmpl.shape,grao.shape)
                tmpl = np.append(tmpl,grao,0)
                print(tmpl.shape,grao.shape)
205/101:
cell_img = []

for mksl in instance_toimage:
    for mksin in range(3,len(mksl)):
        x,y,w,h = cv2.boundingRect(mksl[mksin])
        stck1 = mksl[0]*(mksl[mksin]/255)
        stck2 = mksl[1]*(mksl[mksin]/255)
        stck3 = mksl[2]*(mksl[mksin]/255)
        new_img = np.array([stck1,stck2,stck3],np.uint8)[:,x:x+w,y:y+h]
        print(new_img.shape)
        cell_img.append(new_img)
        cv2.imwrite('jsr.png',new_img.transpose(1,2,0))
        #break
        #cv2.waitkey(0)
        #cv2.destroyAllindows
205/102:
cell_img = []

for mksl in instance_toimage:
    print(mksl.shape)
    for mksin in range(3,len(mksl)):
        x,y,w,h = cv2.boundingRect(mksl[mksin])
        stck1 = mksl[0]*(mksl[mksin]/255)
        stck2 = mksl[1]*(mksl[mksin]/255)
        stck3 = mksl[2]*(mksl[mksin]/255)
        new_img = np.array([stck1,stck2,stck3],np.uint8)[:,x:x+w,y:y+h]
        print(new_img.shape)
        cell_img.append(new_img)
        cv2.imwrite('jsr.png',new_img)
        #break
        #cv2.waitkey(0)
        #cv2.destroyAllindows
205/103:
cell_img = []

for mksl in instance_toimage:
    print(len(mksl.shape))
    for mksin in range(3,len(mksl)):
        x,y,w,h = cv2.boundingRect(mksl[mksin])
        stck1 = mksl[0]*(mksl[mksin]/255)
        stck2 = mksl[1]*(mksl[mksin]/255)
        stck3 = mksl[2]*(mksl[mksin]/255)
        new_img = np.array([stck1,stck2,stck3],np.uint8)[:,x:x+w,y:y+h]
        print(new_img.shape)
        cell_img.append(new_img)
        cv2.imwrite('jsr.png',new_img)
        #break
        #cv2.waitkey(0)
        #cv2.destroyAllindows
205/104:
cell_img = []

for mksl in instance_toimage:
    print(len(mksl))
    for mksin in range(3,len(mksl)):
        x,y,w,h = cv2.boundingRect(mksl[mksin])
        stck1 = mksl[0]*(mksl[mksin]/255)
        stck2 = mksl[1]*(mksl[mksin]/255)
        stck3 = mksl[2]*(mksl[mksin]/255)
        new_img = np.array([stck1,stck2,stck3],np.uint8)[:,x:x+w,y:y+h]
        print(new_img.shape)
        cell_img.append(new_img)
        cv2.imwrite('jsr.png',new_img)
        #break
        #cv2.waitkey(0)
        #cv2.destroyAllindows
205/105:
cell_img = []

for mksl in instance_toimage:
    #print(len(mksl))
    for mksin in range(3,len(mksl)):
        x,y,w,h = cv2.boundingRect(mksl[mksin])
        stck1 = mksl[0]*(mksl[mksin]/255)
        stck2 = mksl[1]*(mksl[mksin]/255)
        stck3 = mksl[2]*(mksl[mksin]/255)
        print(stck1.shape)
        new_img = np.array([stck1,stck2,stck3],np.uint8)[:,x:x+w,y:y+h]
        print(new_img.shape)
        cell_img.append(new_img)
        cv2.imwrite('jsr.png',new_img)
        #break
        #cv2.waitkey(0)
        #cv2.destroyAllindows
205/106:
cell_img = []

for mksl in instance_toimage:
    #print(len(mksl))
    for mksin in range(3,len(mksl)):
        x,y,w,h = cv2.boundingRect(mksl[mksin])
        stck1 = mksl[0]*(mksl[mksin]/255)
        stck2 = mksl[1]*(mksl[mksin]/255)
        stck3 = mksl[2]*(mksl[mksin]/255)
        print(stck1.shape)
        new_img = np.array([stck1,stck2,stck3],np.uint8)[:,y:y+h,x:x+w]
        print(new_img.shape)
        cell_img.append(new_img)
        cv2.imwrite('jsr.png',new_img)
        #break
        #cv2.waitkey(0)
        #cv2.destroyAllindows
205/107:
cell_img = []

for mksl in instance_toimage:
    #print(len(mksl))
    for mksin in range(3,len(mksl)):
        x,y,w,h = cv2.boundingRect(mksl[mksin])
        stck1 = mksl[0]*(mksl[mksin]/255)
        stck2 = mksl[1]*(mksl[mksin]/255)
        stck3 = mksl[2]*(mksl[mksin]/255)
        print(stck1.shape)
        new_img = np.array([stck1,stck2,stck3],np.uint8)[:,y:y+h,x:x+w]
        print(new_img.shape)
        cell_img.append(new_img)
        cv2.imwrite('jsr.png',new_img.transpose(1,2,0))
        #break
        #cv2.waitkey(0)
        #cv2.destroyAllindows
205/108:
cell_img = []

for mksl in instance_toimage:
    #print(len(mksl))
    for mksin in range(3,len(mksl)):
        x,y,w,h = cv2.boundingRect(mksl[mksin])
        stck1 = mksl[0]*(mksl[mksin]/255)
        stck2 = mksl[1]*(mksl[mksin]/255)
        stck3 = mksl[2]*(mksl[mksin]/255)
        print(stck1.shape)
        new_img = np.array([stck1,stck2,stck3],np.uint8)[:,y:y+h,x:x+w]
        print(new_img.shape)
        cell_img.append(new_img)
        cv2.imwrite('jsr.png',new_img.transpose(1,2,0))
        #break
        #cv2.waitkey(0)
        #cv2.destroyAllindows
205/109:
%matplotlib inline
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/AFO')
sys.path.insert(1, '/home/prakharug/AFO/pycoco')
from pycoco.engine import train_one_epoch, evaluate
205/110:
for iml in instance_toimage:
    i=0
    while i<=3:
        i+=1
        tmpl = iml.copy()
        print('hello')
        cnt=0
        for j in range(0,10):
            paster = True
            idx = random.randint(0,len(cell_img)-1)
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            instances.transpose(1,2,0)
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                cv2.imshow(dispim)
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster==True:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[:,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                grao = cv2.cvtColor(back.transpose(1,2,0), cv2.COLOR_BGR2GRAY)
                grao = (grao>0)*255
                grao = np.array([grao])
                #print(cell_paste.shape,k_h)
                #cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                #print(type(tmpl[0:3]),type(back))
                tmpl[0:3] = blend_non_transparent(tmpl[0:3].transpose(1,2,0),back.transpose(1,2,0)).transpose(2,0,1)
                cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                print(tmpl.shape,grao.shape)
                tmpl = np.append(tmpl,grao,0)
                print(tmpl.shape,grao.shape)
205/111:
for iml in instance_toimage:
    i=0
    while i<=3:
        i+=1
        tmpl = iml.copy()
        print('hello')
        cnt=0
        for j in range(0,10):
            paster = True
            idx = random.randint(0,len(cell_img)-1)
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            instances.transpose(1,2,0)
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                cv2.imshow('jsr',dispim)
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster==True:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[:,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                grao = cv2.cvtColor(back.transpose(1,2,0), cv2.COLOR_BGR2GRAY)
                grao = (grao>0)*255
                grao = np.array([grao])
                #print(cell_paste.shape,k_h)
                #cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                #print(type(tmpl[0:3]),type(back))
                tmpl[0:3] = blend_non_transparent(tmpl[0:3].transpose(1,2,0),back.transpose(1,2,0)).transpose(2,0,1)
                cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                print(tmpl.shape,grao.shape)
                tmpl = np.append(tmpl,grao,0)
                print(tmpl.shape,grao.shape)
206/1:
for iml in instance_toimage:
    i=0
    while i<=3:
        i+=1
        tmpl = iml.copy()
        print('hello')
        cnt=0
        for j in range(0,10):
            paster = True
            idx = random.randint(0,len(cell_img)-1)
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            instances.transpose(1,2,0)
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                cv2.imshow('jsr',np.array([dispim]))
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster==True:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[:,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                grao = cv2.cvtColor(back.transpose(1,2,0), cv2.COLOR_BGR2GRAY)
                grao = (grao>0)*255
                grao = np.array([grao])
                #print(cell_paste.shape,k_h)
                #cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                #print(type(tmpl[0:3]),type(back))
                tmpl[0:3] = blend_non_transparent(tmpl[0:3].transpose(1,2,0),back.transpose(1,2,0)).transpose(2,0,1)
                cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                print(tmpl.shape,grao.shape)
                tmpl = np.append(tmpl,grao,0)
                print(tmpl.shape,grao.shape)
206/2:
%matplotlib inline
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/AFO')
sys.path.insert(1, '/home/prakharug/AFO/pycoco')
from pycoco.engine import train_one_epoch, evaluate
206/3:
instance_toimage = []

# for im_name in os.listdir("../dataset1/images"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset1/images/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset1/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset1/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset1/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset1/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset1/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset1/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
206/4:
cell_img = []

for mksl in instance_toimage:
    #print(len(mksl))
    for mksin in range(3,len(mksl)):
        x,y,w,h = cv2.boundingRect(mksl[mksin])
        stck1 = mksl[0]*(mksl[mksin]/255)
        stck2 = mksl[1]*(mksl[mksin]/255)
        stck3 = mksl[2]*(mksl[mksin]/255)
        print(stck1.shape)
        new_img = np.array([stck1,stck2,stck3],np.uint8)[:,y:y+h,x:x+w]
        print(new_img.shape)
        cell_img.append(new_img)
        cv2.imwrite('jsr.png',new_img.transpose(1,2,0))
        #break
        #cv2.waitkey(0)
        #cv2.destroyAllindows
206/5:
def blend_non_transparent(face_img, overlay_img):
    # Let's find a mask covering all the non-black (foreground) pixels
    # NB: We need to do this on grayscale version of the image
    #print("overlay img ",overlay_img.shape)
    gray_overlay = cv2.cvtColor(overlay_img, cv2.COLOR_BGR2GRAY)
    overlay_mask = cv2.threshold(gray_overlay, 1, 255, cv2.THRESH_BINARY)[1]

    # Let's shrink and blur it a little to make the transitions smoother...
    #overlay_mask = cv2.erode(overlay_mask, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3)))
    #overlay_mask = cv2.blur(overlay_mask, (3, 3))

    # And the inverse mask, that covers all the black (background) pixels
    background_mask = (255) - overlay_mask

    # Turn the masks into three channel, so we can use them as weights
    overlay_mask = cv2.cvtColor(overlay_mask, cv2.COLOR_GRAY2BGR)
    background_mask = cv2.cvtColor(background_mask, cv2.COLOR_GRAY2BGR)

    # Create a masked out face image, and masked out overlay
    # We convert the images to floating point in range 0.0 - 1.0
    face_part = (face_img * (1 / 255.0)) * (background_mask * (1 / 255.0))
    overlay_part = (overlay_img * (1 / 255.0)) * (overlay_mask * (1 / 255.0))

    # And finally just add them together, and rescale it back to an 8bit integer image
    return np.uint8(cv2.addWeighted(face_part, 255.0, overlay_part, 255.0, 0.0))
206/6:
for iml in instance_toimage:
    i=0
    while i<=3:
        i+=1
        tmpl = iml.copy()
        print('hello')
        cnt=0
        for j in range(0,10):
            paster = True
            idx = random.randint(0,len(cell_img)-1)
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            instances.transpose(1,2,0)
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                cv2.imshow('jsr',np.array([dispim]))
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster==True:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[:,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                grao = cv2.cvtColor(back.transpose(1,2,0), cv2.COLOR_BGR2GRAY)
                grao = (grao>0)*255
                grao = np.array([grao])
                #print(cell_paste.shape,k_h)
                #cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                #print(type(tmpl[0:3]),type(back))
                tmpl[0:3] = blend_non_transparent(tmpl[0:3].transpose(1,2,0),back.transpose(1,2,0)).transpose(2,0,1)
                cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                print(tmpl.shape,grao.shape)
                tmpl = np.append(tmpl,grao,0)
                print(tmpl.shape,grao.shape)
207/1:
for iml in instance_toimage:
    i=0
    while i<=3:
        i+=1
        tmpl = iml.copy()
        print('hello')
        cnt=0
        for j in range(0,10):
            paster = True
            idx = random.randint(0,len(cell_img)-1)
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            instances.transpose(1,2,0)
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                cv2.imshow('jsr',np.array([dispim]))
                cv2.waitKey(0)
                cv2.destroyAllWindows()
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster==True:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[:,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                grao = cv2.cvtColor(back.transpose(1,2,0), cv2.COLOR_BGR2GRAY)
                grao = (grao>0)*255
                grao = np.array([grao])
                #print(cell_paste.shape,k_h)
                #cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                #print(type(tmpl[0:3]),type(back))
                tmpl[0:3] = blend_non_transparent(tmpl[0:3].transpose(1,2,0),back.transpose(1,2,0)).transpose(2,0,1)
                cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                print(tmpl.shape,grao.shape)
                tmpl = np.append(tmpl,grao,0)
                print(tmpl.shape,grao.shape)
207/2:
for iml in instance_toimage:
    i=0
    while i<=3:
        i+=1
        tmpl = iml.copy()
        print('hello')
        cnt=0
        for j in range(0,10):
            paster = True
            idx = random.randint(0,len(cell_img)-1)
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            instances.transpose(1,2,0)
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                cv2.imshow('jsr',np.array([dispim]))
                cv2.waitKey(0)
                cv2.destroyAllWindows()
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster==True:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[:,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                grao = cv2.cvtColor(back.transpose(1,2,0), cv2.COLOR_BGR2GRAY)
                grao = (grao>0)*255
                grao = np.array([grao])
                #print(cell_paste.shape,k_h)
                #cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                #print(type(tmpl[0:3]),type(back))
                tmpl[0:3] = blend_non_transparent(tmpl[0:3].transpose(1,2,0),back.transpose(1,2,0)).transpose(2,0,1)
                cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                print(tmpl.shape,grao.shape)
                tmpl = np.append(tmpl,grao,0)
                print(tmpl.shape,grao.shape)
207/3:
%matplotlib inline
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/AFO')
sys.path.insert(1, '/home/prakharug/AFO/pycoco')
from pycoco.engine import train_one_epoch, evaluate
207/4:
instance_toimage = []

# for im_name in os.listdir("../dataset1/images"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset1/images/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset1/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset1/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset1/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset1/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset1/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset1/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
207/5:
cell_img = []

for mksl in instance_toimage:
    #print(len(mksl))
    for mksin in range(3,len(mksl)):
        x,y,w,h = cv2.boundingRect(mksl[mksin])
        stck1 = mksl[0]*(mksl[mksin]/255)
        stck2 = mksl[1]*(mksl[mksin]/255)
        stck3 = mksl[2]*(mksl[mksin]/255)
        print(stck1.shape)
        new_img = np.array([stck1,stck2,stck3],np.uint8)[:,y:y+h,x:x+w]
        print(new_img.shape)
        cell_img.append(new_img)
        cv2.imwrite('jsr.png',new_img.transpose(1,2,0))
        #break
        #cv2.waitkey(0)
        #cv2.destroyAllindows
207/6:
def blend_non_transparent(face_img, overlay_img):
    # Let's find a mask covering all the non-black (foreground) pixels
    # NB: We need to do this on grayscale version of the image
    #print("overlay img ",overlay_img.shape)
    gray_overlay = cv2.cvtColor(overlay_img, cv2.COLOR_BGR2GRAY)
    overlay_mask = cv2.threshold(gray_overlay, 1, 255, cv2.THRESH_BINARY)[1]

    # Let's shrink and blur it a little to make the transitions smoother...
    #overlay_mask = cv2.erode(overlay_mask, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3)))
    #overlay_mask = cv2.blur(overlay_mask, (3, 3))

    # And the inverse mask, that covers all the black (background) pixels
    background_mask = (255) - overlay_mask

    # Turn the masks into three channel, so we can use them as weights
    overlay_mask = cv2.cvtColor(overlay_mask, cv2.COLOR_GRAY2BGR)
    background_mask = cv2.cvtColor(background_mask, cv2.COLOR_GRAY2BGR)

    # Create a masked out face image, and masked out overlay
    # We convert the images to floating point in range 0.0 - 1.0
    face_part = (face_img * (1 / 255.0)) * (background_mask * (1 / 255.0))
    overlay_part = (overlay_img * (1 / 255.0)) * (overlay_mask * (1 / 255.0))

    # And finally just add them together, and rescale it back to an 8bit integer image
    return np.uint8(cv2.addWeighted(face_part, 255.0, overlay_part, 255.0, 0.0))
207/7:
for iml in instance_toimage:
    i=0
    while i<=3:
        i+=1
        tmpl = iml.copy()
        print('hello')
        cnt=0
        for j in range(0,10):
            paster = True
            idx = random.randint(0,len(cell_img)-1)
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            instances.transpose(1,2,0)
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                cv2.imshow('jsr',np.array([dispim]))
                cv2.waitKey(0)
                cv2.destroyAllWindows()
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster==True:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[:,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                grao = cv2.cvtColor(back.transpose(1,2,0), cv2.COLOR_BGR2GRAY)
                grao = (grao>0)*255
                grao = np.array([grao])
                #print(cell_paste.shape,k_h)
                #cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                #print(type(tmpl[0:3]),type(back))
                tmpl[0:3] = blend_non_transparent(tmpl[0:3].transpose(1,2,0),back.transpose(1,2,0)).transpose(2,0,1)
                cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                print(tmpl.shape,grao.shape)
                tmpl = np.append(tmpl,grao,0)
                print(tmpl.shape,grao.shape)
208/1:
%matplotlib inline
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/AFO')
sys.path.insert(1, '/home/prakharug/AFO/pycoco')
from pycoco.engine import train_one_epoch, evaluate
208/2:
instance_toimage = []

# for im_name in os.listdir("../dataset1/images"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset1/images/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset1/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset1/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset1/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset1/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset1/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset1/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
208/3:
cell_img = []

for mksl in instance_toimage:
    #print(len(mksl))
    for mksin in range(3,len(mksl)):
        x,y,w,h = cv2.boundingRect(mksl[mksin])
        stck1 = mksl[0]*(mksl[mksin]/255)
        stck2 = mksl[1]*(mksl[mksin]/255)
        stck3 = mksl[2]*(mksl[mksin]/255)
        print(stck1.shape)
        new_img = np.array([stck1,stck2,stck3],np.uint8)[:,y:y+h,x:x+w]
        print(new_img.shape)
        cell_img.append(new_img)
        cv2.imwrite('jsr.png',new_img.transpose(1,2,0))
        #break
        #cv2.waitkey(0)
        #cv2.destroyAllindows
208/4:
def blend_non_transparent(face_img, overlay_img):
    # Let's find a mask covering all the non-black (foreground) pixels
    # NB: We need to do this on grayscale version of the image
    #print("overlay img ",overlay_img.shape)
    gray_overlay = cv2.cvtColor(overlay_img, cv2.COLOR_BGR2GRAY)
    overlay_mask = cv2.threshold(gray_overlay, 1, 255, cv2.THRESH_BINARY)[1]

    # Let's shrink and blur it a little to make the transitions smoother...
    #overlay_mask = cv2.erode(overlay_mask, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3)))
    #overlay_mask = cv2.blur(overlay_mask, (3, 3))

    # And the inverse mask, that covers all the black (background) pixels
    background_mask = (255) - overlay_mask

    # Turn the masks into three channel, so we can use them as weights
    overlay_mask = cv2.cvtColor(overlay_mask, cv2.COLOR_GRAY2BGR)
    background_mask = cv2.cvtColor(background_mask, cv2.COLOR_GRAY2BGR)

    # Create a masked out face image, and masked out overlay
    # We convert the images to floating point in range 0.0 - 1.0
    face_part = (face_img * (1 / 255.0)) * (background_mask * (1 / 255.0))
    overlay_part = (overlay_img * (1 / 255.0)) * (overlay_mask * (1 / 255.0))

    # And finally just add them together, and rescale it back to an 8bit integer image
    return np.uint8(cv2.addWeighted(face_part, 255.0, overlay_part, 255.0, 0.0))
208/5:
for iml in instance_toimage:
    i=0
    while i<=3:
        i+=1
        tmpl = iml.copy()
        print('hello')
        cnt=0
        for j in range(0,10):
            paster = True
            idx = random.randint(0,len(cell_img)-1)
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            instances.transpose(1,2,0)
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                cv2.imwrite('writer.jpg',dispim)
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster==True:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[:,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                grao = cv2.cvtColor(back.transpose(1,2,0), cv2.COLOR_BGR2GRAY)
                grao = (grao>0)*255
                grao = np.array([grao])
                #print(cell_paste.shape,k_h)
                #cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                #print(type(tmpl[0:3]),type(back))
                tmpl[0:3] = blend_non_transparent(tmpl[0:3].transpose(1,2,0),back.transpose(1,2,0)).transpose(2,0,1)
                cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                print(tmpl.shape,grao.shape)
                tmpl = np.append(tmpl,grao,0)
                print(tmpl.shape,grao.shape)
208/6:
%matplotlib inline
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/AFO')
sys.path.insert(1, '/home/prakharug/AFO/pycoco')
from pycoco.engine import train_one_epoch, evaluate
208/7:
instance_toimage = []

# for im_name in os.listdir("../dataset1/images"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset1/images/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset1/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset1/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset1/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset1/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset1/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset1/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
208/8:
cell_img = []

for mksl in instance_toimage:
    #print(len(mksl))
    for mksin in range(3,len(mksl)):
        x,y,w,h = cv2.boundingRect(mksl[mksin])
        stck1 = mksl[0]*(mksl[mksin]/255)
        stck2 = mksl[1]*(mksl[mksin]/255)
        stck3 = mksl[2]*(mksl[mksin]/255)
        print(stck1.shape)
        new_img = np.array([stck1,stck2,stck3],np.uint8)[:,y:y+h,x:x+w]
        print(new_img.shape)
        cell_img.append(new_img)
        cv2.imwrite('jsr.png',new_img.transpose(1,2,0))
        #break
        #cv2.waitkey(0)
        #cv2.destroyAllindows
208/9:
def blend_non_transparent(face_img, overlay_img):
    # Let's find a mask covering all the non-black (foreground) pixels
    # NB: We need to do this on grayscale version of the image
    #print("overlay img ",overlay_img.shape)
    gray_overlay = cv2.cvtColor(overlay_img, cv2.COLOR_BGR2GRAY)
    overlay_mask = cv2.threshold(gray_overlay, 1, 255, cv2.THRESH_BINARY)[1]

    # Let's shrink and blur it a little to make the transitions smoother...
    #overlay_mask = cv2.erode(overlay_mask, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3)))
    #overlay_mask = cv2.blur(overlay_mask, (3, 3))

    # And the inverse mask, that covers all the black (background) pixels
    background_mask = (255) - overlay_mask

    # Turn the masks into three channel, so we can use them as weights
    overlay_mask = cv2.cvtColor(overlay_mask, cv2.COLOR_GRAY2BGR)
    background_mask = cv2.cvtColor(background_mask, cv2.COLOR_GRAY2BGR)

    # Create a masked out face image, and masked out overlay
    # We convert the images to floating point in range 0.0 - 1.0
    face_part = (face_img * (1 / 255.0)) * (background_mask * (1 / 255.0))
    overlay_part = (overlay_img * (1 / 255.0)) * (overlay_mask * (1 / 255.0))

    # And finally just add them together, and rescale it back to an 8bit integer image
    return np.uint8(cv2.addWeighted(face_part, 255.0, overlay_part, 255.0, 0.0))
208/10:
for iml in instance_toimage:
    i=0
    while i<=3:
        i+=1
        tmpl = iml.copy()
        print('hello')
        cnt=0
        for j in range(0,10):
            paster = True
            idx = random.randint(0,len(cell_img)-1)
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = instances.transpose(1,2,0)
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                cv2.imwrite('writer.jpg',dispim)
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster==True:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[:,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                grao = cv2.cvtColor(back.transpose(1,2,0), cv2.COLOR_BGR2GRAY)
                grao = (grao>0)*255
                grao = np.array([grao])
                #print(cell_paste.shape,k_h)
                #cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                #print(type(tmpl[0:3]),type(back))
                tmpl[0:3] = blend_non_transparent(tmpl[0:3].transpose(1,2,0),back.transpose(1,2,0)).transpose(2,0,1)
                cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                print(tmpl.shape,grao.shape)
                tmpl = np.append(tmpl,grao,0)
                print(tmpl.shape,grao.shape)
209/1:
%matplotlib inline
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/AFO')
sys.path.insert(1, '/home/prakharug/AFO/pycoco')
from pycoco.engine import train_one_epoch, evaluate
209/2:
instance_toimage = []

# for im_name in os.listdir("../dataset1/images"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset1/images/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset1/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset1/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset1/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset1/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset1/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset1/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
209/3:
cell_img = []

for mksl in instance_toimage:
    #print(len(mksl))
    for mksin in range(3,len(mksl)):
        x,y,w,h = cv2.boundingRect(mksl[mksin])
        stck1 = mksl[0]*(mksl[mksin]/255)
        stck2 = mksl[1]*(mksl[mksin]/255)
        stck3 = mksl[2]*(mksl[mksin]/255)
        print(stck1.shape)
        new_img = np.array([stck1,stck2,stck3],np.uint8)[:,y:y+h,x:x+w]
        print(new_img.shape)
        cell_img.append(new_img)
        cv2.imwrite('jsr.png',new_img.transpose(1,2,0))
        #break
        #cv2.waitkey(0)
        #cv2.destroyAllindows
209/4:
def blend_non_transparent(face_img, overlay_img):
    # Let's find a mask covering all the non-black (foreground) pixels
    # NB: We need to do this on grayscale version of the image
    #print("overlay img ",overlay_img.shape)
    gray_overlay = cv2.cvtColor(overlay_img, cv2.COLOR_BGR2GRAY)
    overlay_mask = cv2.threshold(gray_overlay, 1, 255, cv2.THRESH_BINARY)[1]

    # Let's shrink and blur it a little to make the transitions smoother...
    #overlay_mask = cv2.erode(overlay_mask, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3)))
    #overlay_mask = cv2.blur(overlay_mask, (3, 3))

    # And the inverse mask, that covers all the black (background) pixels
    background_mask = (255) - overlay_mask

    # Turn the masks into three channel, so we can use them as weights
    overlay_mask = cv2.cvtColor(overlay_mask, cv2.COLOR_GRAY2BGR)
    background_mask = cv2.cvtColor(background_mask, cv2.COLOR_GRAY2BGR)

    # Create a masked out face image, and masked out overlay
    # We convert the images to floating point in range 0.0 - 1.0
    face_part = (face_img * (1 / 255.0)) * (background_mask * (1 / 255.0))
    overlay_part = (overlay_img * (1 / 255.0)) * (overlay_mask * (1 / 255.0))

    # And finally just add them together, and rescale it back to an 8bit integer image
    return np.uint8(cv2.addWeighted(face_part, 255.0, overlay_part, 255.0, 0.0))
209/5:
for iml in instance_toimage:
    i=0
    while i<=3:
        i+=1
        tmpl = iml.copy()
        print('hello')
        cnt=0
        for j in range(0,10):
            paster = True
            idx = random.randint(0,len(cell_img)-1)
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = instances.transpose(1,2,0)
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                cv2.imwrite('writer.jpg',dispim)
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster==True:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[:,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                grao = cv2.cvtColor(back.transpose(1,2,0), cv2.COLOR_BGR2GRAY)
                grao = (grao>0)*255
                grao = np.array([grao])
                #print(cell_paste.shape,k_h)
                #cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                #print(type(tmpl[0:3]),type(back))
                tmpl[0:3] = blend_non_transparent(tmpl[0:3].transpose(1,2,0),back.transpose(1,2,0)).transpose(2,0,1)
                cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                print(tmpl.shape,grao.shape)
                tmpl = np.append(tmpl,grao,0)
                print(tmpl.shape,grao.shape)
209/6:
lob = 1000
for iml in instance_toimage:
    cv2.imwrite("./dataset2/images/"+str(lob)+'.png',iml[0:3].transpose(1,2,0))
    for k in range(0,iml.shape[0]-3):
        cv2.imwrite("./dataset2/ground_truth/"+str(lob)+'_'+str(k+1)+'.png',iml[k+3,:,:].transpose(1,2,0))
    i=0
    while i<=3:
        i+=1
        tmpl = iml.copy()
        print('hello')
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img)-1)
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = instances.transpose(1,2,0)
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                cv2.imwrite('writer.jpg',dispim)
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster==True:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[:,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                grao = cv2.cvtColor(back.transpose(1,2,0), cv2.COLOR_BGR2GRAY)
                grao = (grao>0)*255
                grao = np.array([grao])
                #print(cell_paste.shape,k_h)
                #cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                #print(type(tmpl[0:3]),type(back))
                tmpl[0:3] = blend_non_transparent(tmpl[0:3].transpose(1,2,0),back.transpose(1,2,0)).transpose(2,0,1)
                cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                #print(tmpl.shape,grao.shape)
                tmpl = np.append(tmpl,grao,0)
                #print(tmpl.shape,grao.shape)
                cnt+=1
        if cnt>=3:
            cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
209/7:
lob = 1000
for iml in instance_toimage:
    cv2.imwrite("./dataset2/images/"+str(lob)+'.png',iml[0:3].transpose(1,2,0))
    for k in range(0,iml.shape[0]-3):
        cv2.imwrite("./dataset2/ground_truth/"+str(lob)+'_'+str(k+1)+'.png',iml[k+3].transpose(1,2,0))
    i=0
    while i<=3:
        i+=1
        tmpl = iml.copy()
        print('hello')
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img)-1)
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = instances.transpose(1,2,0)
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                cv2.imwrite('writer.jpg',dispim)
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster==True:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[:,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                grao = cv2.cvtColor(back.transpose(1,2,0), cv2.COLOR_BGR2GRAY)
                grao = (grao>0)*255
                grao = np.array([grao])
                #print(cell_paste.shape,k_h)
                #cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                #print(type(tmpl[0:3]),type(back))
                tmpl[0:3] = blend_non_transparent(tmpl[0:3].transpose(1,2,0),back.transpose(1,2,0)).transpose(2,0,1)
                cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                #print(tmpl.shape,grao.shape)
                tmpl = np.append(tmpl,grao,0)
                #print(tmpl.shape,grao.shape)
                cnt+=1
        if cnt>=3:
            cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
209/8:
lob = 1000
for iml in instance_toimage:
    cv2.imwrite("./dataset2/images/"+str(lob)+'.png',iml[0:3].transpose(1,2,0))
    for k in range(0,iml.shape[0]-3):
        cv2.imwrite("./dataset2/ground_truth/"+str(lob)+'_'+str(k+1)+'.png',np.array([iml[k+3]]).transpose(1,2,0))
    i=0
    while i<=3:
        i+=1
        tmpl = iml.copy()
        print('hello')
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img)-1)
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = instances.transpose(1,2,0)
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                cv2.imwrite('writer.jpg',dispim)
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster==True:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[:,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                grao = cv2.cvtColor(back.transpose(1,2,0), cv2.COLOR_BGR2GRAY)
                grao = (grao>0)*255
                grao = np.array([grao])
                #print(cell_paste.shape,k_h)
                #cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                #print(type(tmpl[0:3]),type(back))
                tmpl[0:3] = blend_non_transparent(tmpl[0:3].transpose(1,2,0),back.transpose(1,2,0)).transpose(2,0,1)
                cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                #print(tmpl.shape,grao.shape)
                tmpl = np.append(tmpl,grao,0)
                #print(tmpl.shape,grao.shape)
                cnt+=1
        if cnt>=3:
            cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
209/9:
lob = 1000
for iml in instance_toimage:
    cv2.imwrite("./dataset2/images/"+str(lob)+'.png',iml[0:3].transpose(1,2,0))
    for k in range(0,iml.shape[0]-3):
        cv2.imwrite("./dataset2/ground_truth/"+str(lob)+'_'+str(k+1)+'.png',np.array([iml[k+3]]).transpose(1,2,0))
    lob+=1
    i=0
    while i<=3:
        i+=1
        tmpl = iml.copy()
        print('hello')
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img)-1)
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = instances.transpose(1,2,0)
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                cv2.imwrite('writer.jpg',dispim)
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster==True:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[:,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                grao = cv2.cvtColor(back.transpose(1,2,0), cv2.COLOR_BGR2GRAY)
                grao = (grao>0)*255
                grao = np.array([grao])
                #print(cell_paste.shape,k_h)
                #cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                #print(type(tmpl[0:3]),type(back))
                tmpl[0:3] = blend_non_transparent(tmpl[0:3].transpose(1,2,0),back.transpose(1,2,0)).transpose(2,0,1)
                cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                #print(tmpl.shape,grao.shape)
                tmpl = np.append(tmpl,grao,0)
                #print(tmpl.shape,grao.shape)
                cnt+=1
        if cnt>=3:
            cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
209/10:
lob = 1000
for iml in instance_toimage:
    cv2.imwrite("./dataset2/images/"+str(lob)+'.png',iml[0:3].transpose(1,2,0))
    for k in range(0,iml.shape[0]-3):
        cv2.imwrite("./dataset2/ground_truths/"+str(lob)+'_'+str(k+1)+'.png',np.array([iml[k+3]]).transpose(1,2,0))
    lob+=1
    i=0
    while i<=3:
        i+=1
        tmpl = iml.copy()
        print('hello')
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img)-1)
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = instances.transpose(1,2,0)
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                cv2.imwrite('writer.jpg',dispim)
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster==True:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[:,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                grao = cv2.cvtColor(back.transpose(1,2,0), cv2.COLOR_BGR2GRAY)
                grao = (grao>0)*255
                grao = np.array([grao])
                #print(cell_paste.shape,k_h)
                #cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                #print(type(tmpl[0:3]),type(back))
                tmpl[0:3] = blend_non_transparent(tmpl[0:3].transpose(1,2,0),back.transpose(1,2,0)).transpose(2,0,1)
                cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                #print(tmpl.shape,grao.shape)
                tmpl = np.append(tmpl,grao,0)
                #print(tmpl.shape,grao.shape)
                cnt+=1
        if cnt>=3:
            cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
209/11:
lob = 1000
for iml in instance_toimage:
    cv2.imwrite("./dataset2/images/"+str(lob)+'.png',iml[0:3].transpose(1,2,0))
    for k in range(0,iml.shape[0]-3):
        cv2.imwrite("./dataset2/ground_truths/"+str(lob)+'_'+str(k+1)+'.png',np.array([iml[k+3]]).transpose(1,2,0))
    lob+=1
    i=0
    while i<=3:
        i+=1
        tmpl = iml.copy()
        print('hello')
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img)-1)
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = instances.transpose(1,2,0)
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                cv2.imwrite('writer.jpg',dispim)
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster==True:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[:,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                grao = cv2.cvtColor(back.transpose(1,2,0), cv2.COLOR_BGR2GRAY)
                grao = (grao>0)*255
                grao = np.array([grao])
                #print(cell_paste.shape,k_h)
                #cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                #print(type(tmpl[0:3]),type(back))
                tmpl[0:3] = blend_non_transparent(tmpl[0:3].transpose(1,2,0),back.transpose(1,2,0)).transpose(2,0,1)
                cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                #print(tmpl.shape,grao.shape)
                tmpl = np.append(tmpl,grao,0)
                #print(tmpl.shape,grao.shape)
                cnt+=1
        if cnt>=3:
            cv2.imwrite("./dataset2/images/"+str(lob)+'.png',tmpl[0:3].transpose(1,2,0))
            for k in range(0,tmpl.shape[0]-3):
                cv2.imwrite("./dataset2/ground_truths/"+str(lob)+'_'+str(k+1)+'.png',np.array([tmpl[k+3]]).transpose(1,2,0))
            lob+=1
        else:
            i-=1
209/12:
lob = 1000
for iml in instance_toimage:
    cv2.imwrite("./dataset2/images/"+str(lob)+'.png',iml[0:3].transpose(1,2,0))
    for k in range(0,iml.shape[0]-3):
        cv2.imwrite("./dataset2/ground_truths/"+str(lob)+'_'+str(k+1)+'.png',np.array([iml[k+3]]).transpose(1,2,0))
    lob+=1
    i=0
    while i<=3:
        i+=1
        tmpl = iml.copy()
        print('hello')
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img)-1)
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = instances.transpose(1,2,0)
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                cv2.imwrite('writer.jpg',dispim)
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster==True:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[:,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                grao = cv2.cvtColor(back.transpose(1,2,0), cv2.COLOR_BGR2GRAY)
                grao = (grao>0)*255
                grao = np.array([grao])
                #print(cell_paste.shape,k_h)
                #cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                #print(type(tmpl[0:3]),type(back))
                tmpl[0:3] = blend_non_transparent(tmpl[0:3].transpose(1,2,0),back.transpose(1,2,0)).transpose(2,0,1)
                cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                #print(tmpl.shape,grao.shape)
                tmpl = np.append(tmpl,grao,0)
                #print(tmpl.shape,grao.shape)
                cnt+=1
        if cnt>=3:
            cv2.imwrite("./dataset2/images/"+str(lob)+'.png',tmpl[0:3].transpose(1,2,0))
            for k in range(0,tmpl.shape[0]-3):
                cv2.imwrite("./dataset2/ground_truths/"+str(lob)+'_'+str(k+1)+'.png',np.array([tmpl[k+3]]).transpose(1,2,0))
            lob+=1
        else:
            i-=1
209/13:
lob = 1000
for iml in instance_toimage:
    cv2.imwrite("./dataset2/images/"+str(lob)+'.png',iml[0:3].transpose(1,2,0))
    for k in range(0,iml.shape[0]-3):
        cv2.imwrite("./dataset2/ground_truths/"+str(lob)+'_'+str(k+1)+'.png',np.array([iml[k+3]]).transpose(1,2,0))
    lob+=1
    i=0
    while i<=3:
        i+=1
        tmpl = iml.copy()
        print('hello')
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img)-1)
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = instances.transpose(1,2,0)
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                cv2.imwrite('writer.jpg',dispim)
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster==True:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[:,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                grao = cv2.cvtColor(back.transpose(1,2,0), cv2.COLOR_BGR2GRAY)
                grao = (grao>0)*255
                grao = np.array([grao])
                #print(cell_paste.shape,k_h)
                #cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                #print(type(tmpl[0:3]),type(back))
                tmpl[0:3] = blend_non_transparent(tmpl[0:3].transpose(1,2,0),back.transpose(1,2,0)).transpose(2,0,1)
                cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                cv2.imwrite('comp.pmg',cell_paste)
                #print(tmpl.shape,grao.shape)
                tmpl = np.append(tmpl,grao,0)
                #print(tmpl.shape,grao.shape)
                cnt+=1
        if cnt>=3:
            cv2.imwrite("./dataset2/images/"+str(lob)+'.png',tmpl[0:3].transpose(1,2,0))
            for k in range(0,tmpl.shape[0]-3):
                cv2.imwrite("./dataset2/ground_truths/"+str(lob)+'_'+str(k+1)+'.png',np.array([tmpl[k+3]]).transpose(1,2,0))
            lob+=1
        else:
            i-=1
209/14:
lob = 1000
for iml in instance_toimage:
    cv2.imwrite("./dataset2/images/"+str(lob)+'.png',iml[0:3].transpose(1,2,0))
    for k in range(0,iml.shape[0]-3):
        cv2.imwrite("./dataset2/ground_truths/"+str(lob)+'_'+str(k+1)+'.png',np.array([iml[k+3]]).transpose(1,2,0))
    lob+=1
    i=0
    while i<=3:
        i+=1
        tmpl = iml.copy()
        print('hello')
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img)-1)
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = instances.transpose(1,2,0)
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                cv2.imwrite('writer.jpg',dispim)
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster==True:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[:,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                grao = cv2.cvtColor(back.transpose(1,2,0), cv2.COLOR_BGR2GRAY)
                grao = (grao>0)*255
                grao = np.array([grao])
                #print(cell_paste.shape,k_h)
                #cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                #print(type(tmpl[0:3]),type(back))
                tmpl[0:3] = blend_non_transparent(tmpl[0:3].transpose(1,2,0),back.transpose(1,2,0)).transpose(2,0,1)
                cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                cv2.imwrite('comp.pmg',cell_paste.transpose(1,2,0))
                #print(tmpl.shape,grao.shape)
                tmpl = np.append(tmpl,grao,0)
                #print(tmpl.shape,grao.shape)
                cnt+=1
        if cnt>=3:
            cv2.imwrite("./dataset2/images/"+str(lob)+'.png',tmpl[0:3].transpose(1,2,0))
            for k in range(0,tmpl.shape[0]-3):
                cv2.imwrite("./dataset2/ground_truths/"+str(lob)+'_'+str(k+1)+'.png',np.array([tmpl[k+3]]).transpose(1,2,0))
            lob+=1
        else:
            i-=1
209/15:
lob = 1000
for iml in instance_toimage:
    cv2.imwrite("./dataset2/images/"+str(lob)+'.png',iml[0:3].transpose(1,2,0))
    for k in range(0,iml.shape[0]-3):
        cv2.imwrite("./dataset2/ground_truths/"+str(lob)+'_'+str(k+1)+'.png',np.array([iml[k+3]]).transpose(1,2,0))
    lob+=1
    i=0
    while i<=3:
        i+=1
        tmpl = iml.copy()
        print('hello')
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img)-1)
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = instances.transpose(1,2,0)
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                cv2.imwrite('writer.jpg',dispim)
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster==True:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[:,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                grao = cv2.cvtColor(back.transpose(1,2,0), cv2.COLOR_BGR2GRAY)
                grao = (grao>0)*255
                grao = np.array([grao])
                #print(cell_paste.shape,k_h)
                #cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                #print(type(tmpl[0:3]),type(back))
                tmpl[0:3] = blend_non_transparent(tmpl[0:3].transpose(1,2,0),back.transpose(1,2,0)).transpose(2,0,1)
                cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                cv2.imwrite('comp.png',cell_paste.transpose(1,2,0))
                #print(tmpl.shape,grao.shape)
                tmpl = np.append(tmpl,grao,0)
                #print(tmpl.shape,grao.shape)
                cnt+=1
        if cnt>=3:
            cv2.imwrite("./dataset2/images/"+str(lob)+'.png',tmpl[0:3].transpose(1,2,0))
            for k in range(0,tmpl.shape[0]-3):
                cv2.imwrite("./dataset2/ground_truths/"+str(lob)+'_'+str(k+1)+'.png',np.array([tmpl[k+3]]).transpose(1,2,0))
            lob+=1
        else:
            i-=1
209/16:
lob = 1000
for iml in instance_toimage:
    cv2.imwrite("./dataset2/images/"+str(lob)+'.png',iml[0:3].transpose(1,2,0))
    for k in range(0,iml.shape[0]-3):
        cv2.imwrite("./dataset2/ground_truths/"+str(lob)+'_'+str(k+1)+'.png',np.array([iml[k+3]]).transpose(1,2,0))
    lob+=1
    i=0
    while i<=3:
        i+=1
        tmpl = iml.copy()
        print('hello')
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img)-1)
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = instances.transpose(1,2,0)
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                cv2.imwrite('writer.jpg',dispim)
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster==True:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[:,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                grao = cv2.cvtColor(back.transpose(1,2,0), cv2.COLOR_BGR2GRAY)
                grao = (grao>0)*255
                grao = np.array([grao])
                #print(cell_paste.shape,k_h)
                #cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                #print(type(tmpl[0:3]),type(back))
                tmpl[0:3] = blend_non_transparent(tmpl[0:3].transpose(1,2,0),back.transpose(1,2,0)).transpose(2,0,1)
                cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                cv2.imwrite('comp.png',cell_paste.transpose(1,2,0))
                #print(tmpl.shape,grao.shape)
                tmpl = np.append(tmpl,grao,0)
                #print(tmpl.shape,grao.shape)
                cnt+=1
        if cnt>=3:
            cv2.imwrite("./dataset2/images/"+str(lob)+'.png',tmpl[0:3].transpose(1,2,0))
            for k in range(0,tmpl.shape[0]-3):
                cv2.imwrite("./dataset2/ground_truths/"+str(lob)+'_'+str(k+1)+'.png',np.array([tmpl[k+3]]).transpose(1,2,0))
            lob+=1
        else:
            i-=1
210/1:
%matplotlib inline
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/AFO')
sys.path.insert(1, '/home/prakharug/AFO/pycoco')
from pycoco.engine import train_one_epoch, evaluate
210/2:
instance_toimage = []

# for im_name in os.listdir("../dataset1/images"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("../dataset1/images/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("../dataset1/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("../dataset1/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset1/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset1/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset1/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset1/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
210/3:
cell_img = []

for mksl in instance_toimage:
    #print(len(mksl))
    for mksin in range(3,len(mksl)):
        x,y,w,h = cv2.boundingRect(mksl[mksin])
        if x == 0 or y ==0 or x+w==mksl.shape[2]-1 or y+h==mksl.shape[1]-1:
            continue
        stck1 = mksl[0]*(mksl[mksin]/255)
        stck2 = mksl[1]*(mksl[mksin]/255)
        stck3 = mksl[2]*(mksl[mksin]/255)
        print(stck1.shape)
        new_img = np.array([stck1,stck2,stck3],np.uint8)[:,y:y+h,x:x+w]
        print(new_img.shape)
        cell_img.append(new_img)
        cv2.imwrite('jsr.png',new_img.transpose(1,2,0))
        #break
        #cv2.waitkey(0)
        #cv2.destroyAllindows
210/4:
def blend_non_transparent(face_img, overlay_img):
    # Let's find a mask covering all the non-black (foreground) pixels
    # NB: We need to do this on grayscale version of the image
    #print("overlay img ",overlay_img.shape)
    gray_overlay = cv2.cvtColor(overlay_img, cv2.COLOR_BGR2GRAY)
    overlay_mask = cv2.threshold(gray_overlay, 1, 255, cv2.THRESH_BINARY)[1]

    # Let's shrink and blur it a little to make the transitions smoother...
    #overlay_mask = cv2.erode(overlay_mask, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3)))
    #overlay_mask = cv2.blur(overlay_mask, (3, 3))

    # And the inverse mask, that covers all the black (background) pixels
    background_mask = (255) - overlay_mask

    # Turn the masks into three channel, so we can use them as weights
    overlay_mask = cv2.cvtColor(overlay_mask, cv2.COLOR_GRAY2BGR)
    background_mask = cv2.cvtColor(background_mask, cv2.COLOR_GRAY2BGR)

    # Create a masked out face image, and masked out overlay
    # We convert the images to floating point in range 0.0 - 1.0
    face_part = (face_img * (1 / 255.0)) * (background_mask * (1 / 255.0))
    overlay_part = (overlay_img * (1 / 255.0)) * (overlay_mask * (1 / 255.0))

    # And finally just add them together, and rescale it back to an 8bit integer image
    return np.uint8(cv2.addWeighted(face_part, 255.0, overlay_part, 255.0, 0.0))
210/5:
lob = 1000
for iml in instance_toimage:
    cv2.imwrite("./dataset2/images/"+str(lob)+'.png',iml[0:3].transpose(1,2,0))
    for k in range(0,iml.shape[0]-3):
        cv2.imwrite("./dataset2/ground_truths/"+str(lob)+'_'+str(k+1)+'.png',np.array([iml[k+3]]).transpose(1,2,0))
    lob+=1
    i=0
    while i<=3:
        i+=1
        tmpl = iml.copy()
        print('hello')
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img)-1)
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = instances.transpose(1,2,0)
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                cv2.imwrite('writer.jpg',dispim)
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster==True:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[:,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                grao = cv2.cvtColor(back.transpose(1,2,0), cv2.COLOR_BGR2GRAY)
                grao = (grao>0)*255
                grao = np.array([grao])
                #print(cell_paste.shape,k_h)
                #cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                #print(type(tmpl[0:3]),type(back))
                tmpl[0:3] = blend_non_transparent(tmpl[0:3].transpose(1,2,0),back.transpose(1,2,0)).transpose(2,0,1)
                cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                cv2.imwrite('comp.png',cell_paste.transpose(1,2,0))
                #print(tmpl.shape,grao.shape)
                tmpl = np.append(tmpl,grao,0)
                #print(tmpl.shape,grao.shape)
                cnt+=1
        if cnt>=3:
            cv2.imwrite("./dataset2/images/"+str(lob)+'.png',tmpl[0:3].transpose(1,2,0))
            for k in range(0,tmpl.shape[0]-3):
                cv2.imwrite("./dataset2/ground_truths/"+str(lob)+'_'+str(k+1)+'.png',np.array([tmpl[k+3]]).transpose(1,2,0))
            lob+=1
        else:
            i-=1
210/6:
cell_img = []

for mksl in instance_toimage:
    #print(len(mksl))
    for mksin in range(3,len(mksl)):
        x,y,w,h = cv2.boundingRect(mksl[mksin])
        if x == 0 or y ==0 or x+w==mksl.shape[2]-1 or y+h==mksl.shape[1]-1:
            continue
        stck1 = mksl[0]*(mksl[mksin]/255)
        stck2 = mksl[1]*(mksl[mksin]/255)
        stck3 = mksl[2]*(mksl[mksin]/255)
        print(stck1.shape)
        new_img = np.array([stck1,stck2,stck3],np.uint8)[:,y:y+h,x:x+w]
        print(new_img.shape)
        cell_img.append(new_img)
        cv2.imwrite('jsr0.png',new_img.transpose(1,2,0))
        #break
        #cv2.waitkey(0)
        #cv2.destroyAllindows
210/7:
cell_img = []

for mksl in instance_toimage:
    #print(len(mksl))
    for mksin in range(3,len(mksl)):
        x,y,w,h = cv2.boundingRect(mksl[mksin])
        if x*y*w*h==0 or x+w==mksl.shape[2]-1 or y+h==mksl.shape[1]-1:
            print("bruh")
            continue
        else:
            stck1 = mksl[0]*(mksl[mksin]/255)
            stck2 = mksl[1]*(mksl[mksin]/255)
            stck3 = mksl[2]*(mksl[mksin]/255)
            print(stck1.shape)
            new_img = np.array([stck1,stck2,stck3],np.uint8)[:,y:y+h,x:x+w]
            print(new_img.shape)
            cell_img.append(new_img)
            cv2.imwrite('jsr0.png',new_img.transpose(1,2,0))
            #break
            #cv2.waitkey(0)
            #cv2.destroyAllindows
210/8:
cell_img = []
j = 1
for mksl in instance_toimage:
    #print(len(mksl))
    for mksin in range(3,len(mksl)):
        x,y,w,h = cv2.boundingRect(mksl[mksin])
        if x*y*w*h==0 or x+w==mksl.shape[2]-1 or y+h==mksl.shape[1]-1:
            print("bruh")
            continue
        else:
            stck1 = mksl[0]*(mksl[mksin]/255)
            stck2 = mksl[1]*(mksl[mksin]/255)
            stck3 = mksl[2]*(mksl[mksin]/255)
            print(stck1.shape)
            new_img = np.array([stck1,stck2,stck3],np.uint8)[:,y:y+h,x:x+w]
            print(new_img.shape)
            cell_img.append(new_img)
            cv2.imwrite('jsr'+str(j)+'.png',new_img.transpose(1,2,0))
            j+=1
            #break
            #cv2.waitkey(0)
            #cv2.destroyAllindows
210/9:
cell_img = []
j = 1
for mksl in instance_toimage:
    #print(len(mksl))
    for mksin in range(3,len(mksl)):
        x,y,w,h = cv2.boundingRect(mksl[mksin])
        if x*y*w*h==0 or x+w==mksl.shape[1]-1 or y+h==mksl.shape[2]-1:
            print("bruh")
            continue
        else:
            stck1 = mksl[0]*(mksl[mksin]/255)
            stck2 = mksl[1]*(mksl[mksin]/255)
            stck3 = mksl[2]*(mksl[mksin]/255)
            print(stck1.shape)
            new_img = np.array([stck1,stck2,stck3],np.uint8)[:,y:y+h,x:x+w]
            print(new_img.shape)
            cell_img.append(new_img)
            cv2.imwrite('jsr'+str(j)+'.png',new_img.transpose(1,2,0))
            j+=1
            #break
            #cv2.waitkey(0)
            #cv2.destroyAllindows
210/10:
cell_img = []
j = 1
for mksl in instance_toimage:
    #print(len(mksl))
    for mksin in range(3,len(mksl)):
        x,y,w,h = cv2.boundingRect(mksl[mksin])
        if x*y*w*h==0 or abs(x+w-mksl.shape[1])<=50 or abs(y+h-mksl.shape[2])<=50:
            print("bruh")
            continue
        else:
            stck1 = mksl[0]*(mksl[mksin]/255)
            stck2 = mksl[1]*(mksl[mksin]/255)
            stck3 = mksl[2]*(mksl[mksin]/255)
            print(stck1.shape)
            new_img = np.array([stck1,stck2,stck3],np.uint8)[:,y:y+h,x:x+w]
            print(new_img.shape)
            cell_img.append(new_img)
            cv2.imwrite('jsr'+str(j)+'.png',new_img.transpose(1,2,0))
            j+=1
            #break
            #cv2.waitkey(0)
            #cv2.destroyAllindows
210/11:
cell_img = []
j = 1
for mksl in instance_toimage:
    #print(len(mksl))
    for mksin in range(3,len(mksl)):
        x,y,w,h = cv2.boundingRect(mksl[mksin])
        if x*y*w*h==0 or abs(x+w-mksl.shape[2])<=50 or abs(y+h-mksl.shape[1])<=50:
            print("bruh")
            continue
        else:
            stck1 = mksl[0]*(mksl[mksin]/255)
            stck2 = mksl[1]*(mksl[mksin]/255)
            stck3 = mksl[2]*(mksl[mksin]/255)
            print(stck1.shape)
            new_img = np.array([stck1,stck2,stck3],np.uint8)[:,x:x+w,y:y+h]
            print(new_img.shape)
            cell_img.append(new_img)
            cv2.imwrite('jsr'+str(j)+'.png',new_img.transpose(1,2,0))
            j+=1
            #break
            #cv2.waitkey(0)
            #cv2.destroyAllindows
210/12:
cell_img = []
j = 1
for mksl in instance_toimage:
    #print(len(mksl))
    for mksin in range(3,len(mksl)):
        x,y,w,h = cv2.boundingRect(mksl[mksin])
        if x*y*w*h==0 or abs(x+w-mksl.shape[2])<=50 or abs(y+h-mksl.shape[1])<=50:
            print("bruh")
            continue
        else:
            stck1 = mksl[0]*(mksl[mksin]/255)
            stck2 = mksl[1]*(mksl[mksin]/255)
            stck3 = mksl[2]*(mksl[mksin]/255)
            print(stck1.shape)
            new_img = np.array([stck1,stck2,stck3],np.uint8)[:,y:y+h,x:x+w]
            print(new_img.shape)
            cell_img.append(new_img)
            cv2.imwrite('jsr'+str(j)+'.png',new_img.transpose(1,2,0))
            j+=1
            #break
            #cv2.waitkey(0)
            #cv2.destroyAllindows
211/1:
%matplotlib inline
import random
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
import numpy as np
import torch.utils.data
import cv2
import torchvision.models.segmentation
import torch
import os
import patchify
from sklearn.datasets import load_sample_image
from sklearn.feature_extraction import image as skimg
import imgaug as ia
import imgaug.augmenters as iaa
from torchvision.models import resnet50, ResNet50_Weights
from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights
import torch.optim
from torch import nn
from torch.utils.data import Dataset, DataLoader
import sys
sys.path.insert(1, '/home/prakharug/AFO')
sys.path.insert(1, '/home/prakharug/AFO/pycoco')
from pycoco.engine import train_one_epoch, evaluate
211/2:
instance_toimage = []

for im_name in os.listdir("../dataset1/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset1/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset1/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset1/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset1/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset1/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset1/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset1/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
211/3:
instance_toimage = []

for im_name in os.listdir("../dataset1/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("../dataset1/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("../dataset1/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("../dataset1/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset1/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset1/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset1/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset1/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
211/4:
instance_toimage = []

for im_name in os.listdir("./dataset1/images"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset1/images/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset1/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset1/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset1/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset1/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset1/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset1/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
211/5:

j = 1
for mksl in instance_toimage:
    #print(len(mksl))
    for mksin in range(3,len(mksl)):
        x,y,w,h = cv2.boundingRect(mksl[mksin])
        if x*y*w*h==0 or abs(x+w-mksl.shape[2])<=50 or abs(y+h-mksl.shape[1])<=50:
            print("bruh")
            continue
        else:
            stck1 = mksl[0]*(mksl[mksin]/255)
            stck2 = mksl[1]*(mksl[mksin]/255)
            stck3 = mksl[2]*(mksl[mksin]/255)
            print(stck1.shape)
            new_img = np.array([stck1,stck2,stck3],np.uint8)[:,y:y+h,x:x+w]
            print(new_img.shape)
            #cell_img.append(new_img)
            cv2.imwrite('./cell_iso1/jsr'+str(j)+'.png',new_img.transpose(1,2,0))
            j+=1
            #break
            #cv2.waitkey(0)
            #cv2.destroyAllindows
211/6:
cell_img = []
j = 1
for mksl in instance_toimage:
    #print(len(mksl))
    for mksin in range(3,len(mksl)):
        x,y,w,h = cv2.boundingRect(mksl[mksin])
        if x*y*w*h==0 or abs(x+w-mksl.shape[2])<=50 or abs(y+h-mksl.shape[1])<=50:
            print("bruh")
            continue
        else:
            stck1 = mksl[0]*(mksl[mksin]/255)
            stck2 = mksl[1]*(mksl[mksin]/255)
            stck3 = mksl[2]*(mksl[mksin]/255)
            print(stck1.shape)
            new_img = np.array([stck1,stck2,stck3],np.uint8)[:,y:y+h,x:x+w]
            print(new_img.shape)
            cell_img.append(new_img)
            #cv2.imwrite('./cell_iso1/jsr'+str(j)+'.png',new_img.transpose(1,2,0))
            j+=1
            #break
            #cv2.waitkey(0)
            #cv2.destroyAllindows
211/7:
def blend_non_transparent(face_img, overlay_img):
    # Let's find a mask covering all the non-black (foreground) pixels
    # NB: We need to do this on grayscale version of the image
    #print("overlay img ",overlay_img.shape)
    gray_overlay = cv2.cvtColor(overlay_img, cv2.COLOR_BGR2GRAY)
    overlay_mask = cv2.threshold(gray_overlay, 1, 255, cv2.THRESH_BINARY)[1]

    # Let's shrink and blur it a little to make the transitions smoother...
    #overlay_mask = cv2.erode(overlay_mask, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3)))
    #overlay_mask = cv2.blur(overlay_mask, (3, 3))

    # And the inverse mask, that covers all the black (background) pixels
    background_mask = (255) - overlay_mask

    # Turn the masks into three channel, so we can use them as weights
    overlay_mask = cv2.cvtColor(overlay_mask, cv2.COLOR_GRAY2BGR)
    background_mask = cv2.cvtColor(background_mask, cv2.COLOR_GRAY2BGR)

    # Create a masked out face image, and masked out overlay
    # We convert the images to floating point in range 0.0 - 1.0
    face_part = (face_img * (1 / 255.0)) * (background_mask * (1 / 255.0))
    overlay_part = (overlay_img * (1 / 255.0)) * (overlay_mask * (1 / 255.0))

    # And finally just add them together, and rescale it back to an 8bit integer image
    return np.uint8(cv2.addWeighted(face_part, 255.0, overlay_part, 255.0, 0.0))
211/8:
lob = 1000
for iml in instance_toimage:
    cv2.imwrite("./dataset2/images/"+str(lob)+'.png',iml[0:3].transpose(1,2,0))
    for k in range(0,iml.shape[0]-3):
        cv2.imwrite("./dataset2/ground_truths/"+str(lob)+'_'+str(k+1)+'.png',np.array([iml[k+3]]).transpose(1,2,0))
    lob+=1
    i=0
    while i<=3:
        i+=1
        tmpl = iml.copy()
        print('hello')
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img)-1)
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = instances.transpose(1,2,0)
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                cv2.imwrite('writer.jpg',dispim)
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster==True:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[:,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                grao = cv2.cvtColor(back.transpose(1,2,0), cv2.COLOR_BGR2GRAY)
                grao = (grao>0)*255
                grao = np.array([grao])
                #print(cell_paste.shape,k_h)
                #cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                #print(type(tmpl[0:3]),type(back))
                tmpl[0:3] = blend_non_transparent(tmpl[0:3].transpose(1,2,0),back.transpose(1,2,0)).transpose(2,0,1)
                cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                cv2.imwrite('comp.png',cell_paste.transpose(1,2,0))
                #print(tmpl.shape,grao.shape)
                tmpl = np.append(tmpl,grao,0)
                #print(tmpl.shape,grao.shape)
                cnt+=1
        if cnt>=3:
            cv2.imwrite("./dataset2/images/"+str(lob)+'.png',tmpl[0:3].transpose(1,2,0))
            for k in range(0,tmpl.shape[0]-3):
                cv2.imwrite("./dataset2/ground_truths/"+str(lob)+'_'+str(k+1)+'.png',np.array([tmpl[k+3]]).transpose(1,2,0))
            lob+=1
        else:
            i-=1
211/9:
instance_toimage = []

# for im_name in os.listdir("./dataset1/images"):
#     tmplist = []
#     lent = len(im_name[:-4])
#     print(im_name,im_name[:lent])
#     imgtmp = cv2.imread("./dataset1/images/"+im_name,1).transpose(2,0,1)
#     #print(imgtmp[0].shape)
#     tmplist.append(imgtmp[0])
#     tmplist.append(imgtmp[1])
#     tmplist.append(imgtmp[2])
#     for gt_name in os.listdir("./dataset1/ground_truths"):
#         #print(gt_name[10+lent],gt_name[10:10+lent])
#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
#             mask = np.array(cv2.imread("./dataset1/ground_truths/"+gt_name,0))
#             # mask = (mask > 0).astype(np.uint8) 
#             tmplist.append(mask)
#     tmplist = np.array(tmplist)
#     print(tmplist.shape)
#     instance_toimage.append(tmplist)

for im_name in os.listdir("./dataset1/test"):
    tmplist = []
    lent = len(im_name[:-4])
    print(im_name,im_name[:lent])
    imgtmp = cv2.imread("./dataset1/test/"+im_name,1).transpose(2,0,1)
    #print(imgtmp[0].shape)
    tmplist.append(imgtmp[0])
    tmplist.append(imgtmp[1])
    tmplist.append(imgtmp[2])
    for gt_name in os.listdir("./dataset1/ground_truths"):
        #print(gt_name[10+lent],gt_name[10:10+lent])
        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]=="_":
            mask = np.array(cv2.imread("./dataset1/ground_truths/"+gt_name,0))
            # mask = (mask > 0).astype(np.uint8) 
            tmplist.append(mask)
    tmplist = np.array(tmplist)
    print(tmplist.shape)
    instance_toimage.append(tmplist)
211/10:
cell_img = []
j = 1
for mksl in instance_toimage:
    #print(len(mksl))
    for mksin in range(3,len(mksl)):
        x,y,w,h = cv2.boundingRect(mksl[mksin])
        if x*y*w*h==0 or abs(x+w-mksl.shape[2])<=50 or abs(y+h-mksl.shape[1])<=50:
            print("bruh")
            continue
        else:
            stck1 = mksl[0]*(mksl[mksin]/255)
            stck2 = mksl[1]*(mksl[mksin]/255)
            stck3 = mksl[2]*(mksl[mksin]/255)
            print(stck1.shape)
            new_img = np.array([stck1,stck2,stck3],np.uint8)[:,y:y+h,x:x+w]
            print(new_img.shape)
            cell_img.append(new_img)
            #cv2.imwrite('./cell_iso1/jsr'+str(j)+'.png',new_img.transpose(1,2,0))
            j+=1
            #break
            #cv2.waitkey(0)
            #cv2.destroyAllindows
211/11:
def blend_non_transparent(face_img, overlay_img):
    # Let's find a mask covering all the non-black (foreground) pixels
    # NB: We need to do this on grayscale version of the image
    #print("overlay img ",overlay_img.shape)
    gray_overlay = cv2.cvtColor(overlay_img, cv2.COLOR_BGR2GRAY)
    overlay_mask = cv2.threshold(gray_overlay, 1, 255, cv2.THRESH_BINARY)[1]

    # Let's shrink and blur it a little to make the transitions smoother...
    #overlay_mask = cv2.erode(overlay_mask, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3)))
    #overlay_mask = cv2.blur(overlay_mask, (3, 3))

    # And the inverse mask, that covers all the black (background) pixels
    background_mask = (255) - overlay_mask

    # Turn the masks into three channel, so we can use them as weights
    overlay_mask = cv2.cvtColor(overlay_mask, cv2.COLOR_GRAY2BGR)
    background_mask = cv2.cvtColor(background_mask, cv2.COLOR_GRAY2BGR)

    # Create a masked out face image, and masked out overlay
    # We convert the images to floating point in range 0.0 - 1.0
    face_part = (face_img * (1 / 255.0)) * (background_mask * (1 / 255.0))
    overlay_part = (overlay_img * (1 / 255.0)) * (overlay_mask * (1 / 255.0))

    # And finally just add them together, and rescale it back to an 8bit integer image
    return np.uint8(cv2.addWeighted(face_part, 255.0, overlay_part, 255.0, 0.0))
211/12:
lob = 1000
for iml in instance_toimage:
    cv2.imwrite("./dataset2/images/"+str(lob)+'.png',iml[0:3].transpose(1,2,0))
    for k in range(0,iml.shape[0]-3):
        cv2.imwrite("./dataset2/ground_truths/"+str(lob)+'_'+str(k+1)+'.png',np.array([iml[k+3]]).transpose(1,2,0))
    lob+=1
    i=0
    while i<=3:
        i+=1
        tmpl = iml.copy()
        cnt=0
        for j in range(0,5):
            paster = True
            idx = random.randint(0,len(cell_img)-1)
            rtrnd=random.randint(0,4)
            seq = iaa.Sequential([
            iaa.WithHueAndSaturation([
            iaa.WithChannels(0, iaa.Add((-30, 10))),
            iaa.WithChannels(1, [
                iaa.Multiply((0.5, 1.5)),
                iaa.LinearContrast((0.75, 1.25))
                ])
            ])
            ,
            iaa.Rot90((rtrnd), keep_size=False)
            ])

            # print(cell_img[idx].shape)
            # test = cell_img[idx]
            # cell_paste = seq(images=test)
            cell_paste = cell_img[idx]
            k_w = cell_paste.shape[2]
            k_h = cell_paste.shape[1]
            o_w = random.randint(0,tmpl.shape[2]-k_w)
            o_h = random.randint(0,tmpl.shape[1]-k_h)
            patch_img = tmpl[0:3,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = tmpl[3:,o_h:o_h+k_h,o_w:o_w+k_w]
            instances = instances.transpose(1,2,0)
            for a in range(instances.shape[2]):
                dispim = instances[:,:,a]
                cv2.imwrite('writer.jpg',dispim)
                if np.all(dispim == 0):
                    continue
                else:
                    paster=False
                    break
            if paster==True:
                back = np.zeros([3,tmpl.shape[1],tmpl.shape[2]],np.uint8)
                back[:,o_h:o_h+k_h,o_w:o_w+k_w] = cell_paste
                grao = cv2.cvtColor(back.transpose(1,2,0), cv2.COLOR_BGR2GRAY)
                grao = (grao>0)*255
                grao = np.array([grao])
                #print(cell_paste.shape,k_h)
                #cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                #print(type(tmpl[0:3]),type(back))
                tmpl[0:3] = blend_non_transparent(tmpl[0:3].transpose(1,2,0),back.transpose(1,2,0)).transpose(2,0,1)
                cv2.imwrite('jsr.png',tmpl[0:3].transpose(1,2,0))
                cv2.imwrite('comp.png',cell_paste.transpose(1,2,0))
                #print(tmpl.shape,grao.shape)
                tmpl = np.append(tmpl,grao,0)
                #print(tmpl.shape,grao.shape)
                cnt+=1
        if cnt>=3:
            print('hello')
            cv2.imwrite("./dataset2/images/"+str(lob)+'.png',tmpl[0:3].transpose(1,2,0))
            for k in range(0,tmpl.shape[0]-3):
                cv2.imwrite("./dataset2/ground_truths/"+str(lob)+'_'+str(k+1)+'.png',np.array([tmpl[k+3]]).transpose(1,2,0))
            lob+=1
        else:
            i-=1
   1: .ipynb_checkpoints/
   2: %history -g
