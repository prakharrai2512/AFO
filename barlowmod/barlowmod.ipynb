{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import argparse\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import signal\n",
    "import subprocess\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from PIL import Image, ImageOps, ImageFilter\n",
    "from torch import nn, optim\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import random\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import numpy as np\n",
    "import torch.utils.data\n",
    "import cv2\n",
    "import torchvision.models.segmentation\n",
    "import os\n",
    "import patchify\n",
    "from sklearn.datasets import load_sample_image\n",
    "from sklearn.feature_extraction import image as skimg\n",
    "import imgaug as ia\n",
    "import imgaug.augmenters as iaa\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights,MaskRCNN_ResNet50_FPN_Weights\n",
    "import torch.optim\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sys\n",
    "sys.path.insert(1, '/home/prakharug/AFO')\n",
    "sys.path.insert(1, '/home/prakharug/AFO/pycoco')\n",
    "from pycoco.engine import train_one_epoch, evaluate\n",
    "import miou_eval as mi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_dims(a, start=0, count=2):\n",
    "    \"\"\" Reshapes numpy array a by combining count dimensions, \n",
    "        starting at dimension index start \"\"\"\n",
    "    s = a.shape\n",
    "    return np.reshape(a, s[:start] + (-1,) + s[start+count:])\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bigWW\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "class MMCellDataset(Dataset):\n",
    "    def __init__(self,root_dir, tester = False, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.img = []\n",
    "        self.nml = []\n",
    "        self.normalize = torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "        self.tester = tester\n",
    "        self.datano =  root_dir.split('/')[1]\n",
    "        print(self.datano)\n",
    "        for im_name in os.listdir(self.root_dir):\n",
    "            self.nml.append(im_name)\n",
    "            # tmplist = []\n",
    "            # lent = len(im_name[:-4])\n",
    "            # print(im_name,im_name[:lent])\n",
    "            # imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)\n",
    "            # tmplist.append(imgtmp[0])\n",
    "            # tmplist.append(imgtmp[1])\n",
    "            # tmplist.append(imgtmp[2])\n",
    "            # for gt_name in os.listdir(\"../\"+self.datano+\"/ground_truths\"):\n",
    "            #     if gt_name[0:0+lent]==im_name[:lent] and gt_name[0+lent]==\"_\":\n",
    "            #         mask = np.array(cv2.imread(\"../\"+self.datano+\"/ground_truths/\"+gt_name,0))\n",
    "            #         tmplist.append(mask)\n",
    "            # tmplist = np.array(tmplist)\n",
    "            # #print(tmplist.shape)\n",
    "            # self.img.append(tmplist)   \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len([name for name in os.listdir(self.root_dir)])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = iaa.Sequential([\n",
    "            iaa.WithHueAndSaturation([\n",
    "            iaa.WithChannels(0, iaa.Add((-15, 15))),\n",
    "            iaa.WithChannels(1, [\n",
    "                iaa.Multiply((0.5, 1.5)),\n",
    "                iaa.LinearContrast((0.5, 1.5))\n",
    "                ])\n",
    "            ])\n",
    "            ,\n",
    "            ])\n",
    "        seq2 = iaa.Sequential([\n",
    "        iaa.Fliplr(0.5), # horizontal flips\n",
    "        iaa.Crop(percent=(0, 0.1)), # random crops\n",
    "        # Small gaussian blur with random sigma between 0 and 0.5.\n",
    "        # But we only blur about 50% of all images.\n",
    "        iaa.Sometimes(\n",
    "            0.5,\n",
    "            iaa.GaussianBlur(sigma=(0, 0.5))\n",
    "        ),\n",
    "        # Strengthen or weaken the contrast in each image.\n",
    "        iaa.LinearContrast((0.75, 1.5)),\n",
    "        # Add gaussian noise.\n",
    "        # For 50% of all images, we sample the noise once per pixel.\n",
    "        # For the other 50% of all images, we sample the noise per pixel AND\n",
    "        # channel. This can change the color (not only brightness) of the\n",
    "        # pixels.\n",
    "        iaa.AdditiveGaussianNoise(loc=0, scale=(0.0, 0.05*255), per_channel=0.5),\n",
    "        # Make some images brighter and some darker.\n",
    "        # In 20% of all cases, we sample the multiplier once per channel,\n",
    "        # which can end up changing the color of the images.\n",
    "        iaa.Multiply((0.8, 1.2), per_channel=0.2),\n",
    "        # Apply affine transformations to each image.\n",
    "        # Scale/zoom them, translate/move them, rotate them and shear them.\n",
    "    ], random_order=True) # apply augmenters in random order\n",
    "\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        im_name = self.nml[idx]\n",
    "        tmplist = []\n",
    "        lent = len(im_name[:-4])\n",
    "        #print(im_name,im_name[:lent])\n",
    "        imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)\n",
    "        tmplist.append(imgtmp[0])\n",
    "        tmplist.append(imgtmp[1])\n",
    "        tmplist.append(imgtmp[2])\n",
    "        cytomask = np.zeros(imgtmp[0].shape)\n",
    "        nucmask = np.zeros(imgtmp[0].shape)\n",
    "        remmask = np.zeros(imgtmp[0].shape)\n",
    "        for gt_name in os.listdir(\"../\"+self.datano+\"/ground_truths\"):\n",
    "            if gt_name[0:0+lent]==im_name[:lent] and gt_name[0+lent]==\"_\":\n",
    "                mask = np.array(cv2.imread(\"../\"+self.datano+\"/ground_truths/\"+gt_name,0))\n",
    "                cytomask += (mask==20).astype(np.uint)\n",
    "                nucmask += (mask==40).astype(np.uint)\n",
    "                remmask += (mask==0).astype(np.uint)\n",
    "                tmplist.append(mask)\n",
    "        tmplist = np.array(tmplist)\n",
    "        #print(\"Tmplist: \",tmplist.shape)\n",
    "        patch_stack = tmplist\n",
    "        patch_stack = np.array(patch_stack)\n",
    "        # k_w = random.randint(700,patch_stack.shape[2])\n",
    "        # k_h = random.randint(700,patch_stack.shape[1])\n",
    "        k_w = 1000\n",
    "        k_h = 1000\n",
    "        # k_w = patch_stack.shape[2]\n",
    "        # k_h = patch_stack.shape[1]\n",
    "        o_w = random.randint(0,patch_stack.shape[2]-k_w)\n",
    "        o_h = random.randint(0,patch_stack.shape[1]-k_h)\n",
    "        #print(cytomask.shape)\n",
    "        cytomask = (cytomask!=0).astype(np.uint8)[o_h:o_h+k_h,o_w:o_w+k_w]\n",
    "        nucmask = (nucmask!=0).astype(np.uint8)[o_h:o_h+k_h,o_w:o_w+k_w]\n",
    "        remmask = (remmask!=0).astype(np.uint8)[o_h:o_h+k_h,o_w:o_w+k_w]\n",
    "        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]\n",
    "        tesc = np.array([patch_img[0]*cytomask,patch_img[1]*cytomask,patch_img[2]*cytomask]).transpose(1,2,0)\n",
    "        #print(tesc.shape)\n",
    "        cytoimg = seq(image = np.array([patch_img[0]*cytomask,patch_img[1]*cytomask,patch_img[2]*cytomask]).transpose(1,2,0))\n",
    "        nucimg = seq(image = np.array([patch_img[0]*nucmask,patch_img[1]*nucmask,patch_img[2]*nucmask]).transpose(1,2,0))\n",
    "        remimg = seq(image = np.array([patch_img[0]*remmask,patch_img[1]*remmask,patch_img[2]*remmask]).transpose(1,2,0))\n",
    "        patch_img2 = seq2(image = cv2.cvtColor((cytoimg+nucimg+remimg), cv2.COLOR_BGR2LAB )).transpose(2,0,1)\n",
    "        patch_img1 = seq2(image = cv2.cvtColor(patch_img.transpose(1,2,0), cv2.COLOR_BGR2LAB )).transpose(2,0,1)\n",
    "        patch_img1 = torch.as_tensor(patch_img1, dtype=torch.float32)\n",
    "        patch_img1 = patch_img1/255\n",
    "        patch_img1 = self.normalize(patch_img1)\n",
    "        patch_img2 = torch.as_tensor(patch_img2, dtype=torch.float32)\n",
    "        patch_img2 = patch_img2/255\n",
    "        patch_img2 = self.normalize(patch_img2)\n",
    "        #instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]\n",
    "        #instances = instances.transpose(1,2,0)\n",
    "        # data = {}\n",
    "        # masks = []\n",
    "        # boxes = []\n",
    "        # area = []\n",
    "        # t=0\n",
    "        # for a in range(instances.shape[2]):\n",
    "        #     dispim = instances[:,:,a]\n",
    "        #     if np.all(dispim == 0):\n",
    "        #         continue\n",
    "        #     x,y,w,h = cv2.boundingRect(dispim)\n",
    "        #     boxes.append([x, y, x+w, y+h])\n",
    "        #     area.append(torch.tensor(h*w))\n",
    "        #     masks.append(dispim/255)\n",
    "        #     t=1\n",
    "        # if t==0:\n",
    "        #     #print(\"Abort\")\n",
    "        #     if self.tester:\n",
    "        #         return \"Problem\",\"Hao gai\"\n",
    "        #     else:\n",
    "        #         #print(\"Abort Abort \")\n",
    "        #         return self.__getitem__((idx+1)%len(self.nml))\n",
    "        # masks = np.array(masks)\n",
    "        # masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "        # boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # area = torch.as_tensor(area, dtype=torch.float32)\n",
    "        # img = torch.as_tensor(patch_img, dtype=torch.float32)\n",
    "        # img = img/255\n",
    "        # data[\"boxes\"] =  boxes\n",
    "        # iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
    "        # data[\"iscrowd\"] = iscrowd\n",
    "        # data[\"labels\"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class\n",
    "        # data[\"masks\"] = masks\n",
    "        # data[\"area\"] = area\n",
    "        # data[\"image_id\"] = torch.tensor(idx)\n",
    "        return torch.stack([patch_img1,patch_img2])\n",
    "train = MMCellDataset(\"../bigWW/images/\")\n",
    "#test = MMCellDataset(\"../dataset12/test/\")\n",
    "fintest = MMCellDataset(\"../test/images/\",True)\n",
    "#train = torch.utils.data.ConcatDataset([train, test])\n",
    "#print(len(test))\n",
    "trainloader = DataLoader(train, batch_size=128, shuffle=True,num_workers=10)\n",
    "#testloader = DataLoader(test, batch_size=1, shuffle=True,collate_fn = collate_fn,num_workers=10)\n",
    "#fintestloader = DataLoader(fintest, batch_size=1, shuffle=True,collate_fn = collate_fn,num_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "498\n"
     ]
    }
   ],
   "source": [
    "# print(train[0][1].shape)\n",
    "# writer = cv2.cvtColor(train[0][1].transpose(1,2,0),cv2.COLOR_LAB2BGR)\n",
    "# cv2.imwrite(\"problem.png\",writer)\n",
    "print(len(train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianBlur(object):\n",
    "    def __init__(self, p):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if random.random() < self.p:\n",
    "            sigma = random.random() * 1.9 + 0.1\n",
    "            return img.filter(ImageFilter.GaussianBlur(sigma))\n",
    "        else:\n",
    "            return img\n",
    "\n",
    "\n",
    "class Solarization(object):\n",
    "    def __init__(self, p):\n",
    "        self.p = p\n",
    "\n",
    "    def __call__(self, img):\n",
    "        if random.random() < self.p:\n",
    "            return ImageOps.solarize(img)\n",
    "        else:\n",
    "            return img\n",
    "\n",
    "\n",
    "class Transform:\n",
    "    def __init__(self):\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224, interpolation=Image.BICUBIC),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomApply(\n",
    "                [transforms.ColorJitter(brightness=0.4, contrast=0.4,\n",
    "                                        saturation=0.2, hue=0.1)],\n",
    "                p=0.8\n",
    "            ),\n",
    "            transforms.RandomGrayscale(p=0.2),\n",
    "            GaussianBlur(p=1.0),\n",
    "            Solarization(p=0.0),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        self.transform_prime = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224, interpolation=Image.BICUBIC),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomApply(\n",
    "                [transforms.ColorJitter(brightness=0.4, contrast=0.4,\n",
    "                                        saturation=0.2, hue=0.1)],\n",
    "                p=0.8\n",
    "            ),\n",
    "            transforms.RandomGrayscale(p=0.2),\n",
    "            GaussianBlur(p=0.1),\n",
    "            Solarization(p=0.2),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    def __call__(self, x):\n",
    "        y1 = self.transform(x)\n",
    "        y2 = self.transform_prime(x)\n",
    "        return y1, y2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def off_diagonal(x):\n",
    "    # return a flattened view of the off-diagonal elements of a square matrix\n",
    "    n, m = x.shape\n",
    "    assert n == m\n",
    "    return x.flatten()[:-1].view(n - 1, n + 1)[:, 1:].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection.backbone_utils import _resnet_fpn_extractor\n",
    "class BarlowTwins(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super().__init__()\n",
    "        #self.args = args\n",
    "        self.backbone = torchvision.models.resnet50(zero_init_residual=True)\n",
    "        #self.backbone = _resnet_fpn_extractor(self.backbone, 5)\n",
    "        self.backbone.fc = nn.Identity()\n",
    "\n",
    "        # projector\n",
    "        sizes = [2048,8192] \n",
    "        layers = []\n",
    "        for i in range(len(sizes) - 2):\n",
    "            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=False))\n",
    "            layers.append(nn.BatchNorm1d(sizes[i + 1]))\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "        layers.append(nn.Linear(sizes[-2], sizes[-1], bias=False))\n",
    "        self.projector = nn.Sequential(*layers)\n",
    "\n",
    "        # normalization layer for the representations z1 and z2\n",
    "        self.bn = nn.BatchNorm1d(sizes[-1], affine=False)\n",
    "\n",
    "    def forward(self, y):\n",
    "        y1 = y[0]\n",
    "        y2 = y[1]\n",
    "        z1 = self.projector(self.backbone(y1))\n",
    "        z2 = self.projector(self.backbone(y2))\n",
    "\n",
    "        # empirical cross-correlation matrix\n",
    "        c = self.bn(z1).T @ self.bn(z2)\n",
    "\n",
    "        # sum the cross-correlation matrix between all gpus\n",
    "        c.div_(128)\n",
    "        #torch.distributed.all_reduce(c)\n",
    "\n",
    "        on_diag = torch.diagonal(c).add_(-1).pow_(2).sum()\n",
    "        off_diag = off_diagonal(c).pow_(2).sum()\n",
    "        loss = on_diag + 1 * off_diag\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 8596.900390625\n",
      "1 loss: 8726.4248046875\n",
      "2 loss: 8644.9306640625\n",
      "3 loss: 9171.0283203125\n"
     ]
    }
   ],
   "source": [
    "# bt = BarlowTwins(None)\n",
    "# print(bt(torch.stack([train[0][0],train[1][0]]),torch.stack([train[0][1],train[1][1]])))\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "model = BarlowTwins(None)\n",
    "model.to(device)\n",
    "dicl = torch.load(\"./resenter2.1.torch\")\n",
    "#print(dicl.keys())\n",
    "ml = OrderedDict()\n",
    "for i in dicl.keys():\n",
    "    if(i[0:4]==\"body\"):\n",
    "        ml[i[5:]] = dicl[i]\n",
    "    else:\n",
    "        ml[i] = dicl[i]\n",
    "#print(dicl)\n",
    "model.backbone.load_state_dict(ml,strict=False)\n",
    "#model.load_state_dict(torch.load(\"alchemy_0_9116.torch\"))\n",
    "# lr=0.1\n",
    "# if(iv>=10==0):\n",
    "#     lr=6e-5\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.00001,momentum=0,weight_decay=0.00004)\n",
    "#optimizer = torch.optim.AdamW(model.parameters(), lr=8e-5)\n",
    "model.train()\n",
    "for epoch in range(1):\n",
    "    for i, images in enumerate(trainloader, 0):\n",
    "\n",
    "            #lrs+=1\n",
    "            images = images.cuda()\n",
    "            #images = list(image.to(device) for image in images)\n",
    "            #targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            optimizer.zero_grad()\n",
    "            loss = model(images)\n",
    "            #losses = sum(loss for loss in loss_dict.values())\n",
    "            #if losses.item()>=0.2:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            del images\n",
    "            #print(\"lr: \",optimizer.param_groups[0]['lr'])\n",
    "            print(i,'loss:', loss.item())\n",
    "            # if lrs==50:\n",
    "            #     lrs=0\n",
    "            #     #scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 200)\n",
    "            #     # for param_group in optimizer.param_groups:\n",
    "            #     #     param_group['lr'] = lr*0.6\n",
    "            #     #     lr = lr * 0.6\n",
    "            #     val = evalm(model,fintestloader)\n",
    "            #     if(val>min_val):\n",
    "            #         print(\"saved \",str(i),\" with mIoU:\",val)\n",
    "            #         min_val=val\n",
    "            #         torch.save(model.state_dict(), \"alchemy_\"+str(iv)+\"_\"+str(val)[0:6]+\"_d12.torch\")\n",
    "        #     #val = evalm(model,fintestloader)\n",
    "        #     val = evalm(model,fintestloader)\n",
    "        #     if(val>min_val):\n",
    "        #         print(\"saved \",str(i),\" with mIoU:\",val)\n",
    "        #         min_val=val\n",
    "        #         torch.save(model.state_dict(), \"alchemy_\"+str(iv)+\"_\"+str(val)[0:6]+\".torch\")\n",
    "        #     #torch.save(model.state_dict(), \"alchemy_\"+str(iv)+\".torch\")\n",
    "        # #scheduler.step()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "body.conv1.weight\n",
      "body.bn1.weight\n",
      "body.bn1.bias\n",
      "body.bn1.running_mean\n",
      "body.bn1.running_var\n",
      "body.bn1.num_batches_tracked\n",
      "body.layer1.0.conv1.weight\n",
      "body.layer1.0.bn1.weight\n",
      "body.layer1.0.bn1.bias\n",
      "body.layer1.0.bn1.running_mean\n",
      "body.layer1.0.bn1.running_var\n",
      "body.layer1.0.bn1.num_batches_tracked\n",
      "body.layer1.0.conv2.weight\n",
      "body.layer1.0.bn2.weight\n",
      "body.layer1.0.bn2.bias\n",
      "body.layer1.0.bn2.running_mean\n",
      "body.layer1.0.bn2.running_var\n",
      "body.layer1.0.bn2.num_batches_tracked\n",
      "body.layer1.0.conv3.weight\n",
      "body.layer1.0.bn3.weight\n",
      "body.layer1.0.bn3.bias\n",
      "body.layer1.0.bn3.running_mean\n",
      "body.layer1.0.bn3.running_var\n",
      "body.layer1.0.bn3.num_batches_tracked\n",
      "body.layer1.0.downsample.0.weight\n",
      "body.layer1.0.downsample.1.weight\n",
      "body.layer1.0.downsample.1.bias\n",
      "body.layer1.0.downsample.1.running_mean\n",
      "body.layer1.0.downsample.1.running_var\n",
      "body.layer1.0.downsample.1.num_batches_tracked\n",
      "body.layer1.1.conv1.weight\n",
      "body.layer1.1.bn1.weight\n",
      "body.layer1.1.bn1.bias\n",
      "body.layer1.1.bn1.running_mean\n",
      "body.layer1.1.bn1.running_var\n",
      "body.layer1.1.bn1.num_batches_tracked\n",
      "body.layer1.1.conv2.weight\n",
      "body.layer1.1.bn2.weight\n",
      "body.layer1.1.bn2.bias\n",
      "body.layer1.1.bn2.running_mean\n",
      "body.layer1.1.bn2.running_var\n",
      "body.layer1.1.bn2.num_batches_tracked\n",
      "body.layer1.1.conv3.weight\n",
      "body.layer1.1.bn3.weight\n",
      "body.layer1.1.bn3.bias\n",
      "body.layer1.1.bn3.running_mean\n",
      "body.layer1.1.bn3.running_var\n",
      "body.layer1.1.bn3.num_batches_tracked\n",
      "body.layer1.2.conv1.weight\n",
      "body.layer1.2.bn1.weight\n",
      "body.layer1.2.bn1.bias\n",
      "body.layer1.2.bn1.running_mean\n",
      "body.layer1.2.bn1.running_var\n",
      "body.layer1.2.bn1.num_batches_tracked\n",
      "body.layer1.2.conv2.weight\n",
      "body.layer1.2.bn2.weight\n",
      "body.layer1.2.bn2.bias\n",
      "body.layer1.2.bn2.running_mean\n",
      "body.layer1.2.bn2.running_var\n",
      "body.layer1.2.bn2.num_batches_tracked\n",
      "body.layer1.2.conv3.weight\n",
      "body.layer1.2.bn3.weight\n",
      "body.layer1.2.bn3.bias\n",
      "body.layer1.2.bn3.running_mean\n",
      "body.layer1.2.bn3.running_var\n",
      "body.layer1.2.bn3.num_batches_tracked\n",
      "body.layer2.0.conv1.weight\n",
      "body.layer2.0.bn1.weight\n",
      "body.layer2.0.bn1.bias\n",
      "body.layer2.0.bn1.running_mean\n",
      "body.layer2.0.bn1.running_var\n",
      "body.layer2.0.bn1.num_batches_tracked\n",
      "body.layer2.0.conv2.weight\n",
      "body.layer2.0.bn2.weight\n",
      "body.layer2.0.bn2.bias\n",
      "body.layer2.0.bn2.running_mean\n",
      "body.layer2.0.bn2.running_var\n",
      "body.layer2.0.bn2.num_batches_tracked\n",
      "body.layer2.0.conv3.weight\n",
      "body.layer2.0.bn3.weight\n",
      "body.layer2.0.bn3.bias\n",
      "body.layer2.0.bn3.running_mean\n",
      "body.layer2.0.bn3.running_var\n",
      "body.layer2.0.bn3.num_batches_tracked\n",
      "body.layer2.0.downsample.0.weight\n",
      "body.layer2.0.downsample.1.weight\n",
      "body.layer2.0.downsample.1.bias\n",
      "body.layer2.0.downsample.1.running_mean\n",
      "body.layer2.0.downsample.1.running_var\n",
      "body.layer2.0.downsample.1.num_batches_tracked\n",
      "body.layer2.1.conv1.weight\n",
      "body.layer2.1.bn1.weight\n",
      "body.layer2.1.bn1.bias\n",
      "body.layer2.1.bn1.running_mean\n",
      "body.layer2.1.bn1.running_var\n",
      "body.layer2.1.bn1.num_batches_tracked\n",
      "body.layer2.1.conv2.weight\n",
      "body.layer2.1.bn2.weight\n",
      "body.layer2.1.bn2.bias\n",
      "body.layer2.1.bn2.running_mean\n",
      "body.layer2.1.bn2.running_var\n",
      "body.layer2.1.bn2.num_batches_tracked\n",
      "body.layer2.1.conv3.weight\n",
      "body.layer2.1.bn3.weight\n",
      "body.layer2.1.bn3.bias\n",
      "body.layer2.1.bn3.running_mean\n",
      "body.layer2.1.bn3.running_var\n",
      "body.layer2.1.bn3.num_batches_tracked\n",
      "body.layer2.2.conv1.weight\n",
      "body.layer2.2.bn1.weight\n",
      "body.layer2.2.bn1.bias\n",
      "body.layer2.2.bn1.running_mean\n",
      "body.layer2.2.bn1.running_var\n",
      "body.layer2.2.bn1.num_batches_tracked\n",
      "body.layer2.2.conv2.weight\n",
      "body.layer2.2.bn2.weight\n",
      "body.layer2.2.bn2.bias\n",
      "body.layer2.2.bn2.running_mean\n",
      "body.layer2.2.bn2.running_var\n",
      "body.layer2.2.bn2.num_batches_tracked\n",
      "body.layer2.2.conv3.weight\n",
      "body.layer2.2.bn3.weight\n",
      "body.layer2.2.bn3.bias\n",
      "body.layer2.2.bn3.running_mean\n",
      "body.layer2.2.bn3.running_var\n",
      "body.layer2.2.bn3.num_batches_tracked\n",
      "body.layer2.3.conv1.weight\n",
      "body.layer2.3.bn1.weight\n",
      "body.layer2.3.bn1.bias\n",
      "body.layer2.3.bn1.running_mean\n",
      "body.layer2.3.bn1.running_var\n",
      "body.layer2.3.bn1.num_batches_tracked\n",
      "body.layer2.3.conv2.weight\n",
      "body.layer2.3.bn2.weight\n",
      "body.layer2.3.bn2.bias\n",
      "body.layer2.3.bn2.running_mean\n",
      "body.layer2.3.bn2.running_var\n",
      "body.layer2.3.bn2.num_batches_tracked\n",
      "body.layer2.3.conv3.weight\n",
      "body.layer2.3.bn3.weight\n",
      "body.layer2.3.bn3.bias\n",
      "body.layer2.3.bn3.running_mean\n",
      "body.layer2.3.bn3.running_var\n",
      "body.layer2.3.bn3.num_batches_tracked\n",
      "body.layer3.0.conv1.weight\n",
      "body.layer3.0.bn1.weight\n",
      "body.layer3.0.bn1.bias\n",
      "body.layer3.0.bn1.running_mean\n",
      "body.layer3.0.bn1.running_var\n",
      "body.layer3.0.bn1.num_batches_tracked\n",
      "body.layer3.0.conv2.weight\n",
      "body.layer3.0.bn2.weight\n",
      "body.layer3.0.bn2.bias\n",
      "body.layer3.0.bn2.running_mean\n",
      "body.layer3.0.bn2.running_var\n",
      "body.layer3.0.bn2.num_batches_tracked\n",
      "body.layer3.0.conv3.weight\n",
      "body.layer3.0.bn3.weight\n",
      "body.layer3.0.bn3.bias\n",
      "body.layer3.0.bn3.running_mean\n",
      "body.layer3.0.bn3.running_var\n",
      "body.layer3.0.bn3.num_batches_tracked\n",
      "body.layer3.0.downsample.0.weight\n",
      "body.layer3.0.downsample.1.weight\n",
      "body.layer3.0.downsample.1.bias\n",
      "body.layer3.0.downsample.1.running_mean\n",
      "body.layer3.0.downsample.1.running_var\n",
      "body.layer3.0.downsample.1.num_batches_tracked\n",
      "body.layer3.1.conv1.weight\n",
      "body.layer3.1.bn1.weight\n",
      "body.layer3.1.bn1.bias\n",
      "body.layer3.1.bn1.running_mean\n",
      "body.layer3.1.bn1.running_var\n",
      "body.layer3.1.bn1.num_batches_tracked\n",
      "body.layer3.1.conv2.weight\n",
      "body.layer3.1.bn2.weight\n",
      "body.layer3.1.bn2.bias\n",
      "body.layer3.1.bn2.running_mean\n",
      "body.layer3.1.bn2.running_var\n",
      "body.layer3.1.bn2.num_batches_tracked\n",
      "body.layer3.1.conv3.weight\n",
      "body.layer3.1.bn3.weight\n",
      "body.layer3.1.bn3.bias\n",
      "body.layer3.1.bn3.running_mean\n",
      "body.layer3.1.bn3.running_var\n",
      "body.layer3.1.bn3.num_batches_tracked\n",
      "body.layer3.2.conv1.weight\n",
      "body.layer3.2.bn1.weight\n",
      "body.layer3.2.bn1.bias\n",
      "body.layer3.2.bn1.running_mean\n",
      "body.layer3.2.bn1.running_var\n",
      "body.layer3.2.bn1.num_batches_tracked\n",
      "body.layer3.2.conv2.weight\n",
      "body.layer3.2.bn2.weight\n",
      "body.layer3.2.bn2.bias\n",
      "body.layer3.2.bn2.running_mean\n",
      "body.layer3.2.bn2.running_var\n",
      "body.layer3.2.bn2.num_batches_tracked\n",
      "body.layer3.2.conv3.weight\n",
      "body.layer3.2.bn3.weight\n",
      "body.layer3.2.bn3.bias\n",
      "body.layer3.2.bn3.running_mean\n",
      "body.layer3.2.bn3.running_var\n",
      "body.layer3.2.bn3.num_batches_tracked\n",
      "body.layer3.3.conv1.weight\n",
      "body.layer3.3.bn1.weight\n",
      "body.layer3.3.bn1.bias\n",
      "body.layer3.3.bn1.running_mean\n",
      "body.layer3.3.bn1.running_var\n",
      "body.layer3.3.bn1.num_batches_tracked\n",
      "body.layer3.3.conv2.weight\n",
      "body.layer3.3.bn2.weight\n",
      "body.layer3.3.bn2.bias\n",
      "body.layer3.3.bn2.running_mean\n",
      "body.layer3.3.bn2.running_var\n",
      "body.layer3.3.bn2.num_batches_tracked\n",
      "body.layer3.3.conv3.weight\n",
      "body.layer3.3.bn3.weight\n",
      "body.layer3.3.bn3.bias\n",
      "body.layer3.3.bn3.running_mean\n",
      "body.layer3.3.bn3.running_var\n",
      "body.layer3.3.bn3.num_batches_tracked\n",
      "body.layer3.4.conv1.weight\n",
      "body.layer3.4.bn1.weight\n",
      "body.layer3.4.bn1.bias\n",
      "body.layer3.4.bn1.running_mean\n",
      "body.layer3.4.bn1.running_var\n",
      "body.layer3.4.bn1.num_batches_tracked\n",
      "body.layer3.4.conv2.weight\n",
      "body.layer3.4.bn2.weight\n",
      "body.layer3.4.bn2.bias\n",
      "body.layer3.4.bn2.running_mean\n",
      "body.layer3.4.bn2.running_var\n",
      "body.layer3.4.bn2.num_batches_tracked\n",
      "body.layer3.4.conv3.weight\n",
      "body.layer3.4.bn3.weight\n",
      "body.layer3.4.bn3.bias\n",
      "body.layer3.4.bn3.running_mean\n",
      "body.layer3.4.bn3.running_var\n",
      "body.layer3.4.bn3.num_batches_tracked\n",
      "body.layer3.5.conv1.weight\n",
      "body.layer3.5.bn1.weight\n",
      "body.layer3.5.bn1.bias\n",
      "body.layer3.5.bn1.running_mean\n",
      "body.layer3.5.bn1.running_var\n",
      "body.layer3.5.bn1.num_batches_tracked\n",
      "body.layer3.5.conv2.weight\n",
      "body.layer3.5.bn2.weight\n",
      "body.layer3.5.bn2.bias\n",
      "body.layer3.5.bn2.running_mean\n",
      "body.layer3.5.bn2.running_var\n",
      "body.layer3.5.bn2.num_batches_tracked\n",
      "body.layer3.5.conv3.weight\n",
      "body.layer3.5.bn3.weight\n",
      "body.layer3.5.bn3.bias\n",
      "body.layer3.5.bn3.running_mean\n",
      "body.layer3.5.bn3.running_var\n",
      "body.layer3.5.bn3.num_batches_tracked\n",
      "body.layer4.0.conv1.weight\n",
      "body.layer4.0.bn1.weight\n",
      "body.layer4.0.bn1.bias\n",
      "body.layer4.0.bn1.running_mean\n",
      "body.layer4.0.bn1.running_var\n",
      "body.layer4.0.bn1.num_batches_tracked\n",
      "body.layer4.0.conv2.weight\n",
      "body.layer4.0.bn2.weight\n",
      "body.layer4.0.bn2.bias\n",
      "body.layer4.0.bn2.running_mean\n",
      "body.layer4.0.bn2.running_var\n",
      "body.layer4.0.bn2.num_batches_tracked\n",
      "body.layer4.0.conv3.weight\n",
      "body.layer4.0.bn3.weight\n",
      "body.layer4.0.bn3.bias\n",
      "body.layer4.0.bn3.running_mean\n",
      "body.layer4.0.bn3.running_var\n",
      "body.layer4.0.bn3.num_batches_tracked\n",
      "body.layer4.0.downsample.0.weight\n",
      "body.layer4.0.downsample.1.weight\n",
      "body.layer4.0.downsample.1.bias\n",
      "body.layer4.0.downsample.1.running_mean\n",
      "body.layer4.0.downsample.1.running_var\n",
      "body.layer4.0.downsample.1.num_batches_tracked\n",
      "body.layer4.1.conv1.weight\n",
      "body.layer4.1.bn1.weight\n",
      "body.layer4.1.bn1.bias\n",
      "body.layer4.1.bn1.running_mean\n",
      "body.layer4.1.bn1.running_var\n",
      "body.layer4.1.bn1.num_batches_tracked\n",
      "body.layer4.1.conv2.weight\n",
      "body.layer4.1.bn2.weight\n",
      "body.layer4.1.bn2.bias\n",
      "body.layer4.1.bn2.running_mean\n",
      "body.layer4.1.bn2.running_var\n",
      "body.layer4.1.bn2.num_batches_tracked\n",
      "body.layer4.1.conv3.weight\n",
      "body.layer4.1.bn3.weight\n",
      "body.layer4.1.bn3.bias\n",
      "body.layer4.1.bn3.running_mean\n",
      "body.layer4.1.bn3.running_var\n",
      "body.layer4.1.bn3.num_batches_tracked\n",
      "body.layer4.2.conv1.weight\n",
      "body.layer4.2.bn1.weight\n",
      "body.layer4.2.bn1.bias\n",
      "body.layer4.2.bn1.running_mean\n",
      "body.layer4.2.bn1.running_var\n",
      "body.layer4.2.bn1.num_batches_tracked\n",
      "body.layer4.2.conv2.weight\n",
      "body.layer4.2.bn2.weight\n",
      "body.layer4.2.bn2.bias\n",
      "body.layer4.2.bn2.running_mean\n",
      "body.layer4.2.bn2.running_var\n",
      "body.layer4.2.bn2.num_batches_tracked\n",
      "body.layer4.2.conv3.weight\n",
      "body.layer4.2.bn3.weight\n",
      "body.layer4.2.bn3.bias\n",
      "body.layer4.2.bn3.running_mean\n",
      "body.layer4.2.bn3.running_var\n",
      "body.layer4.2.bn3.num_batches_tracked\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ccl = model.state_dict()\n",
    "\n",
    "for i in dicl.keys():\n",
    "    if(i[0:4]==\"body\"):\n",
    "        dicl[i] = ccl[\"backbone.\"+i[5:]]\n",
    "        print(i) \n",
    "    else:\n",
    "        pass\n",
    "\n",
    "print()\n",
    "    \n",
    "torch.save(dicl,\"new_resnet2.0.torch\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('prakhar_pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "603527dc4b5f0cf90dba6785ed8ce1bf41404d422ebeb0779687782e89bf431c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
