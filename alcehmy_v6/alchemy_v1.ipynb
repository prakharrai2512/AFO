{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import numpy as np\n",
    "import torch.utils.data\n",
    "import cv2\n",
    "import torchvision.models.segmentation\n",
    "import torch\n",
    "import os\n",
    "import patchify\n",
    "from sklearn.datasets import load_sample_image\n",
    "from sklearn.feature_extraction import image as skimg\n",
    "import imgaug as ia\n",
    "import imgaug.augmenters as iaa\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights,MaskRCNN_ResNet50_FPN_Weights\n",
    "import torch.optim\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sys\n",
    "sys.path.insert(1, '/home/prakharug/AFO')\n",
    "sys.path.insert(1, '/home/prakharug/AFO/pycoco')\n",
    "from pycoco.engine import train_one_epoch, evaluate\n",
    "import miou_eval as mi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_dims(a, start=0, count=2):\n",
    "    \"\"\" Reshapes numpy array a by combining count dimensions, \n",
    "        starting at dimension index start \"\"\"\n",
    "    s = a.shape\n",
    "    return np.reshape(a, s[:start] + (-1,) + s[start+count:])\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') \n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalm(model,fintestloader):\n",
    "    model.eval()\n",
    "    iou = 0\n",
    "    cnt = 0\n",
    "    for i, data in enumerate(fintestloader, 0):\n",
    "        images, targets = data\n",
    "        #print(type(targets))\n",
    "        if(images[0]==\"Problem\"):\n",
    "            continue\n",
    "        #print(type(images[0]))\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        #print(model(images)[0][\"masks\"].squeeze().shape,targets[0][\"masks\"].shape)\n",
    "        tiou,tcnt = mi.miou_eval(targets[0][\"masks\"],(model(images)[0][\"masks\"].squeeze(1)>0.5))\n",
    "        #print(tiou/tcnt)\n",
    "        #print(\"Txnt: \",tcnt)\n",
    "        del images,targets\n",
    "        iou += tiou\n",
    "        cnt += tcnt\n",
    "        #print(iou/cnt)\n",
    "    print(\"mIoU on test is:\",iou.item()/cnt)\n",
    "    model.train()\n",
    "    return iou.item()/cnt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset4\n",
      "test\n"
     ]
    }
   ],
   "source": [
    "class MMCellDataset(Dataset):\n",
    "    def __init__(self,root_dir, tester = False, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.img = []\n",
    "        self.nml = []\n",
    "        self.tester = tester\n",
    "        self.datano =  root_dir.split('/')[1]\n",
    "        print(self.datano)\n",
    "        for im_name in os.listdir(self.root_dir):\n",
    "            self.nml.append(im_name)\n",
    "            # tmplist = []\n",
    "            # lent = len(im_name[:-4])\n",
    "            # print(im_name,im_name[:lent])\n",
    "            # imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)\n",
    "            # tmplist.append(imgtmp[0])\n",
    "            # tmplist.append(imgtmp[1])\n",
    "            # tmplist.append(imgtmp[2])\n",
    "            # for gt_name in os.listdir(\"../\"+self.datano+\"/ground_truths\"):\n",
    "            #     if gt_name[0:0+lent]==im_name[:lent] and gt_name[0+lent]==\"_\":\n",
    "            #         mask = np.array(cv2.imread(\"../\"+self.datano+\"/ground_truths/\"+gt_name,0))\n",
    "            #         tmplist.append(mask)\n",
    "            # tmplist = np.array(tmplist)\n",
    "            # #print(tmplist.shape)\n",
    "            # self.img.append(tmplist)   \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len([name for name in os.listdir(self.root_dir)])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        im_name = self.nml[idx]\n",
    "        tmplist = []\n",
    "        lent = len(im_name[:-4])\n",
    "        #print(im_name,im_name[:lent])\n",
    "        imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)\n",
    "        tmplist.append(imgtmp[0])\n",
    "        tmplist.append(imgtmp[1])\n",
    "        tmplist.append(imgtmp[2])\n",
    "        for gt_name in os.listdir(\"../\"+self.datano+\"/ground_truths\"):\n",
    "            if gt_name[0:0+lent]==im_name[:lent] and gt_name[0+lent]==\"_\":\n",
    "                mask = np.array(cv2.imread(\"../\"+self.datano+\"/ground_truths/\"+gt_name,0))\n",
    "                tmplist.append(mask)\n",
    "        tmplist = np.array(tmplist)\n",
    "        #print(\"Tmplist: \",tmplist.shape)\n",
    "        patch_stack = tmplist\n",
    "        patch_stack = np.array(patch_stack)\n",
    "        # k_w = random.randint(700,patch_stack.shape[2])\n",
    "        # k_h = random.randint(700,patch_stack.shape[1])\n",
    "        k_w = patch_stack.shape[2]\n",
    "        k_h = patch_stack.shape[1]\n",
    "        o_w = random.randint(0,patch_stack.shape[2]-k_w)\n",
    "        o_h = random.randint(0,patch_stack.shape[1]-k_h)\n",
    "        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]\n",
    "        patch_img1 = cv2.cvtColor(patch_img.transpose(1,2,0), cv2.COLOR_BGR2LAB).transpose(2,0,1)\n",
    "        patch_img_final = np.zeros(patch_img.shape)\n",
    "        patch_img_final[0] = patch_img[0]\n",
    "        patch_img_final[1] = patch_img[0]\n",
    "        patch_img_final[2] = patch_img[0]\n",
    "        patch_img = patch_img_final\n",
    "        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]\n",
    "        instances = instances.transpose(1,2,0)\n",
    "        data = {}\n",
    "        masks = []\n",
    "        boxes = []\n",
    "        area = []\n",
    "        t=0\n",
    "        for a in range(instances.shape[2]):\n",
    "            dispim = instances[:,:,a]\n",
    "            if np.all(dispim == 0):\n",
    "                continue\n",
    "            x,y,w,h = cv2.boundingRect(dispim)\n",
    "            boxes.append([x, y, x+w, y+h])\n",
    "            area.append(torch.tensor(h*w))\n",
    "            masks.append(dispim/255)\n",
    "            t=1\n",
    "        if t==0:\n",
    "            #print(\"Abort\")\n",
    "            if self.tester:\n",
    "                return \"Problem\",\"Hao gai\"\n",
    "            else:\n",
    "                #print(\"Abort Abort \")\n",
    "                return self.__getitem__((idx+1)%len(self.nml))\n",
    "        masks = np.array(masks)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        area = torch.as_tensor(area, dtype=torch.float32)\n",
    "        img = torch.as_tensor(patch_img, dtype=torch.float32)\n",
    "        img = img/255\n",
    "        data[\"boxes\"] =  boxes\n",
    "        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
    "        data[\"iscrowd\"] = iscrowd\n",
    "        data[\"labels\"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class\n",
    "        data[\"masks\"] = masks\n",
    "        data[\"area\"] = area\n",
    "        data[\"image_id\"] = torch.tensor(idx)\n",
    "        return img,data\n",
    "train = MMCellDataset(\"../dataset4/images/\")\n",
    "#test = MMCellDataset(\"../dataset12/test/\")\n",
    "fintest = MMCellDataset(\"../test/images/\",True)\n",
    "#train = torch.utils.data.ConcatDataset([train, test])\n",
    "#print(len(test))\n",
    "trainloader = DataLoader(train, batch_size=4, shuffle=True,collate_fn = collate_fn,num_workers=10)\n",
    "#testloader = DataLoader(test, batch_size=1, shuffle=True,collate_fn = collate_fn,num_workers=10)\n",
    "fintestloader = DataLoader(fintest, batch_size=1, shuffle=True,collate_fn = collate_fn,num_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class alchemy(nn.Module):\n",
    "    def __init__(self,**kwargs) -> None:\n",
    "        super(alchemy, self).__init__()\n",
    "        self.vis = False\n",
    "        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,trainable_backbone_layers=4,**kwargs)\n",
    "        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier\n",
    "        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one\n",
    "\n",
    "    def forward(self, images, targets=None):\n",
    "        ret = self.maskrcnn(images,targets)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = alchemy()\n",
    "model.to(device)\n",
    "#model.load_state_dict(torch.load(\"../alchemy_7_0.9183.torch\"))\n",
    "#model.load_state_dict(torch.load(\"alchemy_0_9116.torch\"))\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=8e-5)\n",
    "model.train()\n",
    "lmbda = lambda epoch: 0.1\n",
    "scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m30\u001b[39m): \n\u001b[0;32m----> 3\u001b[0m     model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m      5\u001b[0m     \u001b[39mfor\u001b[39;00m i, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(trainloader, \u001b[39m0\u001b[39m):\n\u001b[1;32m      6\u001b[0m         images, targets \u001b[39m=\u001b[39m data\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch in range(30): \n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        images, targets = data\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        #print(targets[0][\"boxes\"].shape)\n",
    "        optimizer.zero_grad()\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "        del images,targets\n",
    "        print(i,'loss:', losses.item())\n",
    "    \n",
    "    #torch.save(model.state_dict(), \"alchemy_\"+str(epoch)+\".torch\")\n",
    "    #scheduler.step()\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    #evaluate(model, testloader, device=device)\n",
    "    iou = 0\n",
    "    cnt = 0\n",
    "    \n",
    "    # for i, data in enumerate(testloader, 0):\n",
    "    #     images, targets = data\n",
    "    #     images = list(image.to(device) for image in images)\n",
    "    #     targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "    #     #print(type(model(images)[0][\"masks\"]))\n",
    "    #     tiou,tcnt = mi.miou_eval(targets[0][\"masks\"],(model(images)[0][\"masks\"].squeeze(1)>0.5))\n",
    "    #     del images,targets\n",
    "    #     iou += tiou\n",
    "    #     cnt += tcnt\n",
    "    # print(\"mIoU on val is:\",iou.item()/cnt)\n",
    "\n",
    "    if epoch%1==0:\n",
    "            model.eval()\n",
    "            iou = 0\n",
    "            cnt = 0\n",
    "            for i, data in enumerate(fintestloader, 0):\n",
    "                images, targets = data\n",
    "                #print(type(targets))\n",
    "                if(images[0]==\"Problem\"):\n",
    "                    continue\n",
    "                #print(type(images[0]))\n",
    "                images = list(image.to(device) for image in images)\n",
    "                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "                #print(model(images)[0][\"masks\"].squeeze().shape,targets[0][\"masks\"].shape)\n",
    "                tiou,tcnt = mi.miou_eval(targets[0][\"masks\"],(model(images)[0][\"masks\"].squeeze(1)>0.5))\n",
    "                #print(\"Txnt: \",tcnt)\n",
    "                del images,targets\n",
    "                iou += tiou\n",
    "                cnt += tcnt\n",
    "                #print(iou/cnt)\n",
    "            print(\"mIoU on test is:\",iou.item()/cnt)\n",
    "\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = alchemy(box_nms_thresh=0.5)\n",
    "model.to('cuda')\n",
    "model.load_state_dict(torch.load(\"./alchemy_0_0.9294_d12.torch\"))\n",
    "model.eval()\n",
    "#model1 = model\n",
    "for im_name in os.listdir(\"./submission/x/\"):\n",
    "    imgtmp = cv2.imread(\"./submission/x/\"+im_name,1)\n",
    "    imgtmp = cv2.cvtColor(imgtmp, cv2.COLOR_BGR2LAB ).transpose(2,0,1)\n",
    "    imgtmp = torch.as_tensor(imgtmp, dtype=torch.float32)\n",
    "    imgtmp = imgtmp/255\n",
    "    imgtmp = imgtmp.to(device)\n",
    "    output = model([imgtmp])\n",
    "    prefix = im_name[0:-4]\n",
    "    ref = 0\n",
    "    for i in range(output[0]['masks'].shape[0]):\n",
    "        print(output[0]['scores'][i].item())\n",
    "        it = torch.clone(output[0]['masks'][i])\n",
    "        it = it.detach().to('cpu').numpy()\n",
    "        it = it.squeeze(0)\n",
    "        it = (it>=0.5).astype(float)\n",
    "        it = it*255\n",
    "        cv2.imwrite(\"./submission/y/\"+prefix+\"_\"+str(i)+\".bmp\",it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! make sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 2.9674484729766846\n",
      "1 loss: 2.078972578048706\n",
      "2 loss: 1.8179246187210083\n",
      "3 loss: 1.7349241971969604\n",
      "4 loss: 1.4000447988510132\n",
      "5 loss: 1.6187957525253296\n",
      "6 loss: 1.3464940786361694\n",
      "7 loss: 1.2034196853637695\n",
      "8 loss: 1.2970610857009888\n",
      "9 loss: 1.0908443927764893\n",
      "10 loss: 1.1737333536148071\n",
      "11 loss: 1.5782774686813354\n",
      "12 loss: 0.7564636468887329\n",
      "13 loss: 0.9480798244476318\n",
      "14 loss: 0.5944101214408875\n",
      "15 loss: 0.7683669328689575\n",
      "16 loss: 0.8417457938194275\n",
      "17 loss: 0.6229531168937683\n",
      "18 loss: 0.6442954540252686\n",
      "19 loss: 0.6352283954620361\n",
      "20 loss: 0.6371446251869202\n",
      "21 loss: 0.6889986991882324\n",
      "22 loss: 0.6429688334465027\n",
      "23 loss: 0.6057713031768799\n",
      "24 loss: 0.7838340997695923\n",
      "25 loss: 0.60114985704422\n",
      "26 loss: 0.6920921206474304\n",
      "27 loss: 0.721646785736084\n",
      "28 loss: 0.7083263993263245\n",
      "29 loss: 0.6465697288513184\n",
      "30 loss: 0.7629004120826721\n",
      "31 loss: 0.6143295764923096\n",
      "32 loss: 0.9705044031143188\n",
      "33 loss: 0.7588817477226257\n",
      "34 loss: 0.6223141551017761\n",
      "35 loss: 0.7429091334342957\n",
      "36 loss: 0.8063795566558838\n",
      "37 loss: 0.7316264510154724\n",
      "38 loss: 0.649051308631897\n",
      "39 loss: 0.6955145001411438\n",
      "40 loss: 0.4754413068294525\n",
      "41 loss: 0.6986544728279114\n",
      "42 loss: 0.6737812757492065\n",
      "43 loss: 0.7536672353744507\n",
      "44 loss: 0.7779824137687683\n",
      "45 loss: 0.5742893815040588\n",
      "46 loss: 0.9037894010543823\n",
      "47 loss: 0.7991876602172852\n",
      "48 loss: 0.9740332961082458\n",
      "49 loss: 0.5698608756065369\n",
      "mIoU on test is: 0.8725553941310068\n",
      "saved  49  with mIoU: 0.8725553941310068\n",
      "50 loss: 0.7972985506057739\n",
      "51 loss: 0.7170469760894775\n",
      "52 loss: 0.5146068930625916\n",
      "53 loss: 0.6646852493286133\n",
      "54 loss: 0.5223239064216614\n",
      "55 loss: 0.6477312445640564\n",
      "56 loss: 0.8289687633514404\n",
      "57 loss: 0.6783854961395264\n",
      "58 loss: 0.6471593379974365\n",
      "59 loss: 0.6614255905151367\n",
      "60 loss: 0.46564632654190063\n",
      "61 loss: 0.728615403175354\n",
      "62 loss: 0.509443461894989\n",
      "63 loss: 0.6495310068130493\n",
      "64 loss: 0.5974992513656616\n",
      "65 loss: 0.5786082744598389\n",
      "66 loss: 0.5127590894699097\n",
      "67 loss: 0.46096867322921753\n",
      "68 loss: 0.8076952695846558\n",
      "69 loss: 0.8446002006530762\n",
      "70 loss: 0.5177485346794128\n",
      "71 loss: 0.7453575134277344\n",
      "72 loss: 0.6330217719078064\n",
      "73 loss: 0.5776835083961487\n",
      "74 loss: 0.5578233599662781\n",
      "75 loss: 0.4188511371612549\n",
      "76 loss: 0.46086403727531433\n",
      "77 loss: 0.5232225656509399\n",
      "78 loss: 0.5524967908859253\n",
      "79 loss: 0.546695351600647\n",
      "80 loss: 0.6438512206077576\n",
      "81 loss: 0.5483056306838989\n",
      "82 loss: 0.6113459467887878\n",
      "83 loss: 0.6704578399658203\n",
      "84 loss: 0.5828455686569214\n",
      "85 loss: 0.814894437789917\n",
      "86 loss: 0.46325379610061646\n",
      "87 loss: 0.5771939158439636\n",
      "88 loss: 0.3176696300506592\n",
      "89 loss: 0.4031265079975128\n",
      "90 loss: 0.5497308969497681\n",
      "91 loss: 0.5544359087944031\n",
      "92 loss: 0.5826983451843262\n",
      "93 loss: 0.503580629825592\n",
      "94 loss: 0.4204222559928894\n",
      "95 loss: 0.47611963748931885\n",
      "96 loss: 0.3555155396461487\n",
      "97 loss: 0.6417373418807983\n",
      "98 loss: 0.42660194635391235\n",
      "99 loss: 0.45336148142814636\n",
      "mIoU on test is: 0.8839509826126887\n",
      "saved  99  with mIoU: 0.8839509826126887\n",
      "100 loss: 0.8529253005981445\n",
      "101 loss: 0.5009856224060059\n",
      "102 loss: 0.7680790424346924\n",
      "103 loss: 0.49304085969924927\n",
      "104 loss: 0.7292812466621399\n",
      "105 loss: 0.5233098864555359\n",
      "106 loss: 0.32024791836738586\n",
      "107 loss: 1.0461866855621338\n",
      "108 loss: 0.48358961939811707\n",
      "109 loss: 0.38775625824928284\n",
      "110 loss: 0.42176109552383423\n",
      "111 loss: 0.6997866630554199\n",
      "112 loss: 0.4357358515262604\n",
      "113 loss: 0.40879493951797485\n",
      "114 loss: 0.520309567451477\n",
      "115 loss: 0.4263956546783447\n",
      "116 loss: 0.385412335395813\n",
      "117 loss: 0.4837283790111542\n",
      "118 loss: 0.44110581278800964\n",
      "119 loss: 0.5486118793487549\n",
      "120 loss: 0.3826395273208618\n",
      "121 loss: 0.6988123655319214\n",
      "122 loss: 0.44705086946487427\n",
      "123 loss: 0.5651627779006958\n",
      "124 loss: 0.42447957396507263\n",
      "125 loss: 0.4880913197994232\n",
      "126 loss: 0.4582308530807495\n",
      "127 loss: 0.4914042055606842\n",
      "128 loss: 0.6025315523147583\n",
      "129 loss: 0.6340180039405823\n",
      "130 loss: 0.3395210802555084\n",
      "131 loss: 0.5142168998718262\n",
      "132 loss: 0.4493139386177063\n",
      "133 loss: 0.636651873588562\n",
      "134 loss: 0.38837796449661255\n",
      "135 loss: 0.40132150053977966\n",
      "136 loss: 0.40714791417121887\n",
      "137 loss: 0.5742768049240112\n",
      "138 loss: 0.27426835894584656\n",
      "139 loss: 0.36504894495010376\n",
      "140 loss: 0.48154252767562866\n",
      "141 loss: 0.4516133964061737\n",
      "142 loss: 0.4991130828857422\n",
      "143 loss: 0.7638463377952576\n",
      "144 loss: 0.8765967488288879\n",
      "145 loss: 0.45486879348754883\n",
      "146 loss: 0.495038777589798\n",
      "147 loss: 0.6111158132553101\n",
      "148 loss: 0.45040082931518555\n",
      "149 loss: 0.4604817032814026\n",
      "mIoU on test is: 0.8873298772277105\n",
      "saved  149  with mIoU: 0.8873298772277105\n",
      "150 loss: 0.5862452983856201\n",
      "151 loss: 0.46592941880226135\n",
      "152 loss: 0.5170157551765442\n",
      "153 loss: 0.49366405606269836\n",
      "154 loss: 0.39848050475120544\n",
      "155 loss: 0.4398905038833618\n",
      "156 loss: 0.681948721408844\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m targets \u001b[39m=\u001b[39m [{k: v\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m t\u001b[39m.\u001b[39mitems()} \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m targets]\n\u001b[1;32m     47\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> 48\u001b[0m loss_dict \u001b[39m=\u001b[39m model(images, targets)\n\u001b[1;32m     49\u001b[0m losses \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(loss \u001b[39mfor\u001b[39;00m loss \u001b[39min\u001b[39;00m loss_dict\u001b[39m.\u001b[39mvalues())\n\u001b[1;32m     50\u001b[0m \u001b[39m#if losses.item()>=0.2:\u001b[39;00m\n",
      "File \u001b[0;32m~/SBILAB/prakhar_pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn [6], line 10\u001b[0m, in \u001b[0;36malchemy.forward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, images, targets\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m---> 10\u001b[0m     ret \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmaskrcnn(images,targets)\n\u001b[1;32m     11\u001b[0m     \u001b[39mreturn\u001b[39;00m ret\n",
      "File \u001b[0;32m~/SBILAB/prakhar_pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/SBILAB/prakhar_pytorch/lib/python3.8/site-packages/torchvision/models/detection/generalized_rcnn.py:104\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(features, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    103\u001b[0m     features \u001b[39m=\u001b[39m OrderedDict([(\u001b[39m\"\u001b[39m\u001b[39m0\u001b[39m\u001b[39m\"\u001b[39m, features)])\n\u001b[0;32m--> 104\u001b[0m proposals, proposal_losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrpn(images, features, targets)\n\u001b[1;32m    105\u001b[0m detections, detector_losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroi_heads(features, proposals, images\u001b[39m.\u001b[39mimage_sizes, targets)\n\u001b[1;32m    106\u001b[0m detections \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform\u001b[39m.\u001b[39mpostprocess(detections, images\u001b[39m.\u001b[39mimage_sizes, original_image_sizes)  \u001b[39m# type: ignore[operator]\u001b[39;00m\n",
      "File \u001b[0;32m~/SBILAB/prakhar_pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/SBILAB/prakhar_pytorch/lib/python3.8/site-packages/torchvision/models/detection/rpn.py:376\u001b[0m, in \u001b[0;36mRegionProposalNetwork.forward\u001b[0;34m(self, images, features, targets)\u001b[0m\n\u001b[1;32m    374\u001b[0m proposals \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbox_coder\u001b[39m.\u001b[39mdecode(pred_bbox_deltas\u001b[39m.\u001b[39mdetach(), anchors)\n\u001b[1;32m    375\u001b[0m proposals \u001b[39m=\u001b[39m proposals\u001b[39m.\u001b[39mview(num_images, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m4\u001b[39m)\n\u001b[0;32m--> 376\u001b[0m boxes, scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfilter_proposals(proposals, objectness, images\u001b[39m.\u001b[39;49mimage_sizes, num_anchors_per_level)\n\u001b[1;32m    378\u001b[0m losses \u001b[39m=\u001b[39m {}\n\u001b[1;32m    379\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n",
      "File \u001b[0;32m~/SBILAB/prakhar_pytorch/lib/python3.8/site-packages/torchvision/models/detection/rpn.py:280\u001b[0m, in \u001b[0;36mRegionProposalNetwork.filter_proposals\u001b[0;34m(self, proposals, objectness, image_shapes, num_anchors_per_level)\u001b[0m\n\u001b[1;32m    277\u001b[0m boxes \u001b[39m=\u001b[39m box_ops\u001b[39m.\u001b[39mclip_boxes_to_image(boxes, img_shape)\n\u001b[1;32m    279\u001b[0m \u001b[39m# remove small boxes\u001b[39;00m\n\u001b[0;32m--> 280\u001b[0m keep \u001b[39m=\u001b[39m box_ops\u001b[39m.\u001b[39;49mremove_small_boxes(boxes, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmin_size)\n\u001b[1;32m    281\u001b[0m boxes, scores, lvl \u001b[39m=\u001b[39m boxes[keep], scores[keep], lvl[keep]\n\u001b[1;32m    283\u001b[0m \u001b[39m# remove low scoring boxes\u001b[39;00m\n\u001b[1;32m    284\u001b[0m \u001b[39m# use >= for Backwards compatibility\u001b[39;00m\n",
      "File \u001b[0;32m~/SBILAB/prakhar_pytorch/lib/python3.8/site-packages/torchvision/ops/boxes.py:132\u001b[0m, in \u001b[0;36mremove_small_boxes\u001b[0;34m(boxes, min_size)\u001b[0m\n\u001b[1;32m    130\u001b[0m ws, hs \u001b[39m=\u001b[39m boxes[:, \u001b[39m2\u001b[39m] \u001b[39m-\u001b[39m boxes[:, \u001b[39m0\u001b[39m], boxes[:, \u001b[39m3\u001b[39m] \u001b[39m-\u001b[39m boxes[:, \u001b[39m1\u001b[39m]\n\u001b[1;32m    131\u001b[0m keep \u001b[39m=\u001b[39m (ws \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m min_size) \u001b[39m&\u001b[39m (hs \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m min_size)\n\u001b[0;32m--> 132\u001b[0m keep \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mwhere(keep)[\u001b[39m0\u001b[39m]\n\u001b[1;32m    133\u001b[0m \u001b[39mreturn\u001b[39;00m keep\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "min_val = 0\n",
    "for iv in range(20):\n",
    "    model = alchemy()\n",
    "    model.to(device)\n",
    "    #model.load_state_dict(torch.load(\"./alchemy_7_0.9183.torch\"))\n",
    "    #model.load_state_dict(torch.load(\"alchemy_0_9116.torch\"))\n",
    "    # lr=0.1\n",
    "    # if(iv>=10==0):\n",
    "    #     lr=6e-5\n",
    "    #optimizer = torch.optim.SGD(model.parameters(), lr=0.01,momentum=0.9,weight_decay=0.00004)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=8e-5)\n",
    "    model.train()\n",
    "    lmbda = lambda epoch: 1\n",
    "    scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)\n",
    "    #scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,200)\n",
    "\n",
    "    # model.train()\n",
    "    # lrs = 0\n",
    "    # for i, data in enumerate(trainloader, 0):\n",
    "    #     lrs+=1\n",
    "    #     images, targets = data\n",
    "    #     images = list(image.to(device) for image in images)\n",
    "    #     targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "    #     optimizer.zero_grad()\n",
    "    #     loss_dict = model(images, targets)\n",
    "    #     losses = sum(loss for loss in loss_dict.values())\n",
    "    #     #if losses.item()>=0.2:\n",
    "    #     losses.backward()\n",
    "    #     optimizer.step()\n",
    "    #     del images,targets\n",
    "    #     #print(\"lr: \",optimizer.param_groups[0]['lr'])\n",
    "    #     print(i,'loss:', losses.item())\n",
    "    \n",
    "    # for param_group in optimizer.param_groups:\n",
    "    #     param_group['lr'] = 8e-5\n",
    "    #     #lr = lr * 0.6\n",
    "\n",
    "\n",
    "    for epoch in range(4): \n",
    "        model.train()\n",
    "        lrs = 0\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            lrs+=1\n",
    "            images, targets = data\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            optimizer.zero_grad()\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "            #if losses.item()>=0.2:\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "            del images,targets\n",
    "            #print(\"lr: \",optimizer.param_groups[0]['lr'])\n",
    "            print(i,'loss:', losses.item())\n",
    "            if lrs==50:\n",
    "                lrs=0\n",
    "                #scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, 200)\n",
    "                # for param_group in optimizer.param_groups:\n",
    "                #     param_group['lr'] = lr*0.6\n",
    "                #     lr = lr * 0.6\n",
    "                val = evalm(model,fintestloader)\n",
    "                if(val>min_val):\n",
    "                    print(\"saved \",str(i),\" with mIoU:\",val)\n",
    "                    min_val=val\n",
    "                    #torch.save(model.state_dict(), \"alchemy_\"+str(iv)+\"_\"+str(val)[0:6]+\"_d12.torch\")\n",
    "        #val = evalm(model,fintestloader)\n",
    "        val = evalm(model,fintestloader)\n",
    "        if(val>min_val):\n",
    "            print(\"saved \",str(i),\" with mIoU:\",val)\n",
    "            min_val=val\n",
    "            #torch.save(model.state_dict(), \"alchemy_\"+str(iv)+\"_\"+str(val)[0:6]+\".torch\")\n",
    "        #torch.save(model.state_dict(), \"alchemy_\"+str(iv)+\".torch\")\n",
    "    #scheduler.step()  \n",
    "        \n",
    "    del model\n",
    "\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = alchemy()\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load(\"alchemy_0_0.9161_d12.torch\"))\n",
    "evalm(model,fintestloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('prakhar_pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "603527dc4b5f0cf90dba6785ed8ce1bf41404d422ebeb0779687782e89bf431c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
