{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import numpy as np\n",
    "import torch.utils.data\n",
    "import cv2\n",
    "import torchvision.models.segmentation\n",
    "import torch\n",
    "import os\n",
    "import patchify\n",
    "from sklearn.datasets import load_sample_image\n",
    "from sklearn.feature_extraction import image as skimg\n",
    "import imgaug as ia\n",
    "import imgaug.augmenters as iaa\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNN_ResNet50_FPN_V2_Weights\n",
    "import torch.optim\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import sys\n",
    "sys.path.insert(1, '/home/prakharug/Experimental')\n",
    "sys.path.insert(1, '/home/prakharug/Experimental/pycoco')\n",
    "from pycoco.engine import train_one_epoch, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_dims(a, start=0, count=2):\n",
    "    \"\"\" Reshapes numpy array a by combining count dimensions, \n",
    "        starting at dimension index start \"\"\"\n",
    "    s = a.shape\n",
    "    return np.reshape(a, s[:start] + (-1,) + s[start+count:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../dataset/images'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m instance_toimage \u001b[39m=\u001b[39m []\n\u001b[0;32m----> 3\u001b[0m \u001b[39mfor\u001b[39;00m im_name \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39;49mlistdir(\u001b[39m\"\u001b[39;49m\u001b[39m../dataset/images\u001b[39;49m\u001b[39m\"\u001b[39;49m):\n\u001b[1;32m      4\u001b[0m     tmplist \u001b[39m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m     lent \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(im_name[:\u001b[39m-\u001b[39m\u001b[39m4\u001b[39m])\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../dataset/images'"
     ]
    }
   ],
   "source": [
    "instance_toimage = []\n",
    "\n",
    "for im_name in os.listdir(\"../dataset/images\"):\n",
    "    tmplist = []\n",
    "    lent = len(im_name[:-4])\n",
    "    print(im_name,im_name[:lent])\n",
    "    imgtmp = cv2.imread(\"../dataset/images/\"+im_name,1).transpose(2,0,1)\n",
    "    #print(imgtmp[0].shape)\n",
    "    tmplist.append(imgtmp[0])\n",
    "    tmplist.append(imgtmp[1])\n",
    "    tmplist.append(imgtmp[2])\n",
    "    for gt_name in os.listdir(\"../dataset/ground_truths\"):\n",
    "        #print(gt_name[10+lent],gt_name[10:10+lent])\n",
    "        if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]==\"_\":\n",
    "            mask = np.array(cv2.imread(\"../dataset/ground_truths/\"+gt_name,0))\n",
    "            # mask = (mask > 0).astype(np.uint8) \n",
    "            tmplist.append(mask)\n",
    "    tmplist = np.array(tmplist)\n",
    "    print(tmplist.shape)\n",
    "    instance_toimage.append(tmplist)\n",
    "\n",
    "# for im_name in os.listdir(\"../dataset/test\"):\n",
    "#     tmplist = []\n",
    "#     lent = len(im_name[:-4])\n",
    "#     print(im_name,im_name[:lent])\n",
    "#     imgtmp = cv2.imread(\"../dataset/test/\"+im_name,1).transpose(2,0,1)\n",
    "#     #print(imgtmp[0].shape)\n",
    "#     tmplist.append(imgtmp[0])\n",
    "#     tmplist.append(imgtmp[1])\n",
    "#     tmplist.append(imgtmp[2])\n",
    "#     for gt_name in os.listdir(\"../dataset/ground_truths\"):\n",
    "#         #print(gt_name[10+lent],gt_name[10:10+lent])\n",
    "#         if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]==\"_\":\n",
    "#             mask = np.array(cv2.imread(\"../dataset/ground_truths/\"+gt_name,0))\n",
    "#             # mask = (mask > 0).astype(np.uint8) \n",
    "#             tmplist.append(mask)\n",
    "#     tmplist = np.array(tmplist)\n",
    "#     print(tmplist.shape)\n",
    "#     instance_toimage.append(tmplist)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 2\n",
    "img_siz = [512,512]\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu') \n",
    "print(device)\n",
    "\n",
    "from readline import append_history_file\n",
    "from unittest.mock import patch\n",
    "\n",
    "\n",
    "def loadBatch():\n",
    "    rtrnd=random.randint(0,4)\n",
    "    seq = iaa.Sequential([\n",
    "    iaa.Sometimes(\n",
    "        0.5,\n",
    "        iaa.GaussianBlur(sigma=(0, 0.5))\n",
    "    ),\n",
    "    iaa.Rot90((rtrnd), keep_size=False)\n",
    "    ])\n",
    "    batch_Imgs=[]\n",
    "    batch_Data=[]\n",
    "    for i in range(batchSize):\n",
    "        idx=random.randint(0,len(instance_toimage)-1)\n",
    "        patch_stack = seq(images=instance_toimage[idx])\n",
    "        patch_stack = np.array(patch_stack)\n",
    "        k_w = random.randint(512,patch_stack.shape[2])\n",
    "        k_h = random.randint(512,patch_stack.shape[1])\n",
    "        #k_w = patch_stack.shape[2]\n",
    "        #k_h = patch_stack.shape[1]\n",
    "        o_w = random.randint(0,patch_stack.shape[2]-k_w)\n",
    "        o_h = random.randint(0,patch_stack.shape[1]-k_h)\n",
    "        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]\n",
    "        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]\n",
    "        instances = instances.transpose(1,2,0)\n",
    "        data = {}\n",
    "        masks = []\n",
    "        boxes = []\n",
    "        t=0\n",
    "        sem_mask = np.zeros((instances.shape[0],instances.shape[1]))\n",
    "        for a in range(instances.shape[2]):\n",
    "            dispim = instances[:,:,a]\n",
    "            if np.all(dispim == 0):\n",
    "                continue\n",
    "            x,y,w,h = cv2.boundingRect(dispim)\n",
    "            boxes.append([x, y, x+w, y+h])\n",
    "            masks.append(dispim/255)\n",
    "            sem_mask += dispim\n",
    "            t=1\n",
    "        masks = np.array(masks)\n",
    "        sem_mask = sem_mask > 0\n",
    "        sem_mask_c = torch.as_tensor(sem_mask.sum())\n",
    "        neg_mask = (sem_mask == 0)\n",
    "        neg_mask_c = torch.as_tensor(neg_mask.sum())\n",
    "        sem_mask = np.array([sem_mask,sem_mask,sem_mask])\n",
    "        sem_mask = torch.as_tensor(sem_mask,dtype=torch.uint8)\n",
    "        neg_mask = np.array([neg_mask,neg_mask,neg_mask])\n",
    "        neg_mask = torch.as_tensor(neg_mask,dtype=torch.uint8)\n",
    "        if t==0:\n",
    "            return loadBatch()\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        img = torch.as_tensor(patch_img, dtype=torch.float32)\n",
    "        img = img/255\n",
    "        data[\"boxes\"] =  boxes\n",
    "        data[\"labels\"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class\n",
    "        data[\"masks\"] = masks\n",
    "        data[\"sem_mask\"] = sem_mask\n",
    "        data[\"neg_mask\"] = neg_mask\n",
    "        data[\"sem_mask_c\"] = sem_mask_c\n",
    "        data[\"neg_mask_c\"] = neg_mask_c\n",
    "        batch_Imgs.append(img)\n",
    "        batch_Data.append(data)\n",
    "    return batch_Imgs, batch_Data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MMCellDataset(Dataset):\n",
    "    def __init__(self,root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.img = []\n",
    "        for im_name in os.listdir(self.root_dir):\n",
    "            tmplist = []\n",
    "            lent = len(im_name[:-4])\n",
    "            print(im_name,im_name[:lent])\n",
    "            imgtmp = cv2.imread(self.root_dir+im_name,1).transpose(2,0,1)\n",
    "            tmplist.append(imgtmp[0])\n",
    "            tmplist.append(imgtmp[1])\n",
    "            tmplist.append(imgtmp[2])\n",
    "            for gt_name in os.listdir(\"../dataset/ground_truths\"):\n",
    "                if gt_name[10:10+lent]==im_name[:lent] and gt_name[10+lent]==\"_\":\n",
    "                    mask = np.array(cv2.imread(\"../dataset/ground_truths/\"+gt_name,0))\n",
    "                    tmplist.append(mask)\n",
    "            tmplist = np.array(tmplist)\n",
    "            print(tmplist.shape)\n",
    "            self.img.append(tmplist)   \n",
    "    \n",
    "    def __len__(self):\n",
    "        return len([name for name in os.listdir(self.root_dir)])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        patch_stack = self.img[idx]\n",
    "        patch_stack = np.array(patch_stack)\n",
    "        k_w = patch_stack.shape[2]\n",
    "        k_h = patch_stack.shape[1]\n",
    "        o_w = random.randint(0,patch_stack.shape[2]-k_w)\n",
    "        o_h = random.randint(0,patch_stack.shape[1]-k_h)\n",
    "        patch_img = patch_stack[0:3,o_h:o_h+k_h,o_w:o_w+k_w]\n",
    "        instances = patch_stack[3:,o_h:o_h+k_h,o_w:o_w+k_w]\n",
    "        instances = instances.transpose(1,2,0)\n",
    "        data = {}\n",
    "        masks = []\n",
    "        boxes = []\n",
    "        area = []\n",
    "        t=0\n",
    "        for a in range(instances.shape[2]):\n",
    "            dispim = instances[:,:,a]\n",
    "            x,y,w,h = cv2.boundingRect(dispim)\n",
    "            boxes.append([x, y, x+w, y+h])\n",
    "            area.append(torch.tensor(h*w))\n",
    "            masks.append(dispim/255)\n",
    "            t=1\n",
    "        masks = np.array(masks)\n",
    "        masks = torch.as_tensor(masks, dtype=torch.uint8)\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        area = torch.as_tensor(area, dtype=torch.float32)\n",
    "        img = torch.as_tensor(patch_img, dtype=torch.float32)\n",
    "        img = img/255\n",
    "        data[\"boxes\"] =  boxes\n",
    "        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
    "        data[\"iscrowd\"] = iscrowd\n",
    "        data[\"labels\"] =  torch.ones((boxes.shape[0],), dtype=torch.int64)   # there is only one class\n",
    "        data[\"masks\"] = masks\n",
    "        data[\"area\"] = area\n",
    "        data[\"image_id\"] = torch.tensor(idx)\n",
    "        return img,data\n",
    "test = MMCellDataset(\"../dataset/test/\")\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "print(len(test))\n",
    "test_dataloader = DataLoader(test, batch_size=2, shuffle=False,collate_fn = collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class jsrcnn_v5(nn.Module):\n",
    "    def __init__(self,**kwargs) -> None:\n",
    "        super(jsrcnn_v5, self).__init__()\n",
    "        self.vis = False\n",
    "        #self.param = param\n",
    "        self.channel_trans = nn.ModuleList([\n",
    "            torch.nn.Conv2d(3,12,(1,1),stride=1,padding='same'),\n",
    "            #torch.nn.Conv2d(12,24,(1,1),stride=1,padding='same'),\n",
    "            torch.nn.Conv2d(12,3,(1,1),stride=1,padding='same'),\n",
    "            #torch.nn.Conv2d(24,12,(72,72),stride=1,padding='same',padding_mode='replicate'),\n",
    "            #torch.nn.Conv2d(12,12,(48,48),stride=1,padding='same',padding_mode='replicate'),       \n",
    "        ])\n",
    "        # self.maskbone = nn.Sequential(\n",
    "        #     torch.nn.Conv2d(27,12,(3,3),stride=1,padding='same',padding_mode='replicate'),\n",
    "        #     torch.nn.Conv2d(12,12,(24,24),stride=1,padding='same',padding_mode='replicate'),\n",
    "        #     torch.nn.Conv2d(12,12,(32,32),stride=1,padding='same',padding_mode='replicate'),\n",
    "        # )\n",
    "        self.sigma = nn.Sigmoid()\n",
    "        self.norm3 = nn.BatchNorm2d(3,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.norm12 = nn.BatchNorm2d(12,eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "        self.ReLU = nn.ReLU()\n",
    "        # self.tobi = torch.nn.Conv2d(42,1,(5,5),stride=1,padding='same',padding_mode='replicate')\n",
    "        # self.rko = torch.nn.Conv2d(15,3,(1,1),stride=1,padding='same',padding_mode='replicate')\n",
    "        self.maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn_v2(weights=MaskRCNN_ResNet50_FPN_V2_Weights.DEFAULT,trainable_backbone_layers=5,**kwargs)\n",
    "        in_features = self.maskrcnn.roi_heads.box_predictor.cls_score.in_features  # get number of input features for the classifier\n",
    "        self.maskrcnn.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)  # replace the pre-trained head with a new one\n",
    "\n",
    "    def forward(self, images, targets=None):\n",
    "        trniml = []\n",
    "        color_loss = torch.Tensor([0]).to(device)\n",
    "        for ind in range(len(images)):\n",
    "            img = torch.stack([images[ind]])\n",
    "            imgog = img\n",
    "            img = self.channel_trans[0](img)\n",
    "            img = self.norm12(img)\n",
    "            img = self.ReLU(img)\n",
    "            img = self.channel_trans[1](img)\n",
    "            #img = self.ReLU(img)\n",
    "            #img = self.channel_trans[2](img)\n",
    "            img = self.ReLU(self.norm3(img))\n",
    "            images[ind] = img[0]\n",
    "            if self.vis == True:\n",
    "                cv2.imwrite(\"mod_img.png\",(img[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))\n",
    "                cv2.imwrite(\"og_img.png\",(imgog[0].clone().detach().cpu().numpy()*255).transpose(1,2,0))\n",
    "            #img3 = self.channel_trans[2](img)\n",
    "            #img3 = self.ReLU(img3)\n",
    "            # img3 = img\n",
    "            # img4 = self.channel_trans[2](img3)\n",
    "            # img4 = self.ReLU(img4)\n",
    "            # img5 = self.channel_trans[3](img4)\n",
    "            # img5 = self.ReLU(img5)\n",
    "            # imgmask = torch.concat((img5,img4,imgog),1)\n",
    "            # mk = self.maskbone(imgmask)\n",
    "            # segmask = self.tobi(torch.concat((imgmask,mk,imgog),1))\n",
    "            # images[ind] = torch.cat((images[ind],xconv2[0],xconv1[0]),0)\n",
    "            # H = images[ind].shape[1]\n",
    "            # W = images[ind].shape[2]\n",
    "            # kek = images[ind].to(device)\n",
    "            # images[ind] = images[ind].permute((1,2,0))\n",
    "            # images[ind] = images[ind].reshape([H*W,12])\n",
    "            # images[ind] = self.channel_trans(images[ind])\n",
    "            # images[ind] = images[ind].reshape([H,W,3])\n",
    "            # images[ind] = images[ind].permute((2,0,1))\n",
    "        ret = self.maskrcnn(images,targets)\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = jsrcnn_v5(box_nms_thresh=0.5)\n",
    "model.to(device)\n",
    "model.load_state_dict(torch.load('jsr_v5.69_1200.torch'))\n",
    "optimizer = torch.optim.AdamW([{ 'params': model.maskrcnn.parameters(), 'lr' : 1e-6},{ 'params' : model.channel_trans.parameters(), 'lr' : 1e-11}], lr=1e-4)\n",
    "#optimizer = torch.optim.AdamW(model.parameters(),lr=1e-4)\n",
    "model.train()\n",
    "lmbda = lambda epoch: 0.8\n",
    "# for param in model.channel_trans.parameters():\n",
    "#     param.requires_grad = False\n",
    "scheduler = torch.optim.lr_scheduler.MultiplicativeLR(optimizer, lr_lambda=lmbda)\n",
    "\n",
    "minloss = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = 1800\n",
    "for i in range(10000):\n",
    "    if i%20==0:\n",
    "        model.vis = True\n",
    "    images, targets = loadBatch()\n",
    "    images = list(image.to(device) for image in images)\n",
    "    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss_dict = model(images, targets)\n",
    "    # for losst in loss_dict.keys():\n",
    "    #     print(losst,loss_dict[losst])\n",
    "    losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "    losses.backward()\n",
    "    optimizer.step()\n",
    "    print(test+i,'loss:', losses.item())#,\" color_loss:\",loss_dict[\"color_loss\"].item())\n",
    "    model.vis = False\n",
    "    \n",
    "    if i%100==0:\n",
    "        scheduler.step()\n",
    "        print(scheduler.get_lr())\n",
    "        evaluate(model, test_dataloader, device=device)\n",
    "        model.train()\n",
    "    if i%200==0 and i!=0:\n",
    "        evaluate(model, test_dataloader, device=device)\n",
    "        model.train()\n",
    "        torch.save(model.state_dict(), \"jsr_v5.69_\"+str(+test+i)+\".torch\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('prakhar_pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "603527dc4b5f0cf90dba6785ed8ce1bf41404d422ebeb0779687782e89bf431c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
